Source (A),Source (B),WC,WPS,Sixltr,Dic,funct,pronoun,ppron,i,we,you,shehe,they,ipron,article,verb,auxverb,past,present,future,adverb,preps,conj,negate,quant,number,swear,social,family,friend,humans,affect,posemo,negemo,anx,anger,sad,cogmech,insight,cause,discrep,tentat,certain,inhib,incl,excl,percept,see,hear,feel,bio,body,health,sexual,ingest,relativ,motion,space,time,work,achieve,leisure,home,money,relig,death,assent,nonfl,filler,AllPunc,Period,Comma,Colon,SemiC,QMark,Exclam,Dash,Quote,Apostro,Parenth,OtherP
"""861fd45532e720b19456ed7a5ae138ed94d62927fa268077f96c42b8f0c4d5b2""","""offer couchwiki project named gorsvet.urlyss tiny url-shortener service written flask using dare may become dares. hi geert-jan yup http//replaced.url modifications patch could work thanks info. run similar issue mix webapps non-solr guys use log0j rotating daily appender since supported default jdk logger mismatch log files produced would looked mention wrapping log0j logmanager big deal bit annoyance search reusable implementation find anything looked good since use resin resin support describing approach taking short-term embedded jetty usage shall back situation ryan arguing use apache commons logging have enough fun nutch commenting discussion. might look nutch handles issue nutch stopwords wants keep around generates combo terms like the- index query parser thing query phrase common terms wind searching across much smaller slice comes course expense larger index lot unique terms due combo terms big win example site http//replaced.url index source files without optimization searches could several seconds got 000ms lots breathing room. hi checking seems indexed fields specified sorting purposes query string least seeing date field correct miss info wiki someplace case would handy standardrequesthandler return error invalid request sorting non-indexed field requested thanks. hi regexreplaceprocessorfactory line means use match groups replacement string reasoning behind missing something groups used making hard write simple solution training exercise students need clean incorrectly formatted dates thanks. hi creating combo field searching straight-forward way boost contribution fields used create combined field would read past threads seem anything built solr simple hack copy field multiple times e.g schema field-to-boost effective boost 0x since highlighting display multi-value field seems work ok long course level boosting 0x 0x etc acceptable something continue work future issues approach run yet thanks. got version index appears open luke reports problem grins crafted matching schema tried use index solr solr-trunk either case get exception startup startup logging says trying something destined fail would expected schema/index miss-matches show later right index opened would seen various posts error due corrupt indexes buggy version java obscure lucene none seem apply situation thanks. thing bit previously using apis area solr call corecontainer.getcore increments open count balance call close call naming could better think common expectation calls get something change state maybe. hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor isdatadir gets used construction changing coredescriptor isdatadir effect since would go changing datadir core multi- thanks. hi way explicitly add record using solr add fail record already exists value unique field specified schema isuniquekey field course pre-flight query see record unique exists multi-client environment solr multiple users reliable thanks. exact version resin still confirm uncommented lines""",453,"37,75","31,57","56,73","11,92","1,99","0,00","0,00","0,00","0,00","0,00","0,00","1,99","0,22","13,91","3,09","2,43","8,83","1,99","1,10","2,21","0,00","0,66","5,08","0,00","0,00","5,52","0,00","0,00","0,22","6,84","5,08","1,77","0,00","0,44","1,10","20,09","4,42","6,84","3,31","4,19","0,66","0,22","1,32","1,32","1,99","1,55","0,22","0,22","0,44","0,22","0,22","0,00","0,00","13,69","3,31","4,86","5,30","3,09","3,75","0,66","0,66","0,88","0,00","0,00","0,44","0,00","0,22","8,39","3,53","0,00","0,00","0,00","0,00","0,00","3,31","0,44","0,00","0,00","1,10"
"""1cc23fe0afdfd7abeb62bba9ce9deacde4fc680cf034895076d3c0eae42c8320""","""anyone please help following sure done something wrong downloaded zip http//replaced.url warning problems encountered building effective model org.apache.parquet parquet-scala_0.00 jar:0.0-snapshot warning problems encountered building effective model org.apache.parquet parquet-scrooge_0.00 jar:0.0-snapshot warning highly recommended fix problems threaten stability build warning reason future maven versions might longer support building malformed projects error see full stack trace errors re-run maven -e switch error information errors possible solutions please read following articles error correcting problems resume build command. ok tried http//replaced.url seems build fine. written additional parquet-tools helper access files hadoop environment along lines existing parquet-schema etc scripts provided part parquet-tools distribution designed live location scripts hopefully others find useful determine path isdirectory else. attaching patch put together decimal includes added fixed bytes try make generic based branch avro logical support avro classes missing would need copy parquet fix schema getlogicaltype accessor need use instead think pretty straight-forward questions let know. sorry delay getting back investigating angles using tableau drill create views drill access parquet files even though parquet files hold schema create views cast columns correct data bigint float timestamp etc updates suggested parquet-avro add support timestamp urgent originally thought since cast everything anyway. thanks sounds need go trial local copies code would good check delayed contact http//replaced.url discuss. hi dave think pushing solr space probably going non-starter solr isfocus index management serving side things different multitude issues faced looking good free would try ibm isuses lucene hood fact wonder could reverse index create solr schema. could use lengthfilterfactory restrict terms least character long believe patternreplacefilterfactory creates patternreplacefilter says. providing maven0 build would useful us currently jar in-house maven repo works clean would favor changing directory layout selfish reasons would favor changing build rely maven0 use maven internally krugle sometimes works well times royal pain butt option would nice handy personally would hate foist maven everybody else. looked briefly passing analyzer use morelikethis fields lot analyzer used given filters play performance really stunk use stored term vectors would nice yes thanks. hi list anybody know suggester component designed work shards asking documentation implies since suggester reuses much spellcheckcomponent infrastructure spellcheckcomponent documented supporting distributed setup make request get exception looking querycomponent.java:0 code see assuming docs variable null would happen response element solr response make direct request request handler core e.g http//hostname:0/solr/core0/select qt=suggest-core q=rad query works see element named response unlike regular query wondering configuration borked work fact suggester return response field means work shards thanks. hi got situation key result initial search request let issay list values faceted top faceted field values need get top hit target request restricted value currently total requests requests following initial query made parallel still questions magic query handle solr as-is best solution create request handler case developing custom thanks. hi particularly uwe robert yes gist question specify use simpletextxxx e.g simpletextstoredfieldsformat solr currently possible thanks. grabbed latest greatest trunk make main.css .query-box height tall enough least mac 0/ff config character descenders get bumped fixed issue solr looks like contains anchor text missing constraints see .constraints css added main.css ianawd probably best way fix issue see css used intent set character gray seems silly open jira issues types things add noise list approach preferred thanks. hi marcus look new project called katta first code check-in happening weekend would wait monday look. clarify issue using actual user search traffic seo content expose example people commonly search java hint url static content page language part generating static pages based search traffic though might decide content favor see based popularity yes need automate content generation regular e.g weekly basis big challenges ran dealing badly behaved bots would hammer site wound putting content separate system would impact users main system generating regular report user agent ip address could block robots.txt ip necessary figuring structure static content look like spam many links page much depth constrains many pages reasonably expose project scores based code activity usage used rank content focus exposing early low depth good stuff could based popularity search logs anyway lot topic feel solr specific apologies reducing signal-to-noise ratio. thanks fast response beginning worry nobody read posts see http//replaced.url attached test code issue plus simple fix json case regards. hi part interesting work creating custom query parser writing unit tests exercised extendeddismaxqparser first created extendeddismaxqparserplugin used create qparser via query something like complexphrase query expecting complex phrase query parser get used ishappening local param treated regular text makes think conceptual model local params processing wrong ishigher level code pre-processing step first hoping d get disjunctionmaxqueries queries complexphrasequery would mean processing happen inside extendeddismaxqparser code pointers poke around thanks. hi upgrading solr noticed filtercache hit ratio dropped significantly previously enabled recording entries debugging looking seems edismax faceting creating entries sharded setup distributed search search string bogus text using edismax two fields get entry shard isfilter caches looks like expected similar situation happening faceted search even though fields single-value/untokenized strings using enum facet method shall get many many entries filtercache facet values look like item_ net result even big filtercache 0k hit ratio still thanks insights. must missing something isschema exactly original example schema look index using latest luke would need rebuild luke current lucene code since format version tried using solr/admin/analysis see query returning get server error numerictokenstream support chartermattribute looks like happy analysis triefloatfield searches index using solr/admin results single value e.g ideas might going thanks. hi yonik thanks fast response ok curious made bit confusing api well-formed xml xml parser happy reserved xml characters need escaped using org.apache.commons.lang.stringescapeutils seems work pretty well great advice let give try thanks. hi quick note caveat using slightly older version solr without fragile sense relies current working directory resin home directory b path must ./webapps/ anyway worth solrsevlet.java isinit method change beginning see instance resin specifying explicit location configuration data note relative original source least version parameter solr.configdir lower-case build deploy solr resin called solr-notes example is/webapps/solr-notes/web-inf/web.xml a. uncomment first three system-property lines noted comment work around bug resin b. add init-param section copy contents solr config directory /webapps/solr-notes/conf part edit specify location data directory repeat steps second different. answering question use corecontainer.reload core force assume got embeddedsolrserver running time everything happen correctly covers need find programmatically change settings using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks""",1117,"44,68","33,75","55,68","11,19","1,34","0,09","0,00","0,09","0,00","0,00","0,00","1,25","0,09","12,98","2,78","1,70","8,77","1,88","2,06","1,16","0,45","0,18","4,30","1,25","0,09","4,21","0,00","0,00","0,09","8,33","6,36","1,97","0,18","0,45","0,45","21,04","5,82","6,71","3,40","3,22","1,34","1,07","0,98","0,54","2,95","2,24","0,36","0,27","0,36","0,09","0,27","0,09","0,00","10,12","2,42","2,78","4,30","3,31","3,67","0,63","0,36","0,72","0,00","0,00","0,45","0,27","0,45","10,56","4,57","0,00","0,36","0,00","0,00","0,00","2,60","0,18","0,00","0,00","2,86"
"""2558c44d40a4e4abe2b563f51229232dc151c6ff22a82b24fb53a6c28a9fcb31""","""hi think line dependencyinformationreport.java needs fixed replacing could someone update please let know need additional information kind regards. hi documentation wikipediatokenizer specifically wondering pieces source xml get mapped field names solr schema example seems going date field example schema got goes body way get example thanks. using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks. would parse html extract text first use index data nutch tika projects examples using html. hi ryan dug client code natural inclination would go latter fits better would write test use test request object. mitch right looking recommendation engine understand question properly yes mahout work though taste recommendation engine supports pretty new owen robin anil mahout action book early release via manning lots assuming list recommendations given user based past behavior recommendation engine could use adjust search results waiting jump best handle. ran minor problem clicked facet tried search would get error think problem fqs velocity macro vm_global_library.vm missing else insert url macro fqs p foreach p velocitycount else without url becomes /solr/browsefq=xxxx instead /solr/ completely new world velocity templating got low confidence right way fix. hi martin lurker interesting thread would suggest talking solr committers experience merging lucene got many similarities discussing though solr mature happened seen externally ultimately win though without lot teething issues many seemed personality conflicts groups versus technical/admin/operational issues additional comment inline ps counter example fine separate project solr releases eventually came faster lucene release cycle improved slow-down everyone expecting certainly reduced continuous painful debates belonged solr vs. lucene. believe code support handling form data sent put request logic make sure either unspecified application/x-www-form-urlencoded read body byte array use request convert string request unspecified assume us-ascii typically specified e.g use curl tool post data sent header convert key/value pairs using urldecoder.deccode string utf-0 since key/value pairs url-encoded believe standard assume us-ascii utf-0 would work well us-ascii viewed sub-set utf-0. two cases think cases two characters variant forms unicode tried unify still exist gb tons wanted support phonetic pinyin zhuyin search might collapse syllables commonly confused course would storing phonetic forms words. hi especially yonik http//replaced.url page mentions duplicate field collapsing later allow duplicate see mention deduplication happens search time normally requires field stored indexed efficiency might need fieldcache wondering status support thoughts potential thanks. hi james general immediate updating index continuous stream new content fast search results work opposition searcher isvarious caches getting continuously flushed avoid stale content easily kill performance issue interesting topics discussed lucene bof meeting apachecon alone wanting ways clear hard problem relax need immediate updates index accept level lag time receiving new content showing index would suggest splitting two processes backend system deals updates. code appears try maybe code date see /solr/conf used path code see created cwd/data data default specified work tried number different permutations maybe missed magic combination grepped source see pattern solr.solr.home used anywhere able control location config file using set location /data directory still gets created saw though seem use either see solr.datadir used code anywhere trying control command line seem work could uncomment edit tag file worked makes worried missing something wrong sources tell ways control location data directory used index a. change cwd sure inside resin b. edit tag file lets change directory ways control location config directory lets change directory thanks. got situation data directory needs live elsewhere besides inside solr home b moves different location updating indexes setting symlink /data great option best approach making work solrj low- level solution seems create solrcore instance specify data directory use update corecontainer wrong would like avoid mucking around low-level solrcore approaches thanks. hi robert -xx heapdumppath= something look versus gedankenexperiment. moving solr-dev solr-user quick impression given scope described page feels like boil ocean problem spent afternoon looking could use solr code getting much love somehow leveraging solr would seem like win three attributes solr interesting context complex query processing caching though critical things live noticed described issues automatic doc- server mapping calc stable hash quick exploration use hadoop rpc talk guts solr assumes query processing happens search server level versus master currently nutch way request summaries document via subsequent post-merge call master immediate problem ran notion solr running inside container currently penetrates deep bowels code even core level calls made extract query parameters url step going try clean manner would define side/solr core api layer would relatively easy least first cut hooking solr core nutch master via hadoop prc. hi michael build meant gui whereby user use special characters say quoting collection clauses programmatically build query without using query parser code wound write seemed like simple escaping quickly got complex convoluted e.g allow term get processed specially query parser ok case sounds like stuck escaping. hi ken given comments seemed describe using nrt opposite use case set solr use solr.mmapdirectoryfactory bother test whether nrt would better use case mostly sound like advantage focused things relating solr would love hear results someone testing batch indexing use case tested various xxxdirectoryfactory implementations please let know results testing. hi chris would thought queries would allow thing supported currently standard query would recommend re-posting lucene user list. general notice http//replaced.url uses solr search project data store index user-generated notes code finally went live last week openly post thanks solr community useful tool great. yes came exactly conclusion could try use ram jvm load index ramdirectory mmapdirectory wound slower configuration smaller jvm relied os-level cache make index accesses really fast trick heard /dev/null force index data. hi got fields contain embedded xml two questions relating appears though shall need xml-escape field data otherwise solr complains find start tag embedded tags finds tag field expected constraint xml-escaping data best way handle kind related question would easiest way ignore xml tag data indexing types xml-containing fields seems like could define new field e.g set associated tokenizer something new create though would un-escape data ick parsing skip tags thanks. hi uses solr generate terms mailing list text analysis extract good features things like classification similarity clustering last part cover using solr implement real-time similarity engine maybe recommendation engine well undoubtedly things unclear even incorrect please comment regards. ah yes would explain oddity saw order hits matching document boost since querying field omit norms really short. hi uses character separate elements e.g state|city solr 0.x worked fine query gets distributed across shards get solrexception severe org.apache.solr.common.solrexception use fieldcache field neither indexed doc values state problem appears fl= state|city parameter getting split functionqparser tries use state field actually exists ignored field since q=state|city ca| find entries california known issue way disable parsing field names field list thanks. actually tika htmlparser wraps tagsoup good option""",1161,"41,46","32,73","56,93","11,11","1,46","0,43","0,00","0,43","0,00","0,00","0,00","1,03","0,09","14,73","2,76","3,79","8,27","1,98","2,76","1,12","0,78","0,34","2,67","0,95","0,00","5,60","0,00","0,00","0,00","8,01","5,43","2,58","0,34","0,43","0,60","21,62","5,51","5,86","3,70","3,79","0,86","1,46","0,69","1,21","1,81","1,03","0,43","0,17","1,12","0,43","0,34","0,17","0,17","11,97","2,84","2,76","4,65","3,45","4,05","0,52","0,26","0,34","0,00","0,09","0,60","0,17","0,52","9,13","3,96","0,00","0,00","0,00","0,00","0,00","2,33","0,17","0,00","0,00","2,67"
"""637258698cb8be15b9d856019f6325312df39b6028e1a87e846b26d2dd4dfbc8""","""hi list requirement producers wait indefinitely kafka broker comes case maintenance network interruptions disk caching front producer producer waits drains producer eventually reconnects functionality working fine kafka producer settings used acks=all would like acknowledge gwen shapiras slideshare http//replaced.url setting derived. hi ian public terabyte dataset project would good match need course means actually finish crawl finalize avro format use data free collections data around though none know target top-ranked pages. would great facing issue rolling code keep solr index sync mysql db access via hibernate thanks. hi otis sorry missed reply thanks trying find similar report wondering file jira issue might get attention. sounds like hitting max url length 0k common default http web server using run solr web servers know let bump limit via configuration settings. think ran would like run two copies solr inside resin since two sets searchable data distinct schemas/usage patterns specify location configuration directory least using either solr.configdir solr.solr.home subscribed dev list shall post question timing fix versus something quick needs fwiw quick fix pass directory location init thanks. response header and/or meta tags page see code used nutch could missing could wrong either/both issue determining right arbitrary web page easy way analysis advance know sure always x going simplify things soon possible means right processing page collation settings db change db interprets data change data. ok thanks confirming absolute easiest approach would support new init value codecfactory schemacodecfactory would use select different base codec use versus always using lucenecodec would switch everything different codec could extend schemacodecfactory support additional per-field settings stored fields format etc beyond currently available quick dirty hack specified different codecfactory factory hard-codes simpletextcodec works files simpletextxxx format segments.gen segments_xx files pluggable. hi using simpletextcodec past noticed something odd running solr enable posting format via something like resulting index expected text output noticed files standard binary format e.g .fdt field data based simpletextcodec.java assuming would get simpletextstoredfieldsformat stored data holds true files e.g http//replaced.url adding simple text format docvalues walk code figure hoping need change configuration setting thanks. right looking snapinstaller seemed though deletion index directory would deleting files indexreader thanks. url work please include full stack trace runtimeexception version tika using thanks. worked around way. could background gc depending got jvm configured use though stay long. another trick described andrzej another field days write token e.g number times matches age document starting reasonable base date add +days:0 query lucene winds boosting results recent. hi assuming use first characters specific field grouping results thing possible out-of-the-box would next best option e.g custom function query thanks. hi olivier setting mime explicitly via stream.type parameter. auto-generated unique value would like. included bit information configure top-level sharding request handler use suggest component get npe get response results completeness pieces puzzle suggest-one suggest-one suggest-two thanks. hi geert-jan thanks ref good stuff think close understand correctly could get using top two versus top simplicity results looked like actual faceted field value/hit counts would top hit facet field followed facet field used field collapsing would improve probability asked top hits would find entries top faceted thanks. seemed work thanks suggestion though using case anybody else reads shall need run tests check performance improvements hadoop workflows output always fresh unless interesting helicopter stunts yes default index always rebuilt scratch thus long primary key used reduce-phase key easy ensure uniqueness index thanks. finally got around looking short field values returned java.lang.short xmlwriter.writeval textresponsewriter.writeval missing check val instanceof short thus bit code used thing happens binary field since val case byte get b b anybody else run seems odd known issue wondering something odd schema especially true since binaryfield write methods xml json via textresponsewriter handle base00-encoding data wondering normally binaryfield.write methods would get used whether actual problem lies elsewhere. different schemas would combine results two schemas define third core different conf separate conf/solrschema.xml set request handler dispatches two real cores. noted lot different approaches could search email list archive discussion using solr subject solr support integration providing compass-like mechanism keeping solr index sync db via hibernateeventlistener. works excellent trying build distribution trunk use prototyping noticed things fresh check-out build trunk/solr sub-dir due dependencies lucene classes done top-level ant compile cd /solr ant builds noticed run-example target trunk/solr/build.xml description show ant -p. tried ant create-package trunk/solr got error near see contrib/velocity anywhere lucene trunk tree recommended way build solr distribution trunk meantime shall use example/start.jar solr.solr.home thanks. others noted currently updating field means deleting inserting entire document depending use field might able create another core/container field plus key field use join support note http//replaced.url improvement looks like 0.x code line though see fix version. hi robert confuses suggester says based spellchecker supposedly work shards issue got configuration shards already trying leverage auto-complete quick dirty work-around would add custom response handler wraps suggester returns results fields needs merge. problems knowing possible set case seems like minor tweak call solranalyzer though understand mark iscomment setanalyzer method says required using like docnum method call tell analyzer either default standardanalyzer whatever gets set explicitly still get used case term vector. actually tagsoup isreason existence clean html wild tika ishtml parser wraps uses generate stream sax events consumes turns normalized. note shall need ant compile top lucene directory first trying solr-specific builds inside /solr sub-dir least ran trying build solr dist recently. tested impact certainly would cause index files read thus cached depending much ram available came big commercial user lucene think running something like fc0. sounds like roysolr running embedded jetty launching solr using start.jar app container newrelic installed. would use copyfield copy contents new field uses string use faceting. sir christmas card list shall fire tomorrow morning let know goes""",1004,"29,53","31,57","54,18","12,25","1,69","0,00","0,00","0,00","0,00","0,00","0,00","1,69","0,10","14,14","3,49","2,39","8,37","2,59","2,19","1,59","1,10","0,10","3,69","1,29","0,00","3,78","0,00","0,00","0,10","6,18","4,78","1,39","0,10","0,50","0,30","22,71","4,18","6,67","3,49","3,59","1,79","1,00","1,79","1,59","1,39","0,70","0,40","0,30","0,10","0,00","0,10","0,00","0,00","11,35","2,69","3,29","4,88","3,19","4,58","0,80","0,30","0,80","0,10","0,00","0,20","0,00","0,70","10,96","5,88","0,00","0,10","0,00","0,00","0,00","2,49","0,20","0,00","0,00","2,29"
"""61631242ce3a1b35bb227cfd729b9570d47c8ee74a208766faa6fcc599faaeab""","""processing html definitely use something like nekohtml tagsoup note tika uses tagsoup makes easy special processing specific elements give content handler gets fed stream cleaned-up html elements. hi floyd typically would creating custom analyzer converts words pinyin zhuyin index would actual hanzi characters plus via copyfield phonetic version search would use dismax search fields higher weighting hanzi field segmentation error prone requires embedding specialized code typically license high quality results commercial vendor first cut approach would use current synonym support map hanzi possible pronunciations numerous open source datasets contain information note might performance issues huge set synonyms weighting phrase matches sufficiently high using dismax think could get reasonable results. noticed prices showing even though got think issue line hit.vm number.currency function needs get passed something looks like number return could list values square brackets confuse number.currency get price think line needs since returns single value without brackets. never tried sending anything utf-0 solr comment issues shall run based experience date would strongly suggest converting utf-0 post solr. already fronts solr web service us pretty easy something similar use case map user list groups using ldap make required clause solr request field contains. given requirement break document separately controlled pieces would create fronts solr handles conversion could think ways using solr feel like unnatural acts general comment acls relatively easy way handle via group ids use restrict query document groupid list group ids authorized access user query converted query groupid xx groupid yy xx/yy groups user belongs. hi navina thanks confirming putall list required method supporting changelog functionality hoping others confirm range _not_ used samza system i.e used internally needed tasks true adding notes methods required used samza system changelog support vs. optional used task-specific code needed would helpful thanks. think change token frequency would cause jump quantized levels order change md0 hash obviously happen point quantizing frequencies make much less likely using tech page crawl seems working pretty well though done in-depth analysis seen different proximity hash functions ones referring thanks. hi otis learned krugle approach worked us block bots search page expose target content via statically linked pages separately generated backing store optimized target search terms extracted search logs. docs need updated believe code wrote back note escape solr uses support using bug two tokens white- note idea still issue regular escaping work around bug parser last character escape build expression looks like xxx signal escape character wrong general escaping characters query gets tricky parser shall save pain suffering since code dismaxrequesthandler added solr iirc tries smart handling escaping. hi erik missing change needed thanks much. hi jack thanks included details original question issue got index 000m records 00m unique value prefix characters long adding another indexed field would significant hoping way via grouping/collapsing query time possible thanks. note nutch try solve concerns searchers would slow rmi faster hadoop rpc less issue well handle potential summarizer problems case example remote searcher goes away gets bogged times got hit server needs summary case ran load testing though would serious getting completely wrong summary remote index updated search request happened summary requested munge count might enough pretty simple part remember issues servlet-esque objects getting passed deep shall another look afternoon poking weeks ago thanks. hi emmanuel spot room think would issues would useful able leverage solr s. believe solritas supports autocompletion box wondering anybody experience using lucidworks distro leveraging autocomplete plugin hooking solr facets curious tricks traps getting work thanks. describing web mining web crawling extract price data web pages put specific field solr using nutch would need write custom plug-ins know extract price page add custom field crawl results topic nutch mailing list since solr downstream consumer whatever nutch provides. pros/cons using cache-control no-cache response header avoid problem tracked entire thread proposal would use response codes along xml body would client deal container response would typically html versus actual thanks. hi looking using embedded solr lets extract ranked results state publish part task isoperation methods defined problematic though specifically range methods return iterators iterating lots results solr feasible newer paging support still abuse architecture wondering whether need support methods called internally tasks e.g task thus optional assuming state automatically restored changelog samza system calling putall list repeatedly dug details would example required method thanks. hi looking snapinstaller seems following copy new index directory master slave issolr data directory giving index.tmp rename temp index directory index commit send post /solr/update service new index gets use feel like must missing something seems like request middle processed step successful swap could fail due index changing underneath insights thanks. use prng mix ids form auto-incrementing field yes. started using regular dom parser coding hand switched xstream help handling solr pojo mappings seems work fine get past encoding. experience resin definitely explicit character encoding. hi christian erik mentioned appears line standardtokenizerimpl.jflex needs updated include extension b character range. typically run memory solr index update new index searcher getting warmed looking heap often shows ways reduce memory requirements e.g shall see really big chunk used sorted field see http//replaced.url http//replaced.url. another approach add additional acl field contents would list customers ids access document query implicit acl actual query idea would work case use control access source code krugle enterprise product though using ldap-provided groups line mike issuggestion. fwiw krugle started using moinmoin switched confluence general good change us especially scale scope wiki activity increased integration jira handy far different classes pages easiest way would create spaces assume possible w/the confluence config apache would set space different quickly hacked app used convert moinmoin pages markup syntax confluence switch confluence happens look around. hi shawn different use case ones covered response robert wanted call currently use embedded server building indexes part hadoop workflow results get copied production analytics server daily basis writing multiple embedded servers reduce task gives us maximum performance proven reliable method daily rebuild pre-aggregations need analytics use case regards. intermediate approach use find potentially similar files using apply accurate slower comparison determine true similarity data common get files due small text changes frequency term moves quantized bands changes uber hash get combining terms bands still get matches hashes individual number matching fingerprint values. took look search results seems like word red shows description tag every sure extracting color information smart color-specific keywords found associated text. issue ran daily rolling log files maybe missed find functionality jdk logging package however log0j advocating change noting worked around leveraging resin issupport wrapping logger set daily. try uncommenting lines see fixes problem. code see seems using fsdirectory another layer wrapping going tom ever get useful results testing curious impact various xxxdirectoryfactory implementations batch indexing thanks. hi lucene index generated solr optimizing able view using limo download mac try use luke fails luke complains /index/_jdi0.f0 file directory files downloaded follows known issue luke solr-generated lucene indexes references found file directory involve index corruption far tell index fine thanks. hi got pattern document call xy turn two tokens xy approach could use patterntokenizer extract xy custom filter returns xy next call caches next result could extend patterntokenizer return multiple tokens match though figuring specify schema seems another approach would require custom code thanks. hi moving towards embedding multiple solr cores versus using multiple solr webapps way simplifying build/deploy getting control startup/update process would hate lose handy gui inspecting schema importantly trying queries explain turned anybody tried dual-mode method operation thoughts whether workable issues would taken quick look supporting java code ideas would needed hoping easy approach whacking admin support code thanks. well ultimately heading towards single multiple embedded solr cores case could .jsp-based gui/admin functionality peacefully co-exist use embedded cores description roadmap solr gui example assuming files still exist going forward become much gui layer top new/beefed plan eventually get html using json responses request handlers thanks. hi have got ancient lucene tokenizer code m trying avoid forward-porting not think isequivalent solr specifically isapplying shingles output something like worddelimiterfilter e.g mysupersink gets split super sink shingled re using shingle mysuper super supersink sink not follow wdf single filter shingles not created across terms coming wdf ispieces generated wdf actually way make work solr thanks. hi renee mike right question post users list yes separate setconnectiontimeout used though familiar possibility ping response handler responding connection established getting data back. turn facets query facet=true facet.field= shall get back distinct values though might play settings e.g facet.limit=0 get results need. hi yonik ah explains little seen resin parameter used inside init method typically specified using tag know resin-specific yes would easier update resin runs unpack .war kind pain way knew get working versus jindi approach got special reason ask ongoing support init-param technique. available cores dynamic cores requirement custom code wasted. hi feel like must missing something working customized version supports distributed searching multiple local cores assuming support searchcomponents handler needs create/maintain responsebuilder passed various methods responsebuilder finished list shardrequest objects requests received responses shards inside shardrequest responses list shardresponse objects contain things like solrresponse solrresponse field shardresponse private method set package private appear like easy way create shardresponse objects searchcomponents expect receive inside responsebuilder put custom package shardresponse call setsolrresponse builds run locally deploy jar code runtime get illegal access exception running jetty make work re-building solr.war custom pretty painful thanks. way mean configuring current extractingrequesthandler fundamental issue solr uses tika prevents extractingrequesthandler modified work way seems like useful configuration regards. hi would copied/pasted schema fields testbed schema based version solr using back dark ages version handy comment warning changing version fields multivalued default would checked docs realize behavior changed days would used http//replaced.url examine field would seen thanks. useful use morelikethis support see http//replaced.url attached patch. ago similar issue least back think possible use solr iscopy-field support create boosted version field copied field multiple times. hi frederik figure solution problem asking recently ran similar problem similar setup shards server occasionally query long time occasionally see timeout exceptions http requests e.g restarting jetty seems clear problem temporarily looking code solr handles distributed requests got interesting smells would surprised issue related using regards. hi frederik directly run issue solr experienced similar issues related context case custom made solrj requests generated aggregated/analyzed results load testing ran different issues load test software issue scaling assuming case seen happen e.g limit max parallel connections client used talk solr needed tune solrj settings httpconnectionmanager heavy load running free connections given got shards request going spawn http connections know top head manages connections whether possible tune stack trace sure looks like blocked getting free http connections needed optimize configuration jetty jvm gc etc lots knobs twiddle better worse. think need create separate background process least case web crawling challenge efficiently use samza process simultaneously fetch many urls increase complexity process iscode wind manage either multi-threaded async fetch state hadoop-based crawlers limited number parallel reduce tasks fetching see nutch bixo examples e.g fetchbuffer another project involved past. hi running problem queries distributed among multiple shards return binary field data properly hit single core xml response http request contains expected data hit request handler configured distribute request shards xml contains b b looks like wind getting .tostring data actual data anybody else run done fair amount searching hits yet next step create unit test solr nobody raises hand walk thanks. general yes subset terms occur frequently remove terms easy longer search either part query phrase combine common terms following terms works well bit complex significantly grow index either requires data analysis generate target set common terms. area might issues date range queries many docs run oom errors recent thread yonik others good suggestions ways avoid problem know impact would merging results use date ranges guessing low yonik would know best well 0-server configuration would work would 000m docs/server large number even data/doc small 0k real performance going heavily impacted nature data types queries shall need think distribute data avoid performance constrained worst case searchers nutch wound add termination logic avoid long-running queries clog things primarily dealing load best first step create single solr representative data see well performs issues going around limits box 000m docs versus distributed nature though keeping servers alive happy significant ops task finally would decide early whether search query words ok result set happens missing doc server timed looking query-type solution solr would less interesting. csvloader user found/fixed bug involved active development/maintenance piece code james make progress merging support csv dih great. karsten said providing detail actually trying usually makes better helpful/accurate answers guessing search key value right create multi-value field custom indexed stored indexing add entries custom set analyzer strip index key e.g. would first try setting post explicitly utf-0 matter appears resin tomcat issues properly handling form data without matter issue configuring tomcat see http//replaced.url patch applied may 0th year work around appears bug tomcat assume version solr patch. hi sorry noise finally realized running using java code enwikicontentsource lucene benchmark explicitly set fields push results solr. use characterencodingutf0 think shall get back stream utf-0 bytes db know mysqlxmlfield isstart array bytes returned jdbc call create string array using utf-0 encoding use bytes directly writing xml best thing make sure xml send solr starts line make sure converted text xml fields wrap fields. hi hector low query rates using shards approach improve performance multi-core cpus solr cores setup distributing requests effectively use cpu cores parallel request spread shards across spindles maximizing i/o throughput issues approach binary fields work results back b versus actual data short fields get java.lang.short text prefixed every value deep queries result lots extra load e.g 0000th hit shall get shards hits collected/returned dispatcher though unique score returned case followed second request get actual top hits shards something wonky way distributed http requests queued processed load see ioexceptions always n-0 shards succeed shard request fails good reproducible case yet debug. general issue months ago generate reports things like scm commit activity given day larger customers users multiple timezones timezone use wrote blog post http//replaced.url short answer ultimately decided use utc times server report api ui least heinous various options. hi christian encoding using pushing documents solr specified xml post request separate issue mime-type use post using latest scripts solr characters look like xml pushing example encoded two surrogate characters instead code point extension b set xml parsers handle correctly source similar issues seen potential issue xml parser used updated handle extended unicode code points older parsers still failed handle example. hi tommaso slaves configured use vip talk master easy dynamically change master use via updates. related item might check jmaki http//replaced.url jsp-based amount jsp-generated content pretty easy add support dynamic. hi otis working project speak stefan friends going live separately something independent solr/nutch view search plumbing usable multiple environments makes sense view replicating core solr nutch functionality sucks sure outcome. error either schema.xml file messed might still need uncomment lines beginning file ones say uncomment trying use resin version even though using later version resin lots issues xml parsing. confused answer assuming based page referenced url provided approach textprofilesignature would generate different md0 hash single letter change change resulted change quantized frequency word uncommon word would even show signature. low qps multi-core servers believe reason multiple shards server provide better parallelism request thus reduce response time. idea thought given document/field set would contain text single language write special token language e.g analyzer add my-special-token-prefix-esperanto token field query time assuming know language make required term. depending complexity solrj could solution see section talks solrj provides apis create. would use data files. hi list working improving performance solr scheme cascading supports generating solr index output hadoop job use solrj write index locally via embeddedsolrserver mentions using overwrite=false csv request handler way improving performance see http//replaced.url removed support solrj deemed dangerous mere mortals question whether anyone knows much performance boost really provides hadoop-based workflows straightforward ensure unique key field really unique thus performance gain significant might look figuring way trigger lock re-enabling support solrj thanks. hi especially yonik grins trying solr resin following copied solr-nightly.war /webapps/ cloned/edited resin.conf file include mapping solr adding started resin created /webapps/solr-nightly directory expected tried use solr complained finding see stack trace email realized solrconf folder included default .war added inside /webapps/solr-nightly directory result tried moving uncommented lines file yonik indicated necessary get resin work properly solr change anything figured would ask correct way configure solr resin rather continue thrash would help resin jock far usage limited futzing resin config file add simple thanks. sort answering question seems like need get current core use instantiate new solrcore exact config documentation solrcore isconstructor says core already exists stopped replaced unclear whether graceful swap like hard shutdown old core thanks hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor isdatadir gets used construction changing coredescriptor isdatadir effect since would go changing datadir core multi- thanks. right baking fine-grained level security information bad idea example worked pretty well code search krugle project number groups granted access rights file project would inherit list groups user logs get authenticated via ldap set groups belong returned ldap server becomes fairly well-bounded list terms query acl-groups field file/project document set boost portion query. large index would recommend separate solr installation use update/commit changes use snappuller equivalent swap live search unlikely switch lucene tune new parameters control memory usage updating. believe mistake recent email thread list. based experience using jira manner would agree mike automatically assign fix go path wind steadily growing wave deferred issues constantly getting pushed next""",3014,"40,19","34,44","55,51","9,89","1,16","0,27","0,10","0,17","0,00","0,00","0,00","0,90","0,03","13,30","2,65","2,36","8,46","2,22","2,19","0,93","0,63","0,33","3,28","0,50","0,03","5,11","0,00","0,03","0,03","7,70","5,51","2,19","0,23","0,53","0,46","22,20","4,98","6,83","3,25","3,12","1,36","1,43","0,80","1,16","2,16","1,39","0,17","0,40","0,63","0,17","0,40","0,00","0,07","12,44","3,58","2,85","4,74","2,59","4,35","0,70","0,10","0,63","0,07","0,20","0,23","0,27","0,46","7,37","3,75","0,00","0,00","0,00","0,00","0,00","1,69","0,07","0,00","0,00","1,86"
"""9a60aceacd1b17a0e6a9d5926fbc40ec9b2df5679a5c3c6c967c0d0fb8475e20""","""tried compiler much different lib wanted sure lib future. hi tried apply with common sense files many differences would incrementally would better easier apply small changes make mostly introduced help reorganize patches quickly agree improved need check clang-format versions available right new support something like //clang-format comment could check new opts available personally hate clang-format makes short yet believe consistent even perfect automatic tool better chaos regards. unnecessary mostly disrupts git blame mean new parts keep old ugly inconsistent really believe importance constant refactoring maintenance means iterations code naturally changed new coding standards mean naming convention example lines functions untestable fork split split split introduce low level unit tests changes way code looks yet pretty confident move good direction developer never afraid making changes existing code tests ensure nothing gon break tests provide tests needs changed software rot became unmaintainable way tests changed provide future project although like live regards. cmake generate vcxproj files cmakelists.txt sure requires something specific cmakelists part visual projects could removed repo generated example part release keeping repo really makes leats separate build systems maintain compiler cmake could think releasing thrift.exe using visual would remove dependency mingw. i0 byte creates inconsistency. require lua support try adding without-lua. got shall commit soon check impl compact protocol. hi going travis builds submitted couple pull req failed g++ internal compiler error stop pull reqsts merged left still shame marks requests failed anyone able reproduce issue seems happen mostly compilingtransporttest.cpp maybe machine runs memory compilation single file takes 0.0gb machine maybe split smaller compile units. submited patch maybe shall get accepted. would go aliases including short long i0 keep everything consistent best. dense protocol fails tests support messages compact protocol fixed awaiting results merge. seems files yet uploaded apache dist server status beeing released staged change web page wait files available best regards. somebody started need oneway calls half normal call pass-through server oneways really nice feature thrift would nice make work properly thrift already defines oneway additional message use. suspected something break submited pull req see travis say shall look hope explaining comments issue devs. favour keeping things simple possible add new flag current behaviour intuitive simple especially need ask someone add new assignees makes process complicated requires two persons get hold anyone able edit jira users leave assignee usually issue result loose information jira simplification suggested removal close transition hope commiters join discussion shall least try suggest update doc/commitets.md mention anything assignee. yes work vs easiest way use cmake generate vcxproj done automatically pretty sure changes properly reflected make build changes build system like adding new file done directly cmake files vcxproj regenerated still imho easier remembering apply changes build system least twice. may remember working c+0/cpp_v0 gen/lib http//replaced.url yet put hiatus time ago maybe next month shall find time finish. hi randy think could somehow merge cpp0 efforts old branch put little hiatus yet think parts could useful best regards. hey keep old c++ lib least c+0 lib would break iface compatibility avoid confusion point agreed use cpp_v0 new lib user would ask c+0 perfect solution still least solution current c++ lib renamed cpp_v0 gen cpp alias cpp_v0 branch probably little bit outdated not merged rebased contains full copy cpp_v0 lib std :shared_ptr std :thread instead boost counterparts boost usage reduced optional fields using boost :optional need check maybe compilers already support c+0 std :optional could switch c+0 probably released cpp_v0 become simplified threadmanager tested think last thing missing best regards. somebody check missed lang. shall debug shifts masks etc hopefully programming message everything ok. hoped could find anything thrift something apache wide seems projects searching apache standards saw like mention design etc coding put bracket importantly put new feature less text code devs used read code specifications ever saw specs thrift langs design part standards could extracted general shared top level docs rules like keep functions short apply almost langs issues lib public api comments lies. think easier store info jira reference jira issue commit additional info could jira especially simplifies providing info user list etc searching even change commit message personally like client part would prefer component still decide jira integrates workflow change information jira and/or commit message could leave original authors commits could make contributors happy authored commit apache repo. looks like :methodname compiler code look like :method_name definitely prefer first currently playing around compiler use lib assuming recent compiler code move toward look. assuming aliases would present parsing time prefer ix notation like explicit yet imagine lot users bits bytes int make idl look like java/c would try detect typedef i00 int constructs next step would detect keywords supported languages might good idea would require lot work especially adding new lang maybe reserved members names doable stig errors think problem appears generated code langs typedef constructs like c++ idl leads something like typedef int00_t int tries redefine. shall try merge/rebase master include recent changes c++ lib c++ v0 lib probably weekend hope shall find time c+0 quick answer point thrift would introduce std00 std00 etc long answer started branch discussion lead following thrift c++ library best modern c++ updated along new compilers new language library features etc library named cpp v0 mix current new versions c++ standard thrift able make library compatible current newest standard std00 std00 point library become library kept common subset c++ compilers features library enable thrift older outdated environments keeping multiple libs cpp00 cpp00 cpp00 etc would add big maintenance cost thrift best regards. sounds great although prefer scons-type tools everything flexible/portable autotools good assume cmake replace automake completely support two building systems like compiler right huge best. fixed hopefully generators shall peek/statsprocessor etc commits go branch. oh really hope langs standards like please http//replaced.url added tasks consistency especially going roger issuggestion lib/ lang/readme.md would nice consistent lib layout avoid mails dev like saw lang x guidelines keep standard ones regards. hi please check current master think problem reported solved http//replaced.url best. soo see two potential changes result discussion add dictionary reserved keywords validate variable names a. global dictionary containing merge keywords thrift supported langs make idl portable like level might little easier maintain yet might surprising users changing gen x lead thrift errors 0a 0b would expect error /warning like int used c++ java. updated generators langs unfortunately fill new seems oneway message never fully supported. today links working idea shall forward dev somebody higher permissions might needed. good idea ignore gcc failure. change way things encoded fixed decoding procedure protocol version change needed imho still old server old decoding procedures may support oneway methods sent new clients compact protocol""",1134,"31,50","32,45","61,02","15,08","1,85","0,00","0,00","0,00","0,00","0,00","0,00","1,85","0,09","13,23","4,23","2,56","6,88","2,82","4,06","0,53","0,35","0,44","3,88","1,15","0,00","4,50","0,00","0,00","0,18","7,67","5,91","1,76","0,44","0,35","0,62","24,16","5,03","4,59","4,41","5,20","2,03","2,03","1,15","1,32","1,41","0,97","0,18","0,26","0,62","0,00","0,62","0,62","0,00","15,43","4,50","3,00","7,05","3,88","4,67","0,26","0,00","1,76","0,00","0,00","0,35","0,00","1,32","9,96","3,79","0,00","0,53","0,00","0,00","0,00","0,53","0,18","0,00","0,00","4,94"
"""e1081946adf1d4efd0c90d6d8b1923924184bdabb0f7c60c34facbf404c24e98""","""thought yet pretty busy studying learning basics curly brackets keyboard hint curly brackets italian keyboards love idea open source pretty sure skill-wise community let us see project progresses next weeks regards. hi julian notice map function emits every letter author isinstance entry db yields following results key null value using group=true clause increased clarity yields key value key value key value key value key value key r value key x value could another factor contributing slowness view. great hear best luck couchdb ltd.. hi zhengji fact working something similar think gnucash far tested following concepts storing accounting entries array alongside original storing chart accounts database source entries together order obtain general ledger far good absolute beginner couch web done volume testing conceptual level couchdb looks like clean powerful solution problems imho regards. hi krishna run json json validator http//replaced.url says invalid character rec.add doc return rec would try get json jsonlint first pasting expression view hope helps regards bruno. hi robert thanks kind reply done advised removed update_notification restarting query db reference view continues hope mapping view etc correct get back hmmm couchdb apparently points right db pbo_documents stops somewhere road wrong thanks advance congrats fro great software regards bruno. idiot restarted couchdb-lucene running everything works expected thanks patience regards bruno. hi robert thanks expressing interest office right provide procedure soon back later day regards bruno. robert thanks help pulled git yesterday couchdb-external-hook.py assume latest fact download provided two files inside /tools folder home folder look tomorrow try put together list software versions use pretty late europe thanks regards. hi everyone querying view continue getting invalid utf-0 json problem get away database contains two documents attribute view using attribute key like doc.member tinypic http//replaced.url contains view appears futon try rebuilt yesterday trunk 0.0b000000 strange thing aware tests failing changes oauth rev_stemming thought authorization issue read enacted everything found topic still admin party way mac snow leopard pointers help thanks lot bruno. hi everyone sort beginner trying hand couchdb-lucene followed instructions github various blog pages around kind stuck ok since playing around version couchdb welcome version 0.0b000000 installed following instructions github seems running curl http//replaced.url get seemingly satisfactory response couchdb-lucene welcome version build indexing view like document contain property suspect things fail hookup couchdb couchdb-lucene modified .ini file like must something wrong nothing happens even unable find couchdb-lucene logs supposed changed permissions /target/ snapshot/indexes folder still see changes must something really stupid need help btw love couchdb think tight integration lucene recipe great applications user-experience congratulations everyone involved regards bruno. thanks lot escaping works indeed single quoting however need study wiki understand going regards bruno doc.member. hi everyone intend work couchdb-lucene downloaded latest version yesterday everything ok initial setup able obtain correct responses examples read would like proceed complex indexing getting behaviour explain happens change index functions views and/or alter view names i.e properties object start getting code reason no_such_view messages getting messages changed/altered views querying unchanged views within _design document debug level logs bear explanation least noob eyes cleaning curl -x post http//replaced.url help tried deleting index files help either thing seems work starting afresh new _design document design picked unstable state yesterday would really appreciate help regards bruno. exactly background accounting seems fact journal entries stored along original document makes incredibly easy audit track books turn enabled schema-less nature couchdb fact makes easy record data original document _attachment instance together happy see someone else thinking along line regards. yes works misinterpreted paul iscomment simply double quotes single quotes shows beginner thanks taking time point error regards bruno doc.member""",626,"41,73","38,34","63,10","11,02","2,72","0,32","0,16","0,16","0,00","0,00","0,00","2,40","0,00","10,38","1,44","1,76","7,51","0,80","2,40","1,28","0,48","0,32","3,35","0,96","0,00","6,87","0,00","0,00","0,00","9,90","8,47","1,44","0,00","0,32","0,32","17,73","4,47","3,35","2,08","3,67","2,72","1,12","1,28","1,12","4,15","3,04","0,32","0,64","0,64","0,32","0,00","0,32","0,00","13,58","3,83","3,35","5,91","4,31","3,83","1,76","3,04","1,92","0,16","0,00","0,64","0,16","0,64","10,22","4,15","0,00","0,00","0,00","0,00","0,00","2,08","0,32","0,00","0,00","3,67"
"""25d53f5bed4ab97165996b0526a5a9bbc19600442faa72d962fff32cac01dd49""","""hi christian tried result build failure nice colored system seriously different imho misleading error message. sap eventspies used well useless would welcome review convoluted incomplete listener interfaces. point failing build lib 0.0-snapshot pom file result app failed build exists another lib 0.0-snapshot parent 0.0-snapshot abides app-to-lib version range note relationship parent lib dependency relationship instead parent relationship failing case build app correctly succeeds exists another lib 0.0-snapshot dependent parent 0.0-snapshot version range. oh sure parent allow version range hourah thanks let see option ok developers case think reason fill jira entry mean iterating set possible resolutions fail set fails acceptable resolution found fact happen failing parent relationship replaced failing well dependency relationship understand root cause failing build pom considered error stops whole process whereas failing resolve transitive dependency marks particular resolution path unacceptable continue search imho behavior depend relationship parent vs. dependency behavior cases vote continuing version range resolution thanks. higher resolution path build would fail well regards. looks aether thought manage kind issue direct dependency dependency selector given chance exclude dependency graph hint possible configure aether ignore lib 0.0-snapshot resolving version range thanks. hi says suggestions overcome issue explanation let projects three minimalistic sample projects http//replaced.url happens time versions bumped projects deployed remote repo parent 0.0-snapshot still remote repo parent 0.0-snapshot still remote repo parent 0.0-snapshot still remote repo parent 0.0-snapshot still remote repo maven tries build pom lib 0.0-snapshot fails parent 0.0-snapshot available anymore however imho build app fail another dependency resolution lib 0.0-snapshot parent 0.0-snapshot fact relationship parent lib dependency parent-child relationship build app succeeds even transitive dependency parent 0.0-snapshot resolved note app build successful day lib 0.0-snapshot turn evicted parent 0.0-snapshot still remote repo suggestions overcome issue builds failing every day build farm reason thanks ps content mail duplicated http//replaced.url. fyi use-case eventspies sap exactly described igor use event spy instrument hudson/jenkins instance monitor builds maven project thrown hudson/jenkins monitored customization maven distribution dropping files lib/ext setting maven easy upgrade regards""",353,"44,12","39,09","52,41","7,08","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,08","1,13","1,70","4,25","0,57","3,40","0,28","0,57","0,00","3,40","5,10","0,00","10,20","4,82","0,00","0,28","10,48","5,67","4,82","0,00","0,00","4,25","13,88","5,67","3,40","0,57","1,70","1,70","0,57","0,00","0,28","1,70","0,85","0,57","0,00","0,00","0,00","0,00","0,00","0,00","9,35","1,98","2,27","4,25","6,52","6,80","0,28","0,00","0,00","0,28","0,00","0,57","0,85","0,00","11,33","2,83","0,00","0,00","0,00","0,00","0,00","5,95","0,57","0,00","0,00","1,98"
"""0d57a6bd7080012afdf24dc0d5ff3095d0b7a90c793c4a35d2739de2dedcbf01""","""documentation http//replaced.url specify names/positions fields csv file ignore fieldnames seems like would solve requirement different layout could specify mapping import could handy provide map versus value map updatecsv supports could use header provide mapping header fieldnames schema fieldnames. hi would started using embedded solr back via patched version in-progress code base wondered paragraph said given current state solrj expected roadmap solr general would guidelines special circumstances warrant use solrj know back namely multiple indexes run multiple webapps handled multi-core generating lots http traffic handled dih maybe solr search system since integrated system hands customers restart container option anything got wedged might still issue commonly compelling reasons use solrj thanks. hi arno need add boilerpipecontenthandler tika iscontent pretty sure means would need modify solr e.g trunk tikaentityprocessor.gethtmlhandler method would try something like return new boilerpipecontenthandler new contenthandlerdecorator though quick look code curious use. hi otis exactly something similar nutch searches using ehcache http//replaced.url store rewritten query string serialized xml response way dependencies stable searcher/doc ids nutch reference get remote searchers depending entries cache hit docs xml representation storing xml might. hi savannah comments scattered in-line store xpath expressions text file strings load/compile needed definitely yes using tagsoup clean bad html definitely yes needing per-site rules typically xpath optional regex needed extract specific details common sites powered back-end often re-use general rules markup consistent kind thing use bixo http//replaced.url requires knowledge cascading hadoop order yes would separate job field index though often job titles slight variants would probably work much better automatically found common phrases used otherwise get senior bottlewasher sr. actual format extension b characters xml posted. normally would say like getting swap based settings 0gb jvm space used 00gb box confirm nothing else using lots memory right top command showing swap usage right encounter slow search times top command say system load cpu vs. i/o percentages. hi sandhya would post question replaced email.addr.es mailing list include details document confidence level often low enough tika assume good match thus report language. hi erik let us say grins different field besides used autocompletion would places would need hit change field besides terms.fl value layout.vm example asking trying use latest support index uses product_name auto-complete field getting auto-completes happening see solr logs requests made /solr/terms auto-complete look like would expect work seem generating results odd try curling thing use would consider minimum set parameters get expected xml response ideas wrong thanks. curious mean utf-0 complaint mean thanks. interested need sorted output faceted browsing alternative output formats something along lines merge xml responses w/o schema proposal would fine much better would use hadoop prc versus http call sub-searchers assuming better performance might fewer connectivity issues leveraging work done embedded jetty example anybody data points relative performance master schema main search server could get distributed remote searchers would part thanks. hi trying morelikethis support getting odd results realized unless fields used similarity lucene re-analyze field using standardanalyzer case quite different using solr schema first note anybody using morelikethis make sure specify termvectors=true solr schema fields passed query mlt.fl parameters second note wiki page example schema might include reference termvectors field attribute example sample schema says made think initially attributes http//replaced.url make mention termvectors termpositions would edit page currently section talks attributes common ones thanks. shamed taking position vote earlier real experience slf0j indirectly via use jetty know would _something_ logging hood use log0j everywhere finishing earlier work creating jul logger bridged log0j another option would rather avoid. hi got field defined solr schema always contains two fixed values documents get added boost supplied varies max query field expecting ordering results would match boost values longer specifying omitnorms= true field still get seemingly random results use full search interface solr results two documents different boosts looks like expecting value different different document boost values missing thanks. currently links seem wind pointing api-0_0_0-alpha versions expected e.g search streamingupdatesolrserver first hit streamingupdatesolrserver solr api follow link get page http//replaced.url. difference free speech free beer see http//replaced.url. think concern could addressed set defaults fall back try following sources order get value found try contextroot namespace-specific property found pom.artifactid think plugin always able override project-level value first common system property like contextroot would allow multiple plugins configured property finally currently default. preparing patch submission maven-eclipse plugin fix bug enhance functionality setting context-root wtp0.0 project question pom.xml user specify right wtpcomponentwriter grabs webappsrc value maven plugin seems like logical place put contextpath/contextroot parameter time probably appropriate put property used plugin otherwise property could go eclipse plugin configuration feels bit odd reason""",802,"44,56","34,91","55,11","12,09","1,50","0,25","0,12","0,12","0,00","0,00","0,00","1,25","0,00","14,84","4,11","2,37","8,48","3,12","2,24","0,87","0,62","0,12","3,24","1,37","0,00","3,99","0,00","0,00","0,00","6,11","4,74","1,37","0,25","0,37","0,25","24,69","5,11","6,98","5,36","4,86","1,87","0,75","0,50","1,12","2,00","0,62","0,75","0,37","0,50","0,12","0,00","0,00","0,37","11,47","2,00","3,37","4,74","1,87","3,74","0,12","0,12","1,37","0,00","0,00","0,37","0,00","0,75","10,10","4,24","0,00","0,00","0,00","0,00","0,00","2,24","0,25","0,00","0,00","3,37"
"""11411a35018100822513329188627157767176b867623a5d794fb4ba2b98edae""","""problem original poster two years ago solr using shard searching performing sharded query would get empty missing results documents querying shard individually worked anything shards parameter yielded result documents able get results back updating schema include problem seeing solr formulating queries go get records shard including square brackets around ids asking e.g delved solr code saw query string formed querycomponent.createretrievedocs simply calling tostring unique key field value document wanted get value objects somehow arraylists something like strings annoying square brackets showed via tostring emphasizing schema field single-valued lists would hopefully stop appearing think least brackets went away isrelevant querycomponent code check depending may need tha simple tostring comment seems fit theory. outsider thing change seems completely sensible follows try treat dependency plugin versions words let 's merge focus work debuggint lib differences. would someone able fire services say basic zone host. thanks jean-marc n't use cloudera manager. unfortunately 's fair amount larger organizations use n't ignored reality imho. would check copyfield target field something updaterequestprocessors copies baring two field return put regards http//replaced.url. since going spin new apache bigtop rc0 see inline sounds like great. thinking little problem rest api solves honest agree completely n't think rest layer provides feature/function cql valuable except cases like described may common honest get excited creating separate project started thinking value-added services could delivered like capabilities riak etc think way components implemented believe might common needs reusable e.g maybe change spin bit focus services layer top cassandra instead rest interface services layer maybe change sign rest bigot. hi jason talking embedder stuff week looking batch jira issues filed time back n't really sure still problems could give component thanks. hi carlos something sequence changes plexus-spring yesterday started breaking archiva build validation errors change need make get things working n't changed svn redployed old version code latest snapshot repair build thanks. going leave couple reasons currently using maven-0-snapshot either needs sorted rolled back problem recently added pregoal clean clean thanks. hi stefan thanks reply authentication works however getting authorization error curl error unauthorized reason authorized access added reader list replaced email.addr.es someone 00example 0e.com futon able login email address successfully authorize command line curl idea thanks""",376,"31,33","34,57","60,37","13,56","2,66","0,00","0,00","0,00","0,00","0,00","0,00","2,66","0,00","13,56","2,66","2,39","8,51","1,86","3,99","1,06","0,27","0,00","4,26","0,80","0,00","5,05","0,00","0,00","0,27","8,78","6,12","2,66","0,00","0,27","0,53","22,61","4,79","5,59","4,52","5,05","1,06","0,80","0,80","1,86","2,39","1,60","0,53","0,27","0,00","0,00","0,00","0,00","0,00","13,30","4,52","2,13","6,38","3,46","4,26","0,00","1,06","1,33","0,00","0,00","0,27","0,00","0,80","9,57","4,79","0,00","0,00","0,00","0,00","0,00","1,60","0,53","1,86","0,00","0,80"
"""11c430bae59cd170a1bd2f25110f9c28cc855382f89c010b23760d1af6263d8b""","""thank much gets moving. changed zkhost use root level works went using. hi using solr cloud zookeeper several shard setup try use solr cloud bring shard setup seems load fine without errors however go web interface click cloud exception thrown happens shard shard setup anyone seen thank. thank reply work around available could related number external zookeeper servers. outstanding task http//replaced.url done provided wouldn't objections serious considerations make losing compatibility elsewhere. think easiest/simplest/best thing would grab already sorted data files hbase re-write accumulo's rfiles empty visibility labels. fyi jaxws spec specifically avoids mention xmlrootelement beans java first case probably due issues seeing spec really it's top level element names thus need use information instead less ignore xmlrootelement annotation xmlrootelement xmltype attribute you're kind ok non-anonymous point however attribute blow things pretty badly definitely outside stuff jax-ws tck would. like foam vein spume fave. fyi wouldn't see attached file chance commit svn index.html thanks. chunking turned connection authentication currently happen httpurlconnection implementation hides needed information looking jakara commons looks quite promising works like web page says okay it's user wants it's gets surely please keep us line issues familiar jax-ws i'm learning. noticed component depends cxf-rt-bindings-http system regards. discussions make sure you're meeting requirements 've added box scenario understand things would work various requirements andrea couple questions things need done list hoping could answer i'm sure means elaborate something like combining httpdestination httpserverpolicy object configuration bean component put together list scenarios need big case seems you're using jax-ws static apis need customize transports/etc rough idea works head feel like kind nebulous yet. yup it's relative path erlang it's point view fine see couchdb reading files successfully debug mode ran file access logging tool procmon seems find couch/lib/couch-0.0/priv/lib/couch_icu_driver.dll prior tanking i'm seeing errors configure make makefile generate http//replaced.url would appreciate guidance looks like last hurdle thanks. i'm sure following exception expected cause build fail expected makes sense add debug message something like following badargumentexception. hi still couple issues left jira. likely case using wrong snapshot doxia vincent deployed new snapshots doxia lately probably get new ones. long wait someone create complete solution ever feel accumulo user better. 're pretty clear commit-then-review lazy consensus really issue regards commits said still think ignoring warnings best course action compiled warnings command line wouldn't see resource leak warning java voted use java wouldn't issue make move check warnings present building command line wouldn't sufficient please let know added following configuration block. thanks corey. github user mikewalch commented issue would like merge today objections. ugh github decided earlier comment goes whole change line wanted selected vendor ibm set otherwise set -dcrypto.secure.rng.provider=ibmjce looks backwards. note wondering suggestions deal laundry list provided dependencies accumulo core writing packages bit ugly using accumulo start maven utilities automatically dissect provided dependencies make included. log0j configuration ultimately owned user enforce custom appender wouldn't think rely anything ~/.dt either. understand vlad correctly saying operator setup checks output methods threads different operators running potentially different nodes and/or processes connection. great thats thought suspect might seeing otherwise app anything needs done wrt offset management kafka reader app ensure works correctly developer need decide kafka reader app moves kafka topic offset forward acknowledge messages read point successfully written cassandra. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. ok. see using ui application gets suffixed int values make unique case way retrieve current running job within application use case needs access application details application track certain things retrieve dt.attr.application_name set within application regards""",625,"22,32","34,40","58,56","13,44","4,48","1,28","0,64","0,16","0,48","0,00","0,00","3,20","0,00","17,12","4,96","2,08","12,64","1,92","2,24","0,80","1,12","1,12","2,72","0,32","0,00","4,48","0,00","0,00","0,00","7,84","5,92","1,92","0,16","0,16","0,48","23,04","4,80","4,96","4,32","3,84","2,56","1,44","0,80","1,44","3,36","2,24","0,48","0,64","0,64","0,32","0,32","0,00","0,00","11,36","4,32","3,04","3,52","4,16","3,68","1,12","0,64","0,32","0,00","0,00","0,64","0,00","0,64","14,72","5,92","0,00","0,00","0,00","0,00","0,00","1,92","0,32","3,36","0,00","3,20"
"""0c88527e76ab30d41d0b192c43dfa142bb2212fafffd51b02f40c3ce9ded574a""","""philip nelson wrote use injection techniques add logging sounds eh interesting would elaborate. sure msdn library says fileshare.read allows subsequent opening file reading change fileshare.readwrite allows subsequent opening file reading writing might better luck hi quick question using rollinglogfileappender webservice logging works whenever try open log file reading another process get sharing violation log file open file would enable minimallock way use exclusivelock another process read log file kind regards. rebuilt log0net library strong key message log0net missing strong project. browsed log0j isdocumentation little came across file appender listens port certain signal comes rolls file i.e closes current file renames starts new file windows alternative sending hup signal make work would sort external log rotation tool complexity log rotation tool could arbitrary could simple windows think solution. would suggest try find locking file tool handle freeware tell split sec see another instance log0net someone/something trying read file latter case lucky specify reader open file read-only shared mode blocking gone case always good idea see problem really changes. well wrote log0net-0. reference strong file assembly info assemblyinfo.cs use attribute assembly replace empty string file. ok urban understand think find looking inside framework though question since use short form format specifier r mean use old version using took find list format specifiers way solve problem would use timestamp r process log file separately would quite easy calculate differences using excel example really small note maintainers nicko text manual page mentioned says conversion specifier starts percent sign followed conversion character correct. interop component vb components use interop component error tracing logging interops handle config files well hence using xml file config file load set rollingfile appender properties understand like app.config file put log0net configuration application put log0net configuration using normal log0net configuration syntax separate file e.g logging.config configure application simple line main program xmlconfigurator.configure new make sure use configureandwatch absolutely difference log0net reading file code reading specially-formatted file besides course upside reinvent whole configuration code. since currently way conditional settings config file would say implement condition code clean way would use custom property e.g friendlyappdomain set startup either current appdomain predefined constant depending appdomain looks use property configure file would still make application fairly flexible comes configuring file future still change filename anyway without recompiling personal opinion yeah ask anyway cleanest solution today would different config process imagine someone else know log0net make changes app probably go source diving order figure mechanics behind file naming scheme two config files less elegant completely straightforward him/her understand future might able two appenders different configuration inherit common properties virtual appender something like discussed. tested chainsaw java web start great thing liked production environment web application would much appropriate anyone experience thing favourite personally running apache/tomcat considered. yes reason sometimes takes generate argument log.debug code looks something like would course benefit using instead since time-consuming generation message occur log string literals suppose point using. hi guys feel little stupid matter much browse sdk zilch application detect internal error occurred see case bad configuration file detected testing logmanager.getrepository .configured detect errors say fileappender reason unable open file vague memory seeing noappendersconfigurederror find let us turn question something general would think examples good practises implement personally would like series tests startup app make sure people running applications skilled necessarily developers get decent information something happens. really simple solution loggers create note logger longer static counter keep track instances could course replaced something sophisticated naming logger. sounds good thresholds/filters distributed loggers log events thresholds/filters applied logging server injected remoteloggingserverplugin case need simple way distribute logger-specific parts config file distributed servers btw thanks excellent job support mailing list simply beyond comparison wrote. thinking along new requests features rolling file appender comes really done logrotate unix tool specialized rotating log files today googled around little astonishment realized seems like made windows port logrotate really strange apparently logs files opposed event log database windows way servers log files like iis generally include rolling mechanism reducing need still store log files matter many variables think set limit rolling file appender leave fancy parts external solutions even means us build log rotating opinion rolling file appender already grown complex several configuration parameters whose side effects cooperation parameters hard foresee without reading source anyone knows windows port logrotate please shout. sorry line became complicated necessary isget say. timestamp since start application creation logging event. well ilog interface isxxxxenabled methods solve situations need address current log level. dear question general kind use log0net system architecture involves remoting consider example enterprise system consisting webserver w companys dmz application server inside business logic components connects database d. sometimes business logic components need call components server hosts kind third-party software calls w using .net remoting calls using yes realize reveal person previously forced work dcom world question still still .net world several reasons wanting distribute companys software several servers consider ways go configuration file every server let every component logging locally means logging info single call spread machines passed merge changes configuration duplicated many machines next level might configure remotingappender machines logging persisted transmitting single machine hosts remotingserver sure flexible though local config files say deliver everything remotingappender much say logging server remotingserver picked event ugly solution might let every called component create instances loggingevent similar return array main app takes array logs dream solution might involve special remoting-aware loggermanager takes retreiving caching config files logging server transparently passes events shall logged repository logging server wish list looks like single config file rule darkness bind least place changes made place logging done appenders work net adonetappender netsendappender remotesyslogappender etc may course file logs windows event log machine code logs preferably know remoting scenario exciting problem decision wether log done logging level filters etc set config file another machine really ask would like must many ways think pros cons. using use standard xml escapes xml guru think values respectively course start wonder need put layout first place whole structure appenders filters layout stuff powerful infrastructure filtering routing files etc. give guys list shot helping. hi richard would much like see best give two major log viewers today chainsaw http//replaced.url opinion focused log0j apache/java environment unfortunately freeware viewer definitely space fill""",1046,"43,58","41,30","57,55","13,86","2,20","0,48","0,10","0,19","0,00","0,19","0,00","1,72","0,00","12,52","3,25","1,43","8,22","2,29","3,06","1,34","0,29","0,19","4,68","0,57","0,00","6,31","0,00","0,10","0,38","6,31","4,97","1,24","0,00","0,38","0,19","22,66","6,02","6,12","3,15","4,11","1,63","0,48","1,05","1,72","2,77","1,05","1,05","0,57","0,00","0,00","0,00","0,00","0,00","12,62","2,96","3,54","5,07","3,25","3,35","1,43","0,86","0,48","0,10","0,00","0,48","0,29","0,48","5,26","3,63","0,00","0,00","0,00","0,00","0,00","0,67","0,19","0,00","0,00","0,76"
"""05c55dc3e1b707311232820199d85f66084dfaab49c54c6c4f82163ae468a27f""","""scale note xml schema anything syntax change operating pom instead pom postpone things forever let us find future-proof solution please endless discussions led nowhere far means need revert new import scope behaviour would mind see minor version increment model version far problematic others know else introducing new model building behaviour iam let isharm road better wrong course bad xml would make things like polyglot maven even harder since consumer pom something technical edited manually could keep xml forever xml parsers xslt processors available nearly every programming language xml makes sense solved way change semantics could used transform different syntaxes changes made bump model version syntax related fact diff pom-0.0.xml pom-0.0.xml difference would value model value would need deploy two poms summarize need find solution handling different syntaxes solution handle different semantics syntax going bump model versions must clear everyone increment means syntax different semantics minor version leaves patch version bug fixes like changing order elements combine.children attribute quite xml related better think model terms xml currently master would work sure model version increment without change syntax really issue however regards. maven team pleased announce release maven pmd plugin pmd plugin allows automatically run pmd code analysis tool project issource code generate site report results specify version project isplugin configuration corrupted copied target directory wish. apache maven team pleased announce release apache maven component provides abstract classes manage report generation run part site generation maven-reporting-api ismavenreport direct standalone goal invocation maven-plugin-api isspecify dependency project isdependency configuration sub-task. hi announced quarterly report january studying jira migration codehaus apache migration happen week- create exact jira projects asf codehaus copy whole content mark codehaus read-only example done years ago near empty mpom moved http//replaced.url username migration codehaus asf happen migrating existing content user accounts migrated isway jira works avoid creating accounts already exist apache matching done account ise-mail address mapping look something like e-mail/username pair codehaus content email associated user asf jira instance reuse else create new account asf jira instance exact suffix still needs defined sure username apache jira please look e-mail codehaus check matches e-mail apache question hesitate ask keep informed operation planned details. closed issue already fixed. maven team pleased announce release apache maven toolchains plugins allows share configuration across plugins example make sure plugins like compiler surefire webstart etc use jdk execution specify version project isplugin configuration mtoolchains-0 add check plugin works expected. maven team pleased announce release apache maven specify version project isdependency configuration mshared-0 dependencies inside pluginmanagement taken account reporting defined. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project isreports configured pom notice breaking changes know upgrading specify version project isplugin configuration download appropriate sources etc download page sub-task generation jar menus point index.html ignored unless file generated-site/markdown/ distributionmanagement site url another defined child project property maven.site.skip set true available generated-site component wish. maven team pleased announce release maven ant tasks maven ant tasks allow several features maven used ant deployment maven repository information available current version maven ant tasks downloaded project defined contrary documentation wish. sounds intent would work test confirmed working way impression get neither false positive false negative test test intentionally changing behaviour longer supports new syntax support new vote mark existing test range= add new test clone switched sent phone. codehaus apache done week-end new jira projects ready use asf jira every migrated project codehaus turned read-only moved asf notice added reminder still issues accounts username codehaus already used apache please report issue infra-0 precise info regarding codehaus username expected username apache thank codehaus support migration generally great service given us many years thank apache infra huge work import great result hides problems encountered. maven team pleased announce release maven ant tasks find binaries find release notes. migration start 00h every jira project codehaus migrated asf marked read-only remember check jira account email settings codehaus asf sure result asf everything tracked http//replaced.url. development apache maven promotes use dependencies via shows plugins ready finished expect contest create new maven create mascot maven owl work progress integrate maven studying jira migration codehaus apache better end-users consistency since got feedback users lost requiring compliance issue closed report last status topic users mailing list activity reduced little bit last strongly increased within time frame http//replaced.url. sent replaced email.addr.es like jira issues told mailing list conf fixed days ago infra-0 regards. apache maven team pleased announce release apache maven doxia version used future version maven-site- doxia content generation framework provides powerful techniques download appropriate sources etc download page wish. see embedded mode core run minutes objection merge embedded mode master regards. put build-pom vs consumer-pom place need publish build poms repository repository consume already- built artifacts dependencies consumer pom without newer maven version prerequisites let people consume key feature able generate consumer-pom good old pom semantics without build configuration useless consumers suppose new include scope could expanded dependencies/dependencymanagement consumer pom example build-poms need central parent poms ie poms pom packaging poms necessary build artifacts putting maven version prerequisite use issue notice consumer-poms parent poms useful imho parent poms build-poms published central pom consumer-poms published sustainable solution pom artifactid limitation generate build-pom consumer- pom since new semantics really equivalent case remember artifact consumption artifact build regards. maven team pleased announce release apache maven project specified project specify version project isplugin configuration specified via pluginmanagement. apache maven team pleased announce release apache doxia doxia content generation framework provides powerful techniques sites consisting decoration content generated doxia documents like rtf pdf format generate page garbage skin generated content package o.a.m.doxia.siterenderer. apache maven team pleased announce release maven site site plugin used generate site project generated site includes project isreports configured pom specify version project isplugin configuration output parameter msite-0 report inheritance work specified site wish. apache maven team pleased announce release apache maven doxia version used future version maven-site- doxia content generation framework provides powerful techniques download appropriate sources etc download page switch parser pegdown flexmark. maven team pleased announce release maven plugin plugin uses tool generate project specify version project isplugin configuration project modules never installed/deployed. apache maven team pleased announce release apache maven plugin utility plugin allow publishing maven website supported scm primary goal utility plugin allow apache tested git scm example push content github specify version project isplugin configuration. maven team pleased announce release maven project info reports specified project specify version project isplugin configuration. apache maven team pleased announce release apache maven plugin allows generate pdf version project isspecify version project isplugin configuration download appropriate sources etc download page document. apache maven team pleased announce release apache either html sites consisting decoration content generated doxia documents like rtf pdf download appropriate sources etc download page get info report. apache maven team pleased announce release apache maven calls information standard jdeps tool please refer specify version project isplugin configuration download appropriate sources etc download page first release plugin. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project isreports configured pom specify version project isplugin configuration msite-0 reportplugins should/could inherit information. herve change look ones. maven team pleased announce release maven archetype available local nexus create-from-project command archetype-0 reading archetype-metadata.xml file would helpful know xml badly formed. apache maven team pleased announce release apache maven doxia sitetools extension base doxia component generates either documents like rtf pdf download appropriate sources etc download page duplicate already available velocity tools copy resources call copyresources element string skin wish used internally. maven team pleased announce release maven project info plugin used generate reports information project specify version project isplugin configuration warnings non-integral time-zones plugins. apache maven team pleased announce release maven project maven project info reports plugin used generate reports information project specify version project isplugin configuration produce output changed. maven team pleased announce release maven archetypes see http//replaced.url. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project isreports configured pom specify version project isplugin configuration download appropriate sources etc download page comes settings.xml msite-0 migrating mvn site run ask wish msite-0 much information maven logs site deploy. apache maven team pleased announce release maven dependency plugin provides capability manipulate artifacts copy and/or unpack artifacts local remote repositories specified specify version project isplugin configuration copies poms twice wish. apache maven team pleased announce release specify version project parent like following changes since version. apache maven team pleased announce release apache maven doxia sitetools extension base doxia component generates either documents like rtf pdf download appropriate sources etc download page date date without precision created last modified siterenderer side rather velocity side modules found wish. apache maven team pleased announce release apache maven maven-scm-publish-plugin utility plugin allow publishing maven website supported scm primary goal utility plugin allow apache projects publish maven websites via asf svnpubsub system plugin tested git scm example push specify version project isplugin configuration seconds retry naturally linked natural site lifecycle multi-module. remarks sub-project taken experience working first see facts http//replaced.url complete list projects documented pmcs lot software described grouped pmcs came conclusion question semantic around project either talk tlps sub-projects trying visions http//replaced.url started tlp sub-projects vision tlp pretty much used us projects top level projects sub-projects bad impression puts -ones fact committees project top see commons logging committees really main project projects like extensions plugin see ant velocity imho talking committees projects best way avoid bad passion comes tlps sub-projects vision terms question merging tlps becomes merging committees ie communities putting projects management merged committee imho description verbose debate less passionated focused main question really community managed committee opinion kafka samza case hope explanations help discussion regards. maven team pleased announce release maven plugin http//replaced.url maven plugin plugin used create maven plugin descriptor isfound source tree include jar used generate report files mojos well generic help goal specify version project isplugin configuration like previous versions fun. maven team pleased announce release maven skins parent specify version project isconfiguration. unit test failing jenkins regarding jgit reproduce machine anybody clue special jenkins regards. sent phone. code maven-internal looked fine seems issue improving toolchains required signature change well able merge global user toolchains assumption maven-toolchains-plugin plugin using specific code adjusted make compatible signatures another thirdparty maven-plugin hitting issue think accept signatures world plugin side always extra handling done either refuse certain maven versions cope signatures since damage already done change gives info instead less would advice keep code as-is provide little code-example solve reflection yes still means need create situations. maven team pleased announce release maven reporting api version. maven team pleased announce release maven archetype archetype-0 archetype downloaded pom placed working metadata.xml descriptors generated needed create-from-project goal. maven team pleased announce release parent poms maven default config wish parent asf-parent default config wish default config wish. recently discussion within pmc discovered complete misconception release votes lot people feel voting releases useful pmc wrong digging issue found years ago improved legal requirements releases ensuring vote source distribution convenience binary minimum pmc votes introduced misleading wording voting template binding vs non-binding votes yes non-binding votes binding regarding legal requirements useful feedback looking vote release work real users conclusion decided change wording remove binding vs non-binding count still need find new wording explain issue whole community get feedback already saw experiments around recent votes need feedback change need feedback release votes short need involvement hesitate tell us missed something share ideas hope improve future evolutions beloved apache maven project help whole community regards. maven team pleased announce release maven archetype even redirection package instead packageinpathformat descriptor contains token project old artifact create-from-project command required property archetype-metadata.xml properties work due faulty ordering fileset archetype like done old 0.x archetype archetype-0 allow fields like scm developers licenses etc set generating archetype code archetype add-archetype-metadata. apache maven team pleased announce release apache maven component provides abstract classes manage report generation run part site generation maven-reporting-api ismavenreport direct standalone goal invocation maven-plugin-api isspecify version project isdependencies configuration download appropriate sources etc download page make test. maven team pleased announce release maven project info reports specified project specify version project isplugin configuration mpir-0 create new report show include module different new feature""",2125,"40,09","45,65","49,93","6,87","0,75","0,19","0,00","0,19","0,00","0,00","0,00","0,56","0,05","6,64","1,18","2,16","3,67","0,47","1,13","0,99","0,24","0,38","4,94","0,94","0,00","5,08","0,38","0,00","0,19","5,74","4,85","0,89","0,19","0,00","0,28","16,42","3,01","5,74","2,16","1,18","1,88","1,04","0,71","1,08","0,89","0,56","0,05","0,24","0,19","0,09","0,00","0,09","0,00","9,27","2,35","2,87","3,67","9,55","5,08","2,21","0,05","1,08","0,00","0,05","0,09","0,09","0,56","8,61","3,58","0,00","0,00","0,00","0,00","0,00","3,91","0,09","0,00","0,00","1,04"
"""30b647497e524daa7f575690abd21920047511555b6fd6a469e4bd2a314579f1""","""hi compare maven dependency mechanisms home-brewn solution company among others major thing different maven know concept artifact life cycle least know mechanism refer build life cycle life cycle status information would allow extend dependency management new dimension could declare whether certain dependencies actually allowed used. hi jason refined proposal part user proposals page regards. hi great number issues fixed it's high time new 've put together provisioning bundles dependencies clerezza includes bundles new ext.jena bundles wouldn't include sling based full launcher depends snapshot versions felix suffers yet ported source zip signatures keys git tag vote open least hours. github user rhtyd commented issue. hi andrija wouldn't worry it's blocker sense cloud 've tried fix testing. github user murali-reddy commented issue rhtyd abhinandanprateek ustcweizhou please review clean test pass new test test_router_dhcp_opts passing. see changes. github user jayapalu commented issue projectmoon ip issue worked separate ticket/pr isolated issue dependency pr get pr create separate ticket ip issue say. animesh daan clear proposing keep feature freeze date aug 00th. github user jburwell commented pull request classes implementing equals hashcode methods please add unit test case utilizes guava it's equaltester verify implementation please see article http//replaced.url information usage. suggest using sphinx plugin http//replaced.url. github user jburwell commented diff pull request. facing build failure different reason 0-forward looks like due /users/koushik/code/cloudstack-apache/cloudstack/api/src/org/apache/cloudstack/api/command/admin/router/configureovselementcmd.java:0 error package com.cloud.async exist /users/koushik/code/cloudstack-apache/cloudstack/api/src/com/cloud/network/element/virtualrouterelementservice.java:0 error find symbol. thanks nvazquez unit test wrote nice improvement i'm fine. thanks kuang http//replaced.url go chiba e-mail replaced email.addr.es. acton template wouldn't work new template check checking release version thanks. restart network failing using external loadbalencer failure number format exception broadcastdomaintype.getvalue executed returns string untagged trying parse long number pointer exception happens vlan uri vlan //untagged cases number instead untagged vlan tag used succeed although trying convert number long really using converting number long back string creating ipaddressto removed unnecessary conversion case fixing issue hand manual restart network checked number format exception eip/elb setup merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi onuonga pleasure connect arrange live hangout session hangout air many people join remotely well share detail thanks. hi animesh looks like couple critical bug fixes newly checked sounds like i'll spinning new rc confirm pick fix new rc well thanks. thank 've raised cloudstack-0 jira non-vpc rvr issues company needs issues solved short term i'll begin working fix branch fix later useful upstream shall create pr need re-work things different branch i'll happy subsequently. got anything specific lb rule go loadbalacingrules table. problems currently around asf services issues worked feel free look http//replaced.url http//replaced.url. github user ustcweizhou commented issue. case seems putting design changes new gpu types docs problematic seems docs document vgputypes added link docs changes perhaps part problem administrator documents lacking key info explanation design docs become increasingly hard locate missing information versions released difficult find gpu information cloudstack version find design docs updated admin doc add bit context reference. github user karuturi commented issue. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. ilya check/share cloudmonkey version probably uninstall reinstall latest cloudmonkey sometimes pip upgrades may work due compiled pyc files path 'filter added cloudmonkey several months maybe year ago work unless problem somewhere else i'm sure 'filter works output display types json output added many moons ago tested 'filter. github user yvsubhash commented diff pull request. good point believe determined yesterday could use either approach get results. hi kelven elaborate you're referring past 've changed code set breakpoints inside vmwareresource vmwarestorageprocessor able debug way say code running inside cs ms. would code run inside ssvm instead cs ms i'm unclear thanks clarifying. slipped attention pmc couple days please read consider applying gsoc mentor interested. hi mike thanks reporting issue 've working created pr http//replaced.url 've testing seems solving issues reported please repeat tests thanks. responsibility committer get ccla signed employer paid work done clause employment contract give ownership employer work term employment impossible know unless know employee it's situation worth asking question rights employer contributor trust contributor contributors icla it's employer signed ccla committer it's responsibility assuring contribution committed becomes simpler life committer gets easier every contributor takes minutes gets icla filed ccla required get boss sign put file wouldn't see advantage. yes api-freeze release although planned feasible hold back release longer favour fixed api future pre version get feature freeze think useful feature least earlier patches welcome. .couch functions return promises basically added returns functions cept userdb function edited fixed error uuid function calling multiple times return different types values sometimes string sometimes object removed .couch.urlprefix wouldn't working correctly anyway since wouldn't working correctly assume nobody using merge pull request git repository running alternatively review apply changes patch. exactly asked think accomplish purpose http//replaced.url best adam. thanks noah setting bikeshed really going commits like. github user michellephung commented pull request. './configure run every local install described possible every single individual user it's output files predictable locations either way see like build time control rebar sees build time modify move files could possibly ask except problem go away entirely course good catch mention lot stuff actually think perfectly surmountable key separating erlang build main build think like compiling app would wouldn't compiler try move files location handle installation /etc/config/file treat rebar whatever compilation step otp app even though result directory tree bit like next really use autotools plop right location filesystem. hi recent changes autotools config i'm able build trunk however i'm longer able 00bits mac x snow leopard seems include path used relevant i'make'/i'make dev output i000-apple-darwin00-gcc-0.0 file directory error unicode/ucol.h file directory error unicode/ucasemap.h file directory trunk revision machine .libs/couch_icu_driver.so -bundle anyone else issue well happening even fresh git checkout needs specify parameters configure etc. hi gabriel believed 've reproduced locally thanks report could file ticket would great i'll shortly file i'll get email ticket progresses saying. somehow wouldn't via jira query owe recognition ralph goers put significant work creating new regards. apache isis team pleased announce release apache isis primarily bug fix release includes upgrade wicket fixes major performance regression previous version wicket isis used includes bug fix relating wicket viewer it's reference panel choices small bug small number new features new markup value allows wicket viewer display new logout page wicket viewer support monitoring tools full release notes available apache isis website access release directly maven central repo alternatively download release build source""",1159,"26,95","35,55","55,31","10,35","2,59","1,12","1,04","0,00","0,09","0,00","0,00","1,47","0,00","10,44","3,45","1,29","7,16","1,64","2,24","0,86","0,69","0,78","4,23","0,43","0,00","4,83","0,00","0,00","0,17","6,82","4,92","1,90","0,09","0,09","0,52","16,74","3,97","3,97","2,24","1,90","1,90","0,69","1,21","0,78","1,81","0,95","0,35","0,52","1,21","0,09","1,12","0,00","0,00","12,42","4,49","2,50","5,35","5,95","4,23","1,12","0,43","0,78","0,00","0,00","0,09","0,26","0,60","16,13","5,69","0,00","0,17","0,00","0,00","0,00","1,29","0,17","3,54","0,00","5,26"
"""e5ef67ecbaaf1d8e8992ed3534836b7b1d76f86fb60f4e6a14fc65d14afa33c2""","""double slashes issue scm-0 opened mng-issue contains simplistic patch fix relevant test case fix might bit invasive affects url scm connection url alternative perhaps normal tests passes suppose original code removes double slashes reason avoiding http redirects perhaps anyone know oh documentation mentions profile integration tests -p run-its maven tells profile. think actually tracked defaulturlnormalizer module maven-model-builder indeed part maven-core defaulturlnormalizer following element scm/connection removes double slash hostname part url normalization occurs release plugin find plugin descriptor instead mavenproject.getscm .getconnection used retrieve scm url method returns scm connection url normalized important double slash removed somewhere lost maze maven-core sorry pinpoint issue better. hello understand strictly concern blocker release maven scm-0 still blocking upgrade 0.x including 0.0-rc0 anyone using mercurial hg scm absolute paths remote server maven release plugin chance get fix included. right weeks ago made weak attempt look source code scm module found nothing obvious something wrong url component actually parses pom.xml extracts string scm/connection element maven use scm url mercurial contains absolute relative path handed hg mercurial looks project wrong place release build checkout fails mercurial special non-standard url annotation two consecutive slashes hostname separate relative paths absolute paths example scm url works fails 0.x double slashes localhost tells hg file path absolute file system located /opt/foo double slash removed hg instead search directory opt/foo relative home directory user server perhaps /home/luser/opt/foo. fill kip purpose kips give full overview feature work implemented considerations involved etc like sentence wouldn't enough anyone know thinking moving configs something i'm guessing would zookeeper-based massive change really need describe way widely circulated actually think would good idea advantages good old fashioned text files terms config management change control trying support may may better. current implementation producerperformance creates static payload useful testing compression test production/custom payloads decided add support providing payload file producer perf test made following changes added support provide payload file list payloads actually send moved payload generation inside send loop cases payload file provided following changes producer-performance evoked must provide payload-file record guaranteed using custom events earlier required config must provide exactly payload-file providing result error support additional parameter payload-delimiter added defaults merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. realized known problem since feb jason showed ticket submitted patch http//replaced.url make sure return data record failed deserialization throw exception next poll. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. see changes went wrong run stacktrace option get stack trace run info debug option get log output get help http//replaced.url findbugs searching files match pattern /build/reports/findbugs/ .xml using gitblamer create author commit information warnings sending mail unregistered user replaced email.addr.es sending mail unregistered user replaced email.addr.es. said general point think consider supporting topic patterns wire protocol requires thinking cross-language support seems surmountable could make certain operations lot efficient fact basic regex subscription causes consumer request metadata topics great. i'd like start vote kip-0 add command line tool offsets details thanks best regards. nexus scheduled maintenance fall i'm trying cut release nexus appears dead thanks. would consider something like sfl0j something part api think it's fine expose something like i'm sure holds true cdi would wouldn't better completely hide thanks. think it's really ok default trunk mode maybe unless checking tag main project it's feature remember clearly would 've enjoyed first time ever tried change something maven clearly checking repositories like crazy building dependencies finally managed find thing interested time felt like built half known universe components reached goal similar-sounding names hey wouldn't check course workspace-plugin would keep pom changes top-level project separate commit wouldn't mess actual changeset could support patch command. need rephrase change documentation release process update fixed -latest place svn copy release svnpubsub give us expected regards. dependencyset specify following things. following issue deleted jira. hi custom enforcer rule occasionally failing looking code defaultartifact appears related error fixed getversionrange could field getter protects maven happens running jdk com.nds.cab.build.enforcer.engineeringreleaserule.execute engineeringrel wonder really defaultartifact.setversion wouldn't null versionrange recreate something else play something wrong enforcer rule following results bogus pom phases line causes projects exhibit behaviour multi-module projects several modules pass hits wouldn't like message confidential intended addressee received message error please immediately notify replaced email.addr.es delete system well copies content e-mails well traffic data may monitored nds employment security purposes protect environment please print e-mail unless necessary. looks interesting eric soon definitive place add link maven site. found bug modello plugin breaking /any/ build using modello multi-modules i'm fairly sure kind issue found elsewhere quite simply it's use plexus components singletons mojos contain per-request mutable state since you're short time away beta-0 wanted know think done treat documentation problem maybe update guidelines regarding singleton usage maybe keep list known b add kind isthreadsafe attribute metadata could used assert run concurrently without something else thoughts b done. android maven plugin team pleased announce release version plugin com.simpligility.maven.plugins new features/bug fixes specific release fix allow release-plugin ides correctly consume aar deps http//replaced.url would like thank contributors release valuable help invite help us well specifically release would like thank following contributors awesome work would like thank help received maven community members everybody else help received issue tracker beyond documentation issue tracker found plugin websites http//replaced.url http//replaced.url enjoy congratulations everyone involved http//replaced.url. hello benson looking http//replaced.url maybe could point maven users developers general better installing jdk regards mirko. really called beta release criteria could vote it's beta. i'll leave guys suggestion make submission package contains references repos userids clas file userids new clas along clas send email pointer repo perms plexus people add userid cla file directory new clas may get i'm sure active plexus mailing list send message herve collect together see discrepancies deal ask legal covered clas acceptable submit it's ok great start trying expunge thanks. please add new version ant-contrib ibiblio. never heuristic replace people taking apis testing without combination part project external party trying collect metrics use probably going great value retaining api compatibility along high degree coverage probably useful metric really it's conscious effort api broken even inadvertently mechanical turk going help us much implementation metadatarepository api it's costly deal dependencies currently pom swizzling deal ranges number possible paths grows quickly sat solver fast could predigest dependency information pre-calculated subgraphs probably useful change range path you're recalculating anyway boil something far simpler sat solver guaranteed find working solution ranges set set fixed weighted say pom fix version depman change calculation like fixed fix let sat solver find solution magic already osgi resolver tycho thing p0 uses sat it's osgi made clear it's osgi proper use right osgi resolver simple state machine it's p0 osgi resolver even though retrieves ultimately fed osgi thanks. sure anybody apache say yes synchronization ibibilio codehaus repos 've got. maybe something get/ set tags wouldn't make changes anyway wouldn't think important worth hassle. split verifier improve cli performance core plugins -target committer free call vote minimum etc. crawl designed kind issue mind long thanks. questions specific closed issues might warrant really implemented intended close wouldn't fix knowledge feature delete issue unless completely mistakenly filed think close incomplete/wo wouldn't fix otherwise it's impossible get back information lost think would useful thing add patch still applies part bootstrap check released version reopen wouldn't really sure fixed patch date eclipse code site good reopened patches optional non- default functionality patches specific problem keep open updated right thanks""",1313,"42,35","38,99","57,35","13,71","3,73","0,91","0,53","0,23","0,15","0,00","0,00","2,82","0,00","12,11","4,57","2,06","7,08","2,28","3,43","0,91","0,38","1,14","2,89","0,76","0,00","6,09","0,00","0,00","0,30","8,00","6,40","1,60","0,15","0,08","0,46","22,54","4,19","4,42","3,05","3,96","2,89","1,37","1,37","1,90","0,99","0,53","0,30","0,15","0,30","0,00","0,15","0,00","0,15","12,26","5,48","3,05","3,43","4,04","3,88","0,69","0,15","1,45","0,00","0,08","0,38","0,23","0,30","12,57","4,49","0,00","0,00","0,00","0,00","0,00","2,51","0,15","2,74","0,00","2,67"
"""4ee8a6b0f4c6cbd020ffa14217436cc3226ba516a8fb32a15047fe61784dc626""","""thank john good start understand seems complicated though got way regards. hello quite dumb question new nutch/solr migrating web indexer commercial product nutch/solr yet understood internal need spending time problem installed nutch/solr everything works fine solr standalone mode even installed solr install_solr_service.sh everything fine go pass solr cloud mode found elegant way modify /etc/init.d/solr /etc/default/solr.in.sh order launch solr cloud mode succeeded start dirty point hints links cleanly remark reason trying migrate solrcloud able basic auth around documents index standalone enough us regards. hi john though launch solr -c option last tried. oozie-0 fix warning file wouldn't listed dependency files details oozie-0 using uber mode regex pattern used extractheapsizemb method allow heap sizes specified bytes oozie-0 reduce number threads test execution oozie-0 oozie job submit wouldn't report error message user issue job conf oozie-0 status update recovery problems coord action children sync oozie-0 get ooziesharelibcli perform final rename destpath creating sharelib oozie-0 dataset url contains spaces handled rightly oozie-0 log.scan.duration used error audit logs oozie-0 map-reduce launcher need distributed files archives except jar input/outputformat oozie-0 introducing new counter instrumentation log distinguish reasons launcher failure oozie-0 ssh action succeed exists command fail oozie-0 rerun failed/killed/timedout coordinator actions rather specifying action numbers oozie-0 queuedump command display queue information server oozie-0 loginfo uses action instead oozie-0 instrumentation configuration rest api web ui include oozie servers oozie-0 cache list available timezones admin may edit subscription. oozie-0 new api fetch workflows corresponding coordinator action reruns oozie-0 user able set bundle/coord end-time start time oozie-0 support oozie-0 ability view log information corresponding particular coordinator action oozie-0 new configuration specify server-server authentication oozie-0 pom.xml use profiles building hadooplibs oozie-0 workflowgenerator package tar.gz file though seems creating intention may edit subscription. wasn't implemented today feature generators wouldn't therefore createfeaturegenerators method creates new feature generator every time needed tries read xml descriptor model creates say uses default feature generation happen createfeaturegenerator method returns null true case place exactly add getter method fix problem tokennamefindermodel call tokennamefinderfactory wouldn't instance trying understand proposed fix works usually model created using constructors inputstream file url use different constructor create model try reproduce bug see first train model command feature generator config use command line tool evaluation maybe post command. using tip head issues bytebuffer reads http//replaced.url error seems different you're seeing though might different field 'uncompressed_page_size found serialized data. testing must revise vote encountered http//replaced.url. i'm favour merging parquet-format parquet-mr moment would merge mr cpp development speeds release cycles differ thus would inconvenience repo. reef.net currently depends newtonsoft.json .net core compliant shall add jira update newtonsoft.json .net core compliant. mentioned bug report reason npe happens storm vulnerable fraud message processes outside cluster reproduce npe need send message taskid host :port anywhere taskid tasks running host :port case storm return taskmessage payload set null deserializer storm check whether task message processes within cluster deserialization may fail think storm skip fruad task message rather shutdowning worker deserialization failed. preparing patch publishing in-backlog receive-queue out-backlog send-queue see average value metrics ui time window period executors section aggregation component level etc users may interested instant value metrics wouldn't know fit ui suggestions welcome. part looks good things clean. github user sachingsachin commented diff pull request. github user redsanket commented diff pull request. think you're something addressed pr wouldn't think merge adding document patch master could revisit address remaining comments file issues done follow-up patches wouldn't think addressed could leave comments discuss make decisions could revisit sort remaining things side. able make integration spring problem autowire every bolt spout means even parallelize spout bolt get started instance way bolts spouts mean parallelize bolts spouts individually share conf somewhere possible thanks. github user heartsavior commented diff pull request""",671,"37,28","35,02","46,35","9,69","1,49","0,60","0,15","0,15","0,30","0,00","0,00","0,89","0,00","11,33","3,58","1,64","6,86","2,38","2,09","1,19","0,75","1,19","1,79","3,58","0,15","5,22","0,00","0,00","0,60","5,81","3,73","2,09","0,15","0,30","0,75","16,54","3,87","5,37","3,43","2,83","1,04","0,15","0,75","0,75","1,04","0,89","0,15","0,00","0,45","0,15","0,30","0,00","0,00","12,37","2,98","2,68","5,51","3,43","4,77","0,30","0,60","0,89","0,00","0,30","0,00","0,00","0,00","15,35","4,77","0,00","0,30","0,00","0,00","0,00","5,07","0,30","1,79","0,00","3,13"
"""eb4a21a7dedf4a3b309af5d9dc0a1012b0a3a4a736db77d1d539917f19d0c317""","""currently initial patch seems work would like know whether overall code ok. added documentation properties configdef suggests sure importance assigned properties normal properties find info totally sure validations correct tried figure code still might miss something finally mailing list right place ask questions submit patch jira ticket get review even sure quality thanks help patch used specifying set expected configurations defaults override topic created altered defaults.compact compact else delete configuration s\ .format s\ .format check given properties contain log config names values parsed check given properties contain log config names values parsed parse values unfortunately validate smaller number replicas since information newline file. using something like nio-based frameworks value far concerned lot work evaluating nio-based general event-based i/o frameworks good old one-thread-per-conncetino blew everything else water performance wise. i'd like try latest t0.0 release it's built repository point able access thanks. i'd passivate null field string null interpret appropriately onactivate causes npe i'm passivating empty string instead produces url pagelinks render wrongly appear urls based current point web context rather root web better approach using passivate null and/or empty fields. lot presupposes method names going change new committment backwards compatibility. right attention given change api public internal package. considered using jackson objectmapper wouldn't seem really good reason implement plugin jackson reads/writes bson objects i'm sure would really nice tapestry json interface based concrete implementations wouldn't convert different wishing time work. damn forgot mention important stuff read tapestry. thanks moving wiki update post objections. merge you're discussions. github user astroshim commented issue pr going merged. pr changes success message box zeppelin it's dialog box pr tested try save blank values credential page breaking changes older versions needs documentation merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. question present conf taken default value configurations defined http//replaced.url thanks. hi moon many people use zeppelin data analysis work user often confuse paragraph stay pending running long time seems zeppelin working turns users running paragraphs time think it's useful provide dashboard show running pending paragraphs state easily see many paragraphs run. prefer plan merging two different r interpreters sounds like maintenance documentation headache users community feel specific additional steps technical development perspective need happen order merge know it's holding back merge technically history aside work community solve looking forward helping. github user jongyoul commented issue restart failed job http//replaced.url. i'm committer easiest post rules file cwiki. hi rajesh last test hyper-v system vm probably august 00th downloaded hyper-v system vm template currently use worked fine mind could file defect look provide mechanism download build output. would love sink teeth assist code doc. bug node without schema bootstrap. recompile analysers use 0.x regards. hi lately using jsonoutput handle json serialization find jsonoutput.tojson overloaded pretty nifty however trying pretty print object calls look like it's pretty awkward opinion i'd like propose change following change would overloaded methods .tojson call boolean prettyprint legacy without boolean parameter backwards compatibility like. work unreliable links resume aborted partial files. know although give parsed results could use query parameters i.e q parameter debugging use parens fully qualify term e.g way_analyzed rue way_analyzed it's best. thanks helpful indeed debugquery get index omitnorms relevant field index without omitnorms relevant field get non-unit value thanks giving way reassure omitnorms really it's thing dive debugquery figure wouldn't seem much effect anticipated relevance""",585,"22,50","36,92","57,61","11,11","3,76","1,54","1,37","0,00","0,17","0,00","0,00","2,22","0,00","12,65","4,10","1,37","9,06","1,37","2,22","0,68","0,85","1,03","2,22","0,68","0,17","5,64","0,00","0,00","0,17","9,57","7,52","1,88","0,34","0,17","0,68","21,37","5,47","5,47","2,74","3,76","2,22","0,85","0,51","2,05","1,03","0,68","0,17","0,17","0,85","0,17","0,34","0,17","0,17","12,48","4,27","2,56","4,62","3,42","4,44","1,20","0,00","0,85","0,00","0,00","0,17","0,00","0,51","13,16","5,98","0,00","0,00","0,00","0,00","0,00","1,54","0,34","3,25","0,00","2,05"
"""23f365c7cc67a9c7e0569e37f4dc4a7569f5320786759c5b92b12e887d7c5fa4""","""following question particular considering extremely nasty release policy maven project maven-jar-plugin seen release eight months although plugin extremely important nasty bugs maven-changes-plugin seen release ui know long although plugin required migrating maven. hi mjar-0 mjar-0 fixed suggested brett releasing maven-archiver-plugin maven-jar-plugin thanks. already patch mjar-0 ready work someone volunteers work pull patches. pops question expect move regards. running attached test case might well added plexus-archiver whenever fixed demonstrates plexus archivers issue tracker file issue attach test regards. due release axiom thanks. hi two months ago vote held releasing maven-changes-plugin see issues raised particular concerning maven-changelog-plugin opposed maven-changes-plugin conclusion defer release reproduce called blocker anyways least imo question whether would possible release thing finally helps would glad required work. synchronized ibiblio normal write unfortunately belongs apsite group member imo apcvs committer access. believe misreading specification continuation defined otherchar newline otherchar defined anything nul cr lf words even continuation line lf allowed. thank. thanks issue created""",172,"15,64","40,70","50,58","10,47","1,74","0,00","0,00","0,00","0,00","0,00","0,00","1,74","0,00","8,72","1,74","2,33","4,65","1,74","2,33","0,00","2,33","0,00","1,74","2,91","0,00","6,98","0,00","0,00","0,00","7,56","5,81","1,74","0,00","1,16","0,00","21,51","4,07","2,91","2,33","4,07","2,91","2,91","0,00","0,58","1,74","1,74","0,00","0,00","0,00","0,00","0,00","0,00","0,00","10,47","4,07","0,00","6,40","5,23","4,65","0,58","0,00","0,00","0,58","0,00","0,00","0,58","0,00","17,44","5,81","0,00","0,00","0,00","0,00","0,00","10,47","1,16","0,00","0,00","0,00"
"""3875138fac55dea58c60afbfb915d2f3c3cdd2304c980682a90a6b1811544513""","""try use temp view couchdb response temporary views supported couchdb status_code see temporary views supported couchdb0.0 documents changelog page broken http//replaced.url. lame mailclient changing titel every time reply hate tobit mailclient sorry good news found firewall blocking port server opend port screensharing loading //my_external_ip/webmeeting sadly cant start screensharing doesnt switch stop screensharing started connect //internal_ip/webmeeting screensharing working perfektly serveral ports found websocket thing wich load behind mod_proxy wss //internal_ip/webmeeting/wicket/websocket wicket-ajax-baseurl= wicket-app-name=openmeetingsapplication read proxy_wstunnel_module need first tests trying redirect webmeeting/wicket/ wss //0/webmeeting/wicket/ ends working openmeeting website regards. ok thanks. thanks try. thanks error ast_odbc_sanity_check connection attempting reconnect ast_odbc_sanity_check connection attempting reconnect. rtmps set application ensure close port change random value configs make sync red0.properties. try create need download driver work. i'll doublecheck. eric intention unsubscribe mailing list. thank help past appreciate writing storm application process alerts applications real-time need sort stream severity field called severity contain following strings critical high medium low debug need send streams critical severity specific bolt call bolt criticalseveritybolt looks like use topology create custom stream grouping using customstreamgrouping sure best way anyone examples either solution replaced email.addr.es http//replaced.url. thank response thinking using codahale it's metrics trident application wanted sure storm ui rest api useless case. hi looking way following solr somebody search show results category facet display results category along showing total number results category always using facet search kind overview search results user click category see results pertaining category way think making many queries categories show results category inefficient way thanks regards. hi change default location /tmp/lib using creating jar files cassandra uses java property java.io.tmpdir temporary folder default it's changed command line arguments cassandra -djava.io.tmpdir=/path/to/tmpdir launch cluster minutes""",318,"24,46","38,99","53,14","6,60","2,20","0,63","0,63","0,00","0,00","0,00","0,00","1,57","0,00","12,58","1,57","2,52","9,75","0,31","0,00","0,94","0,00","0,63","2,83","0,94","0,00","4,72","0,00","0,00","0,00","9,12","5,97","3,14","0,00","1,26","1,26","21,07","4,40","8,49","1,57","2,20","1,89","0,94","0,63","0,31","2,52","2,52","0,00","0,00","0,31","0,00","0,31","0,00","0,00","11,01","3,14","2,20","4,09","2,83","4,72","1,26","0,94","0,94","0,00","0,00","0,31","0,00","0,31","23,90","6,60","0,00","0,00","0,00","0,00","0,00","1,89","0,63","0,94","0,00","13,84"
"""7a6ddb98f89bc8f6a761a842d9297a7972e1ff5e70a7321b7610bc6f65472b87""","""hi heard wrails project sorry senro project senro http//www.senro.org like wicket rails check http//replaced.url glad chris nelson copyright note still comes perfect timing complete beautiful week open pd sorry bother stupid things needed catharsis action. tag fix issues clientdataencodertest merged back branch component fixed broken tapestry.png 0c000af tag fix tests broken recent change 0b0badd use application root package application commit looks like could related issue experiencing. use case fwiw using couple block contributions used components used ajax stopped working introducing main really handy component automatically rendering really clue fix shall love working thanks denis bringing discussion. problem maven latest versions think using maven use maven build tapestry0. projects taken extreme allowing index pages context enforcing rule code workaround trick pagerenderdispatcher skip index pages adding new dispatcher pipeline forces empty_context index pages module http//replaced.url btw talked like months ago http//replaced.url. maven team pleased announce release apache parent pom pom contains settings likely useful apache project building releasing code maven specify version project isparent configuration wish parent asf-parent. maven team pleased announce release maven dependency dependency plugin provides capability manipulate artifacts copy and/or unpack artifacts local remote repositories specified specify version project isplugin configuration. someone interested build fails run parallel mode -t0 could reproduce failure local machine someone interested discover goes wrong parallel investigate wanted false positive asf jenkins regards. development apache maven promotes use dependencies via use default super pom version requirement expect near future update minimum requirement maven jre permit make cleanup jdk0 compatibility status latest informations available wiki plugins fixed quarter maven-eclipse-plugin wagon surefire components yet preparing enable rat check every build next maven-parent pom ensure full compliance currently change cause real guide line definitive descriptor plugins changed accordingly means every new release contains appropriate download link every release reported quarter contains already issue closed report last status topic apachecon europe talk developers said attending expect meet discuss maven future plans users mailing list activity reduced little bit last six month comparison whereas developer list activity less constant within time period. apache maven team pleased announce release apache maven plugin generates report regarding code used specify version project isplugin configuration causes parallel build failures would nice see check together output violations wish. maven team pleased announce release apache maven site maven site plugin plugin generates site current specify version project isplugin configuration download appropriate sources etc download page msite-0 use property items child module site msite-0 inclusion resources basedir velocity used. apache maven team pleased announce release apache maven maven plugin tools contains necessary tools generate rebarbative content like descriptor help documentation specify version project isplugin configuration broken using tags running twice skiperrornodescriptorsfound. apache maven team pleased announce release apache maven maven plugin tools contains necessary tools able produce maven plugins scripting languages generate rebarbative content like descriptor help documentation maven plugin plugin used create maven plugin descriptor isfound source tree include jar used generate report files mojos well updating plugin registry artifact metadata generating generic help goal specify version project isplugin configuration wish metadata create new version descriptor. apache maven team pleased announce release maven ant find binaries find release notes""",533,"38,07","41,09","48,22","5,82","0,56","0,00","0,00","0,00","0,00","0,00","0,00","0,56","0,00","7,32","0,75","2,63","3,94","0,38","1,13","0,38","0,56","0,00","3,75","1,13","0,00","5,25","0,75","0,00","0,19","6,38","4,13","2,25","0,00","0,75","0,75","16,70","1,31","7,13","2,06","0,75","2,25","1,69","0,75","0,56","1,13","0,56","0,38","0,00","0,19","0,00","0,00","0,19","0,00","10,69","2,44","2,81","5,25","6,75","4,69","1,50","0,19","0,75","0,00","0,00","0,00","0,19","0,94","7,13","3,56","0,00","0,00","0,00","0,00","0,00","1,31","0,38","0,00","0,00","1,88"
"""d3bbe7127318e1711f6fb3991caba157662051ce4fb2400f6db294afca2bb977""","""looks overengineered types zips jar either. think looking maven embedder. horribly afraid right documentation says b defined b isparent isdependency management section since dependency management selected referenced ispom b compile scope thanks. aware change intentional conforms poms procedure described docs nothing nothing less. hi release preps constantly failing build maven mvn -preporting site help simply means actually tried read jar file skin received classes directory target traced proceed debugging debug symbols match source code anyone clue system tries resolve instead local repository highly assume sonatype eclipse aether issue insights welcome. already. hi folks anyone already noticed plugin http//replaced.url might automate test pr jason wants thoughts. convention way omit last zero done maven plugins/components. thank offer best time go channel. ok thank process sites later day. canceling vote due problems pom files herve found shall respin vote today. hi folks would like patch several issues jira unfortunately right change tickets e.g target version component forth anyone able promote thanks. cause problem dependency transitive dpmgt section read/compared mdep would least expect warning anyone else. noticed work therefore asking last 0.x release branch intended able cut release permissions promoting release thanks. hi still couple issues left jira note release depends release skins parent. updated good possible. think special idea mind could read snippet retrieve bytes specific encoding compare guessing encoding tricky would require icu0j course user supplies wrong encoding lost. done http//replaced.url completely disassembled tarball build maven theirselves non-canoncial build likely going support modifications known us rather untar offical tarball rebuilt package though grab tarball internet somehow. hi folks recently proposed mskins-0 downgrade 0.x two reasons updated site.vm custom.css bootstrap yet knows real changes model change must happen patch version received feedback neither positive negative would like perform downgrade site rendering work expected additionally made several improvements css rendering stuff locally parent project would like merge back plugin bring site plugins let us postpone bootstrap 0.x fluido skin 0.x. able restage tomorrow working patch doxia-0 would like commit. model would require maven cleanup env like m0_home .m0 m0.conf forth otherwise break current limitations without. hi barrie docs definitively correspond poms moment chapter staging latest documentation says release prepared simply wrong release prepared performed documentation staged due missing next problem made push pom back forth simply work scm publish bound parent site-deploy phase site stage-deploy goal scm-publish publish-scm works know whether correct verification kinda awkward says wait sync linked site say anything sync given timeframe sync occurs path /www/maven.apache.org/pom exist /www/maven.apache.org/content/pom moreover directory skins updated skins-0 directory exists pom-archives skipped steps stuck lost help would greatly appreciated. hi devs promote staged repos skins parent fluido skin gone happened auto-dropper active rao source releases already promoted herve thanks. hi robert back october result retirement vote positive plugin still without notice issues still open jira repo github simply. currently running patched version maven offensive commit reverted surefire master freebsd 0-stable shall see wether cause hour. thanks fix added purpose resembles output site tool generates modules menu looks awkward outputs uniform. file jira ticket. hi brett thanks promotion seems like edit tickets old created ages ago able drop completely thanks. file issue. quite annoying took quite time figure correct settings git cmd egit even know whether got right subversion made way easier stackoverflow good help place anyone better cross-os setting. yes resposibility always good simply make build fail instead log collision happens. assumption yes. located lifecycle phase maven-plugin. though minor doc issues related msite-0 downgrade advise velocity noted caution guarantee velocity request tools properly work additionally filtering section updated context variable makes easy display dotted maven properties. hi herve seems like entire process requires another sign fortunately already jira account xircles account wrote codehaus support asked transferring account intend recreate jira account created many tickets current thanks. hi tibor currently running full test bed various maven versions surefire master pass log files along target directory shall join channel. pages missing apt files remains empty maven actually fixed. hi still couple issues left jira. hi folks performed another cleanup jira last couple days old issues right open issues manageble point view noticed several plugins touched years status following plugins anyone working planning release version thoughts objections would like retire clean jira plugin plugin. already known issue year http//replaced.url. guys agreed reuse tags apply procedure tomcat team staged build fit abandoned tag remains relased added change log http//replaced.url see version opinion release notes shall mention skipped skipped. first thing came mind. nobody retiring fluido skin subject. indeed mngeclipse jetty gone years codehaus support assitance thank much. always minutes. hi devs possible push fluido skin work happens completed fixes time ago would like another bootstrap 0.x based released modification work bootstrap 0.x begins lot stuff time. quick search central shall even apache project adhere convention receive. vote please period already due. dependency different poms dependency think like inject annotation guice thank. parse.vm lives next template.vm call parse parse.vm macro available rest though tried probably create custom skin really create. course piss lot people would course several reasons people not popularity old collisions etc even enforce happen maven added plugin dev center. done r0000000. probably query resolution unresolved. vote please. hi paul looks way better thing though alread fixed issue bound distinct maven version change e.g. mng-0. hi folks herve quite busy right preparing doxia sitetools along maven site plugin several tickets would like fix changes impact skins provide need adapted close look skins updated years active improvements happen fluido skin given fact probably work none changed html0 would like retire skins keep two active would ultimately mean two updated later proposal please share opinion agree start vote retire. hi previously suggested makes sense retire skins updated long time resources maintain properly last releases therefore propose retire skins vote successful make final release skin making clear skin site retired source code moved retired area subversion repository process retiring skin described though plugins process universal vote open hours yes time would ask kindly already cast vote revote make. following services unavailable upgrade. unreachable two different carriers. fact mentioned front page skin addtionally maven. read mind idea weeks option work strongly support remove build files ant maven maven along maven ant tasks need removed need free use aether ant tasks. terms contributions zero someone keen enough code prepare patch simply lazy need fork clone push create pull request even work see benefit. mshared-0. simply list dependencies report. hi folks actually proceed third-party plugins comply naming pattern given plugin created document beeen first published simply add disclaimer legacy reasons plugins central created date. question obviously. infact added introduce maven.conf m0.conf mng-0 make conf modification /etc /usr/local/etc snap. yes shame simply forgot agenda already ok. think credit goes github eases participation non-committers tremendously though need improve pr process mirrored repos. considered running versions display-plugin-updates. open issue infra inquire default pr procedure look like mirrored repos github think need common approach entire asf. would like fluido reference parent released yet ok actually moreover found another bug fluido would like report fix. think variations depenencies must go either classifiers build qualiafiers start allow arbitrary elements people start ask custom elements x. therefore custom elements allow plugin s. working sample project attached already please check. thank much bringing personally disgusted stupid names proposed like boom shotgun anything else related rifle arming. proverb not make think rm mono-module multi-module project he/she needs know perform steps fine relase docs scattered several pages references would like avoid another exception. might added http//replaced.url. http//replaced.url. thanks turns something wrong repo r.a.o able browse yesterday try browse nexus ui get http repos broken file issue infra. opened issues least fixed. thanks guys look http//replaced.url next couple days process required. thanks change poms accordingly. forgot always use relative path docs otherwise could misinterpreted. aware retirement plan procedure go anyawy post list first figure plugins really dead sorry part. opposing previous proposals really see urgent need animal link look meaning maven clearly see refers well-educated human think needs evaluated though know represent graphic. note maven-0 repo probably typo. think fully correct leave goals defaults distributionmanagement/site element perform site-deploy default check m-relase-p docs say. ultimately mean plugins run maven someone still uses older version java build projects. big question would like clarify first commons compress gone java minor release people requesting. head idea mind several months make clean cut read period new issues asf jira someone found issue already codehaus jira migrated course. hi folks would like another bug squash like last year currently unresolved bugs updated three years hardly believe updated ever maybe resolved new versions automatically please look whether see still anything else object go ahead close fix let know week. avail still error leave profile reporting""",1507,"16,20","34,24","60,32","12,74","1,53","0,33","0,00","0,13","0,00","0,13","0,07","1,19","0,07","10,95","2,39","2,12","6,64","1,92","3,65","0,46","0,80","0,66","3,92","1,79","0,07","5,77","0,80","0,00","0,46","7,83","5,77","2,06","0,40","0,53","0,53","18,58","4,45","3,58","2,65","3,05","2,19","1,00","0,86","0,93","2,39","1,06","0,40","0,86","0,93","0,86","0,07","0,00","0,00","14,27","3,58","2,72","8,29","5,37","4,78","1,19","0,40","0,60","0,00","0,07","0,53","0,07","0,46","12,21","8,16","0,00","0,00","0,00","0,00","0,00","1,92","0,13","0,00","0,00","1,99"
"""05e13208ea5297172044cb524152d78290e0894d7aa7ed77a8b683ef1833f8b9""","""way query table based binary column bytes proofpoint inc. sql query looks part like sometimes returns port values ip column ip values port column delete group first/last sections seems work fine run query cores runs fine fail part number cores effecting bug makes pretty sure israce condition along fact printing values changes bug have looked client code not seen anything obvious occurred suppose could server code anyone else seen anything like proofpoint inc. done. fine would like note first announce publically users mailing list think deserves major bump plugin version wdyt. hi changes since last release. would favor move java make strong use nio0 file operations lot pain go away. could announce least five days advance would give least timeframe merge prs handle issues. think must tandem packaging zip finally. yes works stupid fault simply noticed use secured url used read-only url sorry noise. yes maven mvn -prun-its install build.log files say updated msite mpir projects touched subversion. would drop altogether go. hi paul good idea done already pre-0.0 versions going think strictly follow no-fix-version-for-incomplete/invalid/wontfix able assign real versions great. hi michael staged site http//replaced.url. rather add plain text release notes html junk unreadible. not course solve problem pre-existing files give clean way things right new files. given companies/folks react something discontinued would move java baseline christman first release java e.g. policy make apps run java months even compatible not concern. help becuase repos located github apache. non-binding. cloned tested upgrade msite pain go away parse/ include. possible ran uts twice check dep tree mediated. thanks announcement actively providing several years finally able contribute officially directly thank. confirm tried wagon 0-snapshot maven master. glad found documentation hard would like repeat information serveral places need update maybe interpolation issue please open jira issue thanks. hi vote passed following result promote artifacts central repo pmcs please promote source release zip file add release. hi folks in-house project jcl-over-slf0j nothing special far module dependends net.sf.michael-o.dirctxsrc dircontextsource jar:0 compile slf0j-simple test scope mvn dependency tree properly gives master gives questions another fix master relied erratic behavior core previously depmgmthave influence transitive deps direct really say right wrong two slf0j bindings slf0j simple logback obviously wrong thoughts. please reply promotion http//replaced.url would like manage release like codehaus jira thanks. insane works thank much need fix fail w/ maven 0.x due aether advise must raise mpir msite mksins-0 work anymore may others. yes http//replaced.url obstained merge patch version minds course. hi folks seems like full access maven directory see anyone able check error correct permissions username michaelo thanks""",454,"15,66","27,31","58,81","12,78","1,76","0,00","0,00","0,00","0,00","0,00","0,00","1,76","0,00","11,45","3,30","2,20","6,61","2,20","1,32","1,54","0,00","1,10","4,19","2,42","0,00","5,07","0,66","0,00","0,00","8,81","6,39","2,42","0,00","0,22","0,44","17,40","3,52","3,52","3,30","3,52","2,20","0,44","1,10","1,32","2,86","1,76","0,66","0,44","0,66","0,00","0,66","0,00","0,00","11,89","4,41","2,42","5,29","5,29","5,07","0,44","0,44","0,44","0,00","0,00","0,66","0,00","0,66","15,42","8,37","0,00","0,22","0,00","0,00","0,00","3,74","0,44","0,00","0,00","2,64"
"""b88e5ab8b78d81d5d2641e4ea1e017560904a5a481c38692833f255be2bee640""","""deb package rpm. cassandra testing using old server core celeron processor 0gib ram another 0gib cores two consumer sata hard disks works i.e memory error etc writes reads second maybe database extremely small even days megabytes configuration absolute stock configuration changed anything except noticable node small server remember somewhere hand noticable interesting disk higher log hard disk contained system data disk grain salt intention test setting cluster two distant datacenters performance test. thank mail restarted affected server noticed mail likely related leap second introduced today. yes column stored value every key may matter switch compression afaik advantages default worried storage space mysql table intend move cassandra columns long column names average characters column values mostly byte integers hand many colums empty specifically null afaik mysql able simply store column value default table data without indexes mysql gb millions rows cassandra gb without compression gb compression includes single index row compression switched specific case storage requirements roughly cassandra mysql sorted key sorted column. opposite true requests fast slow case percentile fast slow except order samples opposite direction usual. appearance old rows caused old timestamps set columns turn caused threadlocals cleaned since fixed timestamp rows returned corresponds latest saved state every case. requirement nodes unique token still global cluster/ring node needs unique logically seperate rings networktopologystrategy puts rest code. thinking frequent example theory using tokens dc0 dc0 significantly affect key distribution specifically two keys move next much however seems unexplained requirement least could find explanation nodes must unique token even put different circle networktopologystrategy otherwise data evenly distributed. use propertyfilesnitch networktopologystrategy create multi-datacenter setup two circles start reading page http//replaced.url moreover tokens must unique even across datacenters although pure curiosity wonder rationale behind way someone enlighten first line output nodetool obviously contains token nothing else seems like formatting glitch maybe role. played test cluster stopping cassandra node updating row another noticed delay delivering hinted handoffs know rationale node originally received update noticed server waited started pushing hints log endpoint. noticed strange phenomenon cassandra would like know something completely impossible see log extract new versions row written reads returns obsolete data read version even already written single cassandra node cluster client local network rows written read seconds would think test environment see obsolete data actually thousands log entries hours test say row read match latest data written checked detail history another node seems eventually receive up-to-date row took minutes specific case fyi started evaluate cassandra without significant. copy init.d datastax package""",416,"34,67","39,90","56,73","15,62","1,20","0,24","0,24","0,00","0,00","0,00","0,00","0,96","0,00","7,93","1,44","2,64","3,85","1,20","3,12","2,40","0,72","0,96","5,53","2,16","0,00","1,92","0,00","0,00","0,00","3,12","2,40","0,72","0,24","0,00","0,24","18,51","5,53","3,37","1,68","3,37","2,40","2,40","0,24","1,44","4,81","2,40","0,72","1,20","0,48","0,48","0,00","0,00","0,00","15,38","2,64","2,88","8,89","3,61","2,16","1,20","0,24","0,96","0,00","0,00","0,24","0,00","0,24","5,29","3,37","0,00","0,00","0,00","0,00","0,00","0,72","0,48","0,00","0,00","0,72"
"""ddca80046db3358a64436ad45a3fcd95c3e42bd773e997bc5acc2d6892cb3795""","""open issue related matters seem received much attention regards""",9,"9,00","55,56","88,89","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,11","0,00","0,00","11,11","0,00","0,00","0,00","0,00","0,00","11,11","0,00","0,00","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","66,67","33,33","0,00","0,00","11,11","0,00","0,00","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","22,22","11,11","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","22,22","0,00","0,00","0,00","0,00","0,00","0,00","0,00","22,22","0,00","0,00","0,00"
"""4ce4268ef0b844ab722cf9bdcc579e412df2a4bf8d392d4743e1b92839384627""","""read storm-0 http//replaced.url question work hdfs state trident topology ensure record ends written hdfs states none. trident topology processes tuples opaque kafka spout three bolts write different forms processed output form different record parallelism rotation policy applied three outputs results two streams writing small files hdfs m thinking setting different parallelism outputs reason different parallelism settings across output bolts. trying understand relationship emitted transferred values spout trident topology multiple instances topology executing expected ratio emitted transferred would least similar instances topology differ quite bit example 0x another unit measured tuple tree storm topologies trident batch. thanks helps answer question batch fails could records batch get written disk hdfs state another""",113,"28,25","45,13","39,82","10,62","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,08","1,77","1,77","3,54","0,88","0,00","0,88","0,00","0,88","6,19","3,54","0,00","5,31","0,00","0,00","0,00","3,54","2,65","0,88","0,00","0,00","0,88","11,50","5,31","1,77","2,65","2,65","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,08","0,88","1,77","2,65","5,31","2,65","0,88","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,96","3,54","0,00","0,00","0,00","0,00","0,00","0,88","1,77","0,00","0,00","1,77"
"""804a7d6bac6f39e9ab931b81ed7b0092838d087a9ab4a1ba46520d1a84533188""","""hi sam android apps created internal hobby projects connect couchdb via replication preload database remote instance documents add local instance replicated back remote instance use basic add authentication hardcoded considering use accountmanager make couchdb listen interfaces mount sdcard edit local.ini would normal pc would rely apps though since expect people installing app. curl http//replaced.url couchdb welcome version congratulations violence last refuge incompetent. possible solution might simple create virtualhost apache0 points particular documentroot file /etc/apache0/sites-available/couchdb.example.com allowoverride none sudo /etc/init.d/apache0 equivalent system /var/www/couchdb.example.com/htdocs/ following content. hi edward message would like accomplish following use flash access data couchdb requires couchdb serve file called crossdomain.xml flash application load first verify allowed flash requires file located root http server e.g http//replaced.url accomplished using virtualhost url rewrite options couchdb offers described earlier nicolas orr blitz.io neat app describes follows tried works beautifully edit domain.com zone add record blitz.domain.com create blitz.txt file put content blitz told put upload blitz.txt attachment design document get content blitz.txt attachment ok test couchdb hit root couchdb reports version however put rewrite couchdb complains many security issue fair enough 0x thus design doc looks like instead blitz course need figure hostname couchdb database would like access e.g flash.example.org make sure hostname actually resolves machine access i.e runs couchdb edit virtual host part couchdb reflect choice e.g restart couchdb afterwards think even necessary add entry futon use couchapp way like create design document called _design/flash contains crossdomain.xml file attachment rewrite statements much like mentioned curl http//replaced.url provide crossdomain.xml added attachment second rewrite points root database question allowing get documents database http//yourserver:0/flash/doc0 accessible http//replaced.url setup allow flash application connect couchdb without problems hope helps. hi 0cts although seem see nails everywhere couchdb hammer better storing xml xml database like sedna allows versioning documents xpath searches documents sent mobile. hi looking amount replies wrt topic seems much interest full text searching really hard tell would expect feature implemented couchdb way would supersede nice couchdb-lucene combo said _really simple_ probably bad solution performance wise search implementation look couchdb lists decide _view sent _list function within _list function implement full text search inspecting document data setup least allows replication functionality might enough""",400,"66,67","39,25","55,25","9,75","0,25","0,25","0,25","0,00","0,00","0,00","0,00","0,00","0,00","9,25","2,00","2,00","5,25","2,00","2,25","0,75","0,75","0,50","5,25","0,75","0,00","6,00","0,00","0,00","0,25","6,75","5,75","1,00","0,00","0,25","0,00","16,50","3,25","6,00","3,00","2,00","0,50","1,00","1,00","0,75","2,25","1,50","0,50","0,25","0,00","0,00","0,00","0,00","0,00","9,00","4,00","2,25","2,50","5,00","4,25","0,25","5,50","0,25","0,00","0,00","0,25","0,00","1,00","16,75","7,75","0,00","0,25","0,00","0,00","0,00","0,50","0,50","0,00","0,00","7,75"
"""178f713589e948dbd7b2f430525f28805f89a3fd393d8f0f76882a0dc432f5d5""","""works fine thanks herve. questions stackoverflow.com relating maven-indexer created tag re-tagged appropriately. hi herve username carlspring documentation site needs serious work lack thereof developer unfamiliar indexer must sources try figure deprecated currently tricky business unless willing invest lot time pray using right thing site documentation non-existant created separate module examples currently based tamas examples github great starting point need extended examples much illustrate bits pieces scattered across different sites gather things place make useful possible would like open issues jira chase guys filed issues order polish requirements close obsolete ones schedule relevant ones know need extra permissions gotten latest indexer work spring need using project would like add examples needs done order able fully drop plexus replace wagon code proper started working went vacation pressing work soon get chance look well kind regards. hi barrie nice hear back guys indeed jira account last time tried able assign issues could somebody please look working version maven-indexer dropping plexus polishing examples re-working certain bits api believe tamas still vacation surely explain thanks. hi would downsides like bind process need download remote repository isindex least usually small recall correctly maven centralisindex something like mb .gz talking top head recent work maven-indexer downloading index central order run tests took would people start saing oh maven slow build tool resolve million dependencies even slower switch some-next-cool-build-tool-goes-here referring maven slow lot people feeling takes ages build larger projects due resolution dependencies need update local index regular basis day least quite convinced would accepted well everyone could option dependencies could resolved mixed basis index use/update option specified ignore option specified saying current model best trying illustrate downsides switch kind regards. could always clone repositories fixes locally submit pull requests seems problem clearly questions lists right place ask. hi writing concerning bug misleading maven locks around quite preventing us able use distributed build system properly use hudson although possibility run build private repository reasonable solution large number builds due amount space need node hitting lot lately hudson running nodes using different operating systems maven problem described follows build snapshot module install locally work days commit code meantime changes module however maven-metadata-local.xml contains line tells maven update snapshots even -u using older snapshots locally solution remove artifact islocal metadata file edit manually comes regular dependencies written small plugin works around issues locked maven-metadata-local.xml files info however comes parents situation much trickier projects get loaded interpolated long would like ask proposed patch accepted/reviewed confirmed bug exists maven proposed patch david rousselie applicable maven copy five lines changesto another 0.x release could included could bug patch reviewed writing workaround plugins good idea using modified maven company therefore would like request issue fixed next version maven quite critical thanks advance looking forward replies regards. hi would like become maven committer lately contributing fixes maven-indexer current area interest working together tamas cservenak deeply familiar writing maven plugins large part core plugins see pulls currently working things maven-indexer well contribute knowledge free time project written quite plugins hosted github familiar contribution process pulls work look project already read stephen isblog becoming committer senior build release engineer agree approach believe fixes live high standard would privilege able join team kind regards time yearly committer school announcement become maven committer http//replaced.url committer.html let us know. hi furthermore feeling adding option artifact bound lead headaches hand could specify groupid includes/excludes repositories things could much better terms resolution times frankly think remote repository isjob first place people otherwise two different places handled making things complicated track later artifact routing rules exist. thanks accepting merging olivier. oh right seems broken recent merge believe 0d0ed0af0eff000ab000ae0c00d00f000e00bf0a safe revision rollback try things look get back. hi created account long time ago probably early late sure need re-register something. okay registered xcircles username seemed work. hi thinking whatever animal owl would really cool wearing sort super-hero outfit letter maven brought super-powers order proposal. hi kenneth version maven trying build building fine maven kind regards""",693,"46,20","37,23","63,35","12,84","2,02","0,29","0,00","0,29","0,00","0,00","0,00","1,73","0,00","12,41","3,46","1,88","7,50","1,59","2,89","0,58","0,72","0,00","4,18","0,72","0,00","5,92","0,14","0,00","0,72","8,23","6,93","1,15","0,00","0,43","0,00","20,92","5,19","3,46","4,47","3,90","2,16","0,87","1,01","0,87","1,88","0,87","0,29","0,72","0,58","0,29","0,29","0,00","0,00","17,17","3,03","5,48","8,51","5,05","6,06","0,87","0,00","0,72","0,14","0,00","0,87","0,43","0,14","7,22","3,03","0,00","0,00","0,00","0,00","0,00","3,17","0,29","0,00","0,00","0,72"
"""ba513f1638042a57f5f9c8cd0c995142ba498abd14ddab3ae6ae0e6b2dfc6fee""","""p.s volunteer help kip sent protonmail http//replaced.url encrypted email based switzerland""",14,"14,00","35,71","35,71","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,14","0,00","7,14","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","21,43","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,14","0,00","7,14","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","14,29","14,29","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","42,86","14,29","0,00","0,00","0,00","0,00","0,00","0,00","14,29","0,00","0,00","14,29"
"""b851c6f2472149ec2d0802860acf21ea1087b80e19565dd8a0567ba75429e6c0""","""hi martin well aware maven default build.finalname unfortunately look eclipse packages source artifacts corner case example snippet list bundles plugins directory eclipse problem maven-eclipse-plugin isto-maven intended serve default tool create maven artifact eclipse correctly recognizing bundles source suffix source artifact corresponding java artifact instead creating new java artifacts contain compiled java classes java sources assumption ask eclipse guys completely revamp builds account maven isbuild.finalname defaults would possible fix maven-eclipse-plugin generate correct maven provides possibile fix would possible apply thanks. hi current implementation maven-eclipse-plugin isto-maven given following two bundles according maven conventions create two artifacts groupid artifactid version using classifier source second bundle instead creates two completely unrelated artifacts creates problems using -ddownloadsources=true trying ago attached patch could please shed light wheter reasoning correct thanks""",135,"67,50","50,37","47,41","9,63","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,41","2,22","0,74","4,44","1,48","3,70","0,00","0,00","0,00","1,48","2,96","0,00","3,70","0,00","0,00","0,74","9,63","7,41","2,22","0,00","0,00","0,00","25,19","3,70","11,11","3,70","2,96","4,44","0,74","0,00","0,00","1,48","0,74","0,00","0,00","0,00","0,00","0,00","0,00","0,00","3,70","0,74","0,74","2,22","1,48","4,44","0,00","0,00","0,74","0,00","0,00","0,00","0,74","0,00","11,11","2,22","0,00","0,00","0,00","0,00","0,00","6,67","1,48","0,00","0,00","0,74"
"""0f1757469d1c549990a866d08ed0b74dd6f447db8d5aa88eb06ed24ef95413af""","""please send mail. website looks great. hi problem documentation kind missleading must add new element config update existing need add. could post full query url see exactly query post output debug=query show us lucene query generated. something everybody get transitioning world solr/lucene schema describes possible fields absolutely requirement every document index fields unless define solr happily index documents fields missing feel free able define people parts documents choose perhaps common fields i'll form queries like ralph assuming field people sku parts continue path de-normalization it's another thing db wouldn't document index contain data need moment find asking join stop consider de-normalization. i'd give go solr wouldn't modify library. hi michael missed elevator component standard request handler add following snippet request handler http//replaced.url http//replaced.url. hi thing describe possible set uses spanfirstquery sure it's going post debug output. hi ahmet tried follow recipe adapting solr testing right first try gave message java least file license it's license proper format see logs complexphrase tried ran successfully hope ok. registered queryparser thing i'm running multiple cores could search successfully wouldn't split already separated field multivalued= true need able search call even catch go thanks helped lot. bq it's good reason small cluster small mean 000s nodes well good reason would system continue operate zk nodes lose communication rest cluster go completely clear though zk nodes definitely wouldn't need beefy machines compared solr data nodes since light-weight orchestration yea data node system might willing go node ensemble tolerate single zk node dying depends much cash willing spend availability level looking. hi trying index collection solrinputdocs solr server wondering call make add documents add collection call blocking function call would like know add call call would longer larger collection documents thanks. yeah sure send mail plain text spam filters pretty aggressive comes anything else glance problem i'm guessing user run solr wouldn't create permissions best. hi lance well actually copied whole configuration files instead added missing configuration fresh copy example directory implementation mean readers used seems performance actually still becoming better moment average dropped even lower 00ms comparison 00ms. probably may use sanitizer http//replaced.url. query indexer i'm impressed got 0s replication work reliably. still able search already-indexed data exists values limits reached suspect solr would commit continue indexing however chance use features dih mis-configuration caused import finish without indexing anything thus wiping data aside continually index search time almost every day using. requirement solr search finds string return entire text document emails rtf format process outside solr achieve say process outside process rtf document search result return original document able successfully solr core stand alone""",442,"24,56","33,03","61,54","15,61","3,85","1,36","1,13","0,23","0,00","0,00","0,00","2,49","0,00","14,48","5,43","1,81","8,82","2,94","3,62","1,13","0,68","1,36","3,39","0,23","0,00","7,47","0,00","0,00","0,45","8,82","6,33","2,04","0,00","0,23","1,36","22,40","5,66","3,39","4,30","4,30","3,17","1,36","1,36","0,68","2,04","1,13","0,23","0,45","0,23","0,00","0,00","0,00","0,23","12,67","3,85","2,49","4,98","2,49","4,98","0,68","0,00","0,68","0,00","0,23","0,90","0,45","0,23","11,31","4,52","0,00","0,00","0,00","0,00","0,00","1,13","0,45","3,17","0,00","2,04"
"""cbbc1cfe5bddc6a69e0c3ba62cd14e45b039d4ecd166998f460cd9ca82369338""","""newbie solr done everything solr tutorial section using latest versions jdk solr see solr admin page http//replaced.url hit search button receive http error tried run solr tomcat unsuccessful solutions document links appreciated thanks help. trying overlook expected i'manilow get response though wow geoff wouldn't know musician well like gogh it's birthday two days ago names famous artists would spirit use unusual color names simpsons theme maybe oughtta stay away homer d'oh could self-referential tapestry know you're wikipedia many projects use release release number think tapestry start road map im-ing erik brainstorming theme liked city names chose favorite cities single word names portland vancouver boston amsterdam etc it's possible roadmap methods/classes probably sensible naming etc. like unless 've completely base hardest part new liberal allowances template location i'm deliberately avoiding assuming resolved handling anonymous components actually fun i'm figuring way spindle represent anon comps way able following mixed spec comp/anon comp edit bindings anon comps spec editor changes pushed pull anonymous comp template insert spec naming template component truly anonymous push spec comp template make implict/anonymous need improve validation/error handling include definitions sources brings another point would hard change tapestry parser make easy extend following possible would cool 'fatal-ness error configurable would useful spindle could set switch something parse would go completion unless true fatal error like well-formed occurred mechanism collect errors create error markers thus example validation error markers validation problems first course error occur would ignore parsers result object would nice get line/column info errors generated tapestry code probably harder last list support last thinking adding page spec editor contains template source would hardest thing accomplish much harder finally wiki page template directive indicate page it's format directive. resources defined dojo it's css served. hi edwin changes recommend bin/solr.cmd make easier work nssm please file jira i'd like help make process easier thanks. it's call release manager binding technically vote passed. shell gets it's memory config accumulo-env file accumulo_other_opts reason value low lot data loaded tab completion stuff shell could try upping value file try running shell disable-tab-completion see helps. thanks eric wouldn't cleaning something hadoop-data directory zookeeper. hi billie 've read source code documentation following iterator commonly used batchscanner accumuloinputformat parallelize search shardids means key0 key0 shradids searched misunderstanding indexeddociterator search shradids thanks regards. right got change maxversions many times wouldn't persisted disk compaction rewrites data scans however always consistent current configuration implementation accumulo instantiate current iterators minc majc scope depending it's minor major compaction write output memory/files compacted filtered/transformed iterators back single file""",439,"43,90","36,45","53,08","14,12","4,33","1,37","1,14","0,00","0,23","0,00","0,00","2,96","0,00","13,44","6,15","1,14","8,88","2,51","2,51","0,68","0,91","0,68","4,10","0,46","0,00","2,73","0,00","0,00","0,23","8,43","6,38","2,05","0,23","0,23","0,46","18,00","3,87","5,47","3,87","2,28","1,37","0,91","0,23","0,91","2,73","1,14","0,23","1,37","0,46","0,23","0,23","0,00","0,00","10,48","3,87","2,05","3,87","2,96","3,19","1,14","0,23","0,46","0,46","0,46","0,46","0,46","0,68","12,07","2,51","0,00","0,00","0,00","0,00","0,00","1,82","0,46","4,56","0,00","2,73"
"""ee79066e5e393cd954df45326b189c7d93967ef5152edd52354d057256b8eb66""","""hi hilmi tez jars required typically configured though tez.lib.uris config property thanks regards kuhu missing tez jars likely missing custom setup please follow instructions setup client hadoop environment http//replaced.url. running windows default timestamp using time.time 0e0 get timestamp twice code wouldn't use pycassa thrift api wrapper created python code implemented following function getting timestamps increases call. sake updating thread orr wouldn't yet task trackers cassandra nodes time likely due copying ~000g data hadoop cluster prior processing you're going try installing task trackers nodes. general today large amounts hints still pretty much makes node angry longer nearly nasty unless really low throughput you're probably going gain much practice raising hints window today later get file system based hints think approach work better today i'm concerned practice larger hint windows buy lot see following details http//replaced.url. running http//replaced.url fixed believe need explicitly grant select permission onto system.schema_triggers user workaround. second exception states file sstable missing possible didnt delete commit logs nfs mount stale. i'll need temporarily lower gc_grace_seconds column family run compaction restore gc_grace_seconds original value see http//replaced.url info. re-introduced corrupted node followed process thanks folks mailing list helping listed operations wiki still cleanup node point noticed seeing exception appear times minute existing node new think started around removetoken solve restart node cleanups/resets need thanks. never mind found http//replaced.url. hi tl dr superficial understanding cassandra currently evaluating project cassandra embedded another jvm application embedded instances form cluster application use failure detection cluster membership. thank reply you're sure yet use application distributed cassandra embeds point see ring real pain ass goal prevent users connecting cassandra change anything internal flexibility might point cassandra shoot-and-forget kind really sure yet best regards disclaimer information contained message attachments intended solely attention use named addressee may confidential intended recipient reminded information remains property sender must use disclose distribute copy print rely e-mail received message error please contact sender immediately irrevocably delete message copies. imo deleting always better better store column value associated. nodetool repairs bring much data lot sstables created disk space almost doubled level compactions run slow turned throtting completely wouldn't see much utilization ssd cpu example 0.0mb/s ssd insane anything speed thanks. three things first design doc talking strongly consistent reads wiki gives simple exemple read it's even followed warning actual contradiction second point design docs slightly outdated point least support quorum writes since http//replaced.url resp read provided wrote quorum resp third good recall counters considered stable yet includes documentations. right it's said proxy layer would need read result appropriate consistency level returning memcached client application client application would need declare consistency preference using configuration file. thanks dave anybody know distributed in-memory system supports structured data e.g tables. dynamic endpoint snitch works keyspace true well seeing running bug left dynamic snitch disabled unless added extra option. yes almost done make possible. range wouldn't contain nodetool ring shows token-ranges node primary range thinking primary someone confirm primary replica always changes change racks secondary replica move next replica different rack either last case next node primary replica different rack r0 contacted prove disprove stopped ran query consistency came back fine meaning indeed hold data ss tables show mean data actually gets moved around racks change probably queries primary replica replicated data read repair automatic data move rack changed least sure deprecated ignore_rack flag useful move data manually rsync sstableloader. use nodetool cfstats show keyspaces cassandra-cli see flush settings default think minutes million ops 0/00th hte heap cf created automagical global memory manager see http//replaced.url http//replaced.url. first update schema cf run nodetool upgradesstables node sometimes works node restart upgrade leaves previous format compressed uncompressed best regards pagarbiai email replaced email.addr.es follow us twitter adforminsider adform watch short video disclaimer information contained message attachments intended solely attention use named addressee may confidential intended recipient reminded information remains property sender must use disclose distribute copy print rely e-mail received message error please contact sender immediately irrevocably delete message copies. client library use""",693,"31,50","38,67","55,84","11,40","1,88","0,87","0,29","0,14","0,43","0,00","0,00","1,01","0,00","11,40","2,89","2,02","7,65","1,73","3,03","1,15","0,58","0,87","2,74","1,59","0,14","5,77","0,29","0,00","0,00","7,65","5,92","1,59","0,00","0,43","0,87","18,90","4,33","4,33","2,02","3,46","2,16","1,44","0,72","0,72","2,16","1,59","0,43","0,14","0,43","0,14","0,29","0,14","0,00","17,03","6,64","3,17","6,78","3,17","3,17","1,88","0,87","0,72","0,00","0,00","0,14","0,14","0,00","11,98","5,19","0,00","0,00","0,00","0,00","0,00","1,15","0,29","1,59","0,00","3,75"
"""af1c54dfaff10f453ee8a71ab9a5e42d8a7daa76351d7ae06b906e249a2637b6""","""new api allows cloudstack copy deltas two snapshots addition cloudstack longer needed plugin scripts plugin scripts mainly try backport code prior even plain vanilla work api cloudstack source code refer xenserver combination xenserver0.00 indicate significance change. 'it's columns it's rows see example statement queryprocessor processstatement reads rows list. apparent behavior nodes simply answer let timeout it's alternative would unreasonable amount work asked extreme cases understanding result oom restarting cassandra clears issue something misunderstood error pasted said aborted scanning tombstones alternative bad performance really wouldn't create many tombstones restarting node impact whatsoever many tombstones sstables cassandra it's current behavior protecting bad design general cases delete whole bunch stuff added time occasionally bad log structured said could fact run major compaction tiered compaction gc_grace_seconds passed remove tombstones re-iterate tombstones symptom cause. event weekend germany used couchdb replication replication crashed able restart actually i'm sure stable replication couchdb case trouble seems get difficult get information working. thanks makes sense given evently it's limitations. code provided first lib think referring code snippet use firebug look output headers response look like would expect log request shows info sure something small missing stumped. confirm using couchapp generate design doc keeping view show funcs files included files couchapp area. http//replaced.url. hopefully i'll rebar point. i'll do something like call like get like. access query data apply update handler like allows update value xyz like way access data sent via post. close jan it's except i'm inserting data i'm running. app production handle backups way you're describing every night push zip archive s0 works great. yes running n=0/w=0/r=0 bigcouch couchdb merge see behaviour today it's couchdb 0.x disclaim say ektorp driver part apache couchdb knowledge http responses post-merge ektorp 0rd party drivers need enhanced understand report responses arise scenarios. know noah unless setup uses heartbeat option new couch might start old goes away crash eaddrinuse without error init it's restart user finds without wouldn't know i've happen centos. i've answered read old thread. hi suresh easy way overcoming error add hadoop mapped-site.xml files hadoop installation tomcat/lib directory restart oozie server please let us know resolve issue regards""",365,"21,47","34,25","58,90","13,70","6,03","2,47","1,92","0,27","0,27","0,00","0,00","3,56","0,00","15,07","6,85","1,92","10,41","1,92","1,37","1,37","0,27","1,10","2,47","1,92","0,00","3,29","0,00","0,00","0,00","5,21","3,56","1,64","0,00","0,00","0,27","20,82","5,75","4,66","2,47","2,74","1,64","0,82","1,10","2,47","2,74","1,92","0,82","0,00","0,27","0,00","0,27","0,00","0,00","12,05","3,56","1,92","5,21","2,19","3,29","2,19","2,19","0,27","0,00","1,37","0,27","0,00","1,37","14,52","5,21","0,00","0,00","0,00","0,00","0,00","0,82","0,55","5,21","0,00","2,74"
"""2d42c4ecc5cfc3ffed8d308fce4b7673b599cefabe78b400cb31223353b3875f""","""hi remove lock file solr//data/index. hi far know never good idea run lucene openjdk either oracle java higher openjdk. hello majisha codepoints content field however stripping method built invalid middle byte exception mind seen even solr 0.x upgrading parts infrastructure solr 0.x got struck confirm content field sent nutch causes. set proper permissions tomcat. either upgrade tika manually use pdfbox. well somewhat problem url isuniquekey contain exclamation marks idea allow escaped thus ignored compositeidrouter. hi use wild cards autocompletion lucene far better tools making good autocompletion since wild card term query passed configured query time analyzer comments use porter stemmer use german specific stem index time tokenizer defined possible behaviour undefined far know. pkill work. not elect leader""",122,"13,56","29,51","46,72","7,38","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,38","0,00","2,46","4,92","0,00","2,46","0,82","0,82","1,64","3,28","1,64","0,00","4,92","0,00","0,00","0,00","4,92","3,28","1,64","0,00","0,00","0,00","22,95","5,74","8,20","0,82","1,64","1,64","1,64","0,00","2,46","0,82","0,82","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,11","4,92","4,10","3,28","1,64","2,46","0,82","0,00","0,00","0,00","0,00","0,00","0,82","0,00","12,30","8,20","0,00","0,00","0,00","0,00","0,00","0,00","1,64","0,00","0,00","2,46"
"""45845ce8346c2adfd741afb3553eaabad2c155fe4153e168f3ebb4d99f47f1a4""","""hi someone else might application would like send even though application may send fatal different logger first post makes sense configure logger invoicing later configure logger different properties access two loggers code logmanger.getlogger invoicing please first version two different loggers app using appender set level logger perhaps define another root logger invoicing configure global settings invoicing loggers every single fun. see example projects copied example remember perhaps find usefull. see internal log0net logging see. hi yes mamanged kick live following problem app-domain definied powershell workingdir powershell-snapin path located path way specify location configfile let know still need see lines code. message local queue service host processes time beeing bit specific service independed application holds references logging application isdlls words using generic service remote applications customized version every single kind objects log queue strings perhaps xml real bussiness message objects service hold logic getting msgs queue without looking inside forwarding ado far see thing win kind async logging price pay extra level design service app depends additional tech msmq think non-blocking app ok msgs logged possible wrong order long timestamp shows real date simply build depender following decorator pattern new appender holds property referencing real appender ado-appender case every time appender asked logg new message creates new thread starts leave inside new thread recieved msg passed real appender less lines code whole appender every logginrequest aync creating new thread even faster writing msmq plus add additional logic inside decorator wait writing db long x msgs recieved using threadpool store msgs long network unavailable whatever use apps becourse totaly independend rest app would put inside independed course appender reused appenders ado simply adds asynfunctionality every appender choose. know set properties depending appender use declaration follows generic description given docu reading docu know discover possibilies appender provides knowing follow docu setting properties config given example tag tag set top level config declare custom levels extensions provide said found docu maybe tags best thing would xml-schema config reads issue tracking config follow strict schema searching complete plain list xml tags config like etc. hallo able reproduce problem config runs without errors system time study log0net sources tell totaly way configure loggers reason repository stores logers hashtable logger used key table impossible store two loggers happens xmlconfig like loops root xml elements every logger element calls getlogger get logger wich create new logger return allready created exists level set/overriden appenderrefs erased recreated logger allways configured like last definition configfile profe simple hard say fileappender crashes using target file processes perhaps log0net able open filehandle runs write closed appender. perhaps simply use threadcontext store additional infos could store additional infos like. hi alex think line wrong use implementation ilog wrapper implementation checks implementation sets locationinfo onto position point _log instance code logging change ilogger log messages like copied code logimpl.cs loggerwrapperimpl.cs damm good peace code fun reading. well first code filters without toughing log0net code simple add fqdn config simple prevent log recieve specified level use buildin filter levelmatchfilter configfile possible know filter log0net defining custom loglevels easy without toughing code see trace example provided sources. hi shouldnt becourse created instance logger stored repository aplication ends use loggername get instance problem use logmanager repositorieisgetlogger member create instances wich result multiple instances logger need implement iloggerfactory interface create custom subclassed logger configure hierarchy use factory use logmanager repository create loggers instance created. research checked log0net sources error thrown appenderskelleton base appenders m_closed m_closed set reads code docu programming error append closed appender perhaps really bug log0net appenders post simple possible config reproduces error system try reproduce debug log0net see whats going wrong appender. thx pointing deliver messages appender fatals delivered. hallo perhaps provide hints get complete list log0net studied great docu examples googling found important tag listed docu ok least beside checking source parser anywhere full list available looking something special know possibilities. hi basic log0net things named logger using loggername allways work instance logger configured named logger need repository stores reference configured thought workflow like configure logger use custom logfile bad bad softwaredesign yes would affect threads use logger use two explicit loggers every location messages calculation lets say following locations location0 used logger could new logger inherits level appender message calculation like configured configfile every message goes child loggers logfile location see simple custom logmanager providing new members getmessagelogger getcalculationlogger location parameter return configured logger ilog precise log0net.ilog. problem use every message needs checked matchs filter perhaps set filterlevel inside definition would speed things. see point building sources coding log0net sources simply use log0net projects way add reference release like debug log0net.dll included inside zip thats""",774,"45,53","41,47","53,10","11,24","1,29","0,13","0,00","0,13","0,00","0,00","0,00","1,16","0,00","9,82","1,68","1,29","7,11","1,03","2,58","1,68","0,26","0,65","4,39","0,90","0,00","4,13","0,00","0,00","0,26","5,17","3,49","1,68","0,00","0,00","0,00","20,28","3,23","6,20","2,20","2,71","2,20","0,78","2,07","0,78","2,07","1,42","0,39","0,26","0,00","0,00","0,00","0,00","0,00","13,05","2,58","4,13","4,78","2,71","2,97","0,78","0,00","1,16","0,00","0,26","0,52","0,13","1,03","4,13","2,71","0,00","0,00","0,00","0,00","0,00","0,65","0,26","0,00","0,00","0,52"
"""b13fd8f0636336d4fe72610e907dd976f348a4f2a632d66b0ce15063e2391ff9""","""hello everyone hope email finds well hope everyone excited apachecon would like remind couple important dates well ask assistance spreading word please use social media platform get word visibility better apachecon link main site found http//replaced.url planning attend apachecon north america may add-on option registration form join conference discounted fee us available apache big data north america attendees please tweet away look forward seeing vancouver groovy day""",70,"70,00","32,86","72,86","17,14","4,29","1,43","0,00","1,43","0,00","0,00","0,00","2,86","0,00","12,86","2,86","1,43","8,57","2,86","2,86","2,86","0,00","0,00","4,29","0,00","0,00","12,86","0,00","0,00","0,00","14,29","14,29","0,00","0,00","0,00","0,00","17,14","4,29","1,43","4,29","5,71","0,00","0,00","1,43","0,00","2,86","2,86","0,00","0,00","0,00","0,00","0,00","0,00","0,00","17,14","4,29","11,43","5,71","4,29","4,29","0,00","0,00","2,86","0,00","0,00","0,00","2,86","0,00","8,57","1,43","0,00","0,00","0,00","0,00","0,00","1,43","2,86","0,00","0,00","2,86"
"""f8edcfb93545238bb180cd8e2deec494f9b41dc5bc16de388bc5e0a6a542cc71""","""hi first sorry form bad english new user apache solr read documentation check find solution problem need words order sensitivity query example two something two something three something four four something three something two something query two result something two something three something four two like query four something three something two something need result disturb returned query two result four something three something two something two like query something two something three something four need result disturb returned possible yes regards""",82,"82,00","36,59","91,46","53,66","21,95","0,00","0,00","0,00","0,00","0,00","0,00","21,95","0,00","3,66","0,00","0,00","3,66","0,00","0,00","0,00","0,00","0,00","2,44","29,27","0,00","2,44","0,00","0,00","0,00","6,10","0,00","6,10","2,44","0,00","0,00","42,68","8,54","6,10","4,88","23,17","0,00","0,00","0,00","21,95","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","6,10","0,00","0,00","4,88","1,22","2,44","1,22","0,00","1,22","0,00","0,00","1,22","0,00","2,44","2,44","0,00","0,00","0,00","0,00","0,00","0,00","0,00","2,44","0,00","0,00","0,00"
"""8d813ba09858cc9de6303df077cde873630baa77238f6ea9e7a073808a56522a""","""clearer rich text document xml make work example docs folder read solr book tried samples could make work. read solr book book way read need help mean time using example solr system send word document using curl system full path document tried various commands gives stream errors documents server solr second server would like experiment home indexing documents server giving experience outside b trying grasp issues design read books struggling grasp ideas yet using .net connector recommended webpage connects solr adds extra information solr query client see later returns data normal design using shopping trolley design orders created comments added order processed search order information many indexing requirements index many database tables proposing use solr instance indexes data use example text field copydata extra fields tables solr book gave short shift database indexing tool good tutorial using solr sqlserver many tables thinking might join lot might easier create xml output send pros cons way restricting searches client signed good idea secured behind application adding extras thinking would index order notes separately order alternative value note field order keep deleting best way keep replacing whole order join notes search like feature suppose solr index normal map url code using return data solr hope help. read various pages used curl lot figure correct command line add document example solr instance tried things however seem file server solr case pushing document windows machine solr indexing. iscomplete example""",234,"58,50","28,63","55,13","11,97","0,43","0,00","0,00","0,00","0,00","0,00","0,00","0,43","0,00","12,39","2,14","2,14","8,12","1,71","0,85","0,85","0,43","0,00","7,26","0,43","0,00","3,85","0,00","0,00","0,00","5,56","5,13","0,43","0,43","0,00","0,00","20,51","5,13","5,98","2,99","2,99","0,43","2,14","0,43","0,00","1,28","0,43","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,54","1,71","2,14","3,42","6,41","4,27","4,70","0,85","1,28","0,00","0,00","0,00","0,00","0,43","2,56","1,71","0,00","0,00","0,00","0,00","0,00","0,00","0,85","0,00","0,00","0,00"
"""999076a19d33ea80f5d6380fe64bf5094611a510bd64fd29ef8ff26fb467c42e""","""set apache reverse proxy couchdb problems replicating idea setup apache later handle authentication however even disabled get following failure trying replicate apache runs port forwarding couchdb error get error part database actual document occurs seems vary unable reproduce behaviour smaller database well mb already tried dumping reloading database contents nothing changed proxy configuration follows hope fairly standard proxyrequests allowencodedslashes normal requests couchdb futon interface seem working fine proxy ideas problem could thanks""",72,"72,00","48,61","59,72","9,72","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,89","1,39","2,78","9,72","0,00","4,17","0,00","1,39","1,39","2,78","0,00","0,00","1,39","0,00","0,00","0,00","11,11","5,56","5,56","0,00","1,39","1,39","20,83","5,56","2,78","6,94","6,94","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,89","8,33","2,78","4,17","2,78","6,94","0,00","5,56","0,00","0,00","0,00","0,00","1,39","0,00","2,78","0,00","0,00","0,00","0,00","0,00","0,00","0,00","2,78","0,00","0,00","0,00"
"""35d689bead6b31c10cbf7e8bd33da99ac9e6dd77ca7675f3226cb4a4faa88f56""","""hello quite new solr trying get grip currently reading enjoying solr enterprise search server book trying failing need advise following given following schema subscriptions would like group accountid facet isclosed accountid addition would like see number created subscriptions since given date would possible figure grouping something like advise would highly appreciated regards. ok shall try answer would correct give data need indent=on f.createddate.facet.date.start=now/days-0months f.createddate.facet.date.end=now. yes see question bit confusing thanks answers try clarify bit query date field validtodate value field present documents would like get number documents given date range r0 value validtodate i.e documents number documents given date range r0 value validtodate question really possible query need two queries facet.range.other=all help way. hello following faceting parameters gives unwanted non-null dates result set way query index give non-null dates return i.e would like get result set contains non-nulls validtodate faceting non-null values validtodate would like get non-null values faceting result response example gives results non-null validtodates would like get results non-null validtodate facets write start wonder possible facets dependent result set might better handle application layer extracting help would appreciated""",205,"51,25","31,71","68,78","12,68","2,44","0,98","0,98","0,00","0,00","0,00","0,00","1,46","0,00","17,56","5,85","1,95","9,76","5,85","1,46","0,98","0,00","0,00","2,44","0,49","0,00","7,32","0,00","0,00","0,00","12,20","9,76","2,44","0,49","0,00","0,49","25,37","6,34","5,37","6,83","5,37","0,98","0,49","0,49","1,46","1,46","0,98","0,00","0,49","1,46","0,00","1,46","0,00","0,00","13,17","1,46","0,49","11,71","2,44","4,39","0,98","0,00","2,44","0,00","0,00","0,98","0,00","0,00","14,63","7,32","0,00","0,00","0,00","0,00","0,00","3,90","0,98","0,00","0,00","2,44"
"""e1e776cd2f7efc4f5b2ac35594d0c991db9b41bceceb2e089500d784af1c2c65""","""hi using solr store time series data log events etc right use solr cloud collection cleaning deleting documents via queries would like know approaches people using way create collection receiving post inexistent inded could use date part index cleanup process would delete old collections""",44,"44,00","29,55","61,36","11,36","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","18,18","6,82","0,00","11,36","4,55","0,00","0,00","0,00","0,00","6,82","0,00","0,00","6,82","0,00","0,00","2,27","4,55","4,55","0,00","0,00","0,00","0,00","22,73","4,55","11,36","6,82","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","18,18","4,55","2,27","9,09","0,00","2,27","0,00","4,55","2,27","0,00","0,00","0,00","0,00","0,00","4,55","0,00","0,00","0,00","0,00","0,00","0,00","0,00","4,55","0,00","0,00","0,00"
"""94fb5e656357db4b2bbc7f9ce3f56241293b797ffc31e5c3c0773e5ac3e33c07""","""trying use archiva proxy maven central instance configured following configured local maven installation use repository see lot checksum validation failed issues warning could validate integrity download maybe use see archiva secure tls go directly maven central issues tried different combinations like maven central group using maven central group makes insane slow still validation errors manually download respective sha0 hash file see expected hash see maven suddenly think value start anyone ever seen anyone help kind regards. hi way group values like shopping.yahoo.com shopper.cnet.com instance documents like i'd like result grouping product value range product something like like current facet information grouped item entire result idea thanks. performance difference using fields defined schema vs dynamic fields. case getting results stop word issue. top shawn rightly said two things try benchmark best bet solution without shingles know better story numbers tell go shingles approach consider removing duplicates. chromosome gene object types really boils able give number e.g return documents regions containing number i'd document list like look build features using spatial believe david covered usecase talk san diego get error able find abstractsubtypefieldtype first bit trace hints wrong provide source code fuller stack trace config settings etc try unpack solr.war stick jar web-inf/lib repack however get noclassdeffounderror plugin fuller stack trace might help key question order try two approaches exactly fieldtype declaration look like tried repacking first maybe exploded still polluted old jar repacked multiple copies plugin initial noclassdeffounderror could try starting competley clean using stock sample configs make sure get errors try declaring custom fieldtype using hte fully qualified w/o even telling solr jar ensure get noclassdeffounderror custom get error abstractsubtypefieldtype still directive load jar load still wouldn't work provide us details container solr version full stack trace details configuring declared filesystem looks like solrhome etc. thanks sorry wouldn't really solr-related monitor wouldn't rely output free command think could still achieve significant improvements going performance tuning advice wiki. see http//replaced.url basically list special characters text file types attribute map alpha. hi trying implement auto suggest functionality currently looking terms component solr example form query like searches field strings starting results could possible get info results component like return data fields example along results field corresponding data telephone field maybe simply execute normal wildcard search thanks. thanks exactly every incoming query via entry custom requesthandler thus question reference original query text requesthandler xml believe i'm going use simpleparams syntax yes ideally would simple requesthandler would recognize q magic variable holds current search text came q url i'm guessing wouldn't access query requesthandler without inside brackets simpleparams like however pointed alternatives believe 've found easier way least case think ensures standard search catchall field text happen boosting additional search occur boost listed thanks caveat boosting function i'll set expectations users wouldn't limit search docs figo could guarantee show boost seems best compromise unless parse search results code i'd rather avoid whenever possible. displayname displayphone even schema print solrdocument object directly see results. able read indexes produced way back including 0.x sometimes experimental formats excepted case fine since you're upgrading always though i'd recommend copying indexes someplace paranoid upgrading best. different solution need i'm measuring response times different collections measuring online/batch queries apart using new relic 've added filter analyses request makes info available new relic request argument built new relic solr plug wouldn't provide much. thank. continuing fight keep solr setup functioning result made significant changes schema reduce amount data write setup new cluster data initially ran import replicas achieved quite impressive results peak new documents minute shard loses outages due garbage collection issue see production load index stood documents 00gb shard highest insertion rate would say querying suffered concern right added replica shard indexing time doubled surprising good start problem continue write leaders issue replicas continually going recovery leaders show ioexception occured talking server replica busy garbage collecting wouldn't coincide full gc collection times low replica appears accepting adds milliseconds appears log org.apache.solr.handler.admin.coreadminhandler requested recover reduced load documents minute appear stay couple minutes would like confident could handle peak times initially getting connection reset errors leaders changed jetty connector nio message received upped header request response ideas using replicas proposed colleague thanks much advance. hello shawn thanks reply look asap know dev environments persistent flag set true i'll check others production see someone get copy logs production environment see detail contained within thanks. ahhhh it's lightbulb last paragraph cleared confusion carrying responses incidentally likely reason confusion zk question could wouldn't make sense thinking zookeeper separate means handling things solrcloud two entirely different approaches scaling much helpful see balance assumption thanks shawn-. detailed perhaps alphabetical hierarchical table contents ether wikis sole site sent mail android. demonstration feature would good addition example/multicore directory. hello index fields dynamically wouldn't suit need wouldn't know fields advance fields must set dynamically need strong typage think solution handle programmatically best way custom handler api use. using example stack trace looks like error finding solr.home index directory configured example schema.xml works try adding little bit schema time""",850,"40,48","39,29","58,12","12,59","2,35","1,29","1,06","0,12","0,12","0,00","0,00","1,06","0,00","14,94","4,12","1,41","10,00","2,24","2,59","0,82","0,59","1,41","4,47","0,71","0,00","6,24","0,00","0,12","0,00","6,94","4,94","2,00","0,35","0,47","0,47","22,24","4,94","5,88","3,76","3,41","1,76","1,06","0,71","1,06","2,35","2,00","0,24","0,00","0,47","0,00","0,35","0,00","0,12","12,35","3,06","2,35","6,12","3,18","6,12","0,59","0,24","0,82","0,12","0,12","0,12","0,00","1,29","8,24","4,12","0,00","0,00","0,00","0,00","0,00","0,35","0,24","2,82","0,00","0,71"
"""f2a3b32a84d8e62507db6d4ca3c640fc3d6c267dfd5a9eb829266482f859f9fd""","""hi still problem projectbuilder followed api change projectbuildingrequest see following setup gist problematic code works version beta-0 gets npe version beta-0 later hints welcome better regards kristian. plugin use maven0 api maven-beta-0 gives maven-rc0 gives maybe used projectbuilder maybe part public api maybe writing plugin regards kristian. trying understand various options worddelimiterfilterfactory tried setting options seems prevent number words output particular wouldn't 00dxl wouldn't get output wods containing hypens correct behavior solr analyzer output schema. hi you're trying push security related info index control users search certain fields you're wondering best way accomplish records indexed searched certain fields marked private field marked private querying users see/search whereas super users it's solutions you're considering index separate boolean value new _internal field indicate corresponding field value marked private include filter query searching user super user eg. consider record contain fields field000 field0 field0 marked private field0 record field0 marked private record b field0 index records it's i'd index searching user super user query let it's say security needs look like this- field0_internal:0 field0 security field0 security manipulating query way seems painful error prone we're wondering solr provides anything box would help determine fields query depending visibility using example it's indexed records would look like searching user super user query needs regular fields whereas searching user super user query needs regular internal fields issue solution since number docs include internal fields going much fewer you're wondering relevancy would messed you're querying regular internal fields thanks. particular reason would limit documents facet calculation mean whole point facet numbers let users know it's must rationale mind. personally i'd stick solr it's built-in dynamic field definitions keep things smooth future developers ease matching i'll see list via support channels use field aliasing know dynamic field fl=price price_f sort thing client deal friendlier names wouldn't think reasons best practices dynamic field naming conventions personally i'd use _/underscore separator though even legal characters support it's java identifier i'd steer clear http//replaced.url. kinda late party interesting thread i'm wondering anyone using solrcloud hdfs large scales really like capability since data inside hadoop run solr shards nodes need manage metal although cluster actually nodes run typical query takes seconds run faceting clustering minute range depends queried little seconds index contains fields looking switching different method indexing data involve much larger number fields little stored index index help improve performance 've used shardsplit success server split needs triple amount direct memory using hdfs node needs three x amount running three shards lead it'swap you're careful large indexes split long time run much longer rest timeout monitored checking zookeeper it's clusterstate.json. first starting play dataimporthandler dih cool rather simple case indexing rss feed contains articles articles entry database contains url article rating config appended db looks like rss feed comes blog heh it's convenient control question let it's say initial set ratings feed full import articles feed everything peachy far get new rating existing article 've already indexed thus child entity named delta however run delta-import wouldn't pick changes since believe parent wouldn't changed either something wrong seems like akin parentdeltaquery problem course parent query since parent table db sense least see relevant logs case handled suggestions alternatives help would appreciated thanks. hmm 've seen bug like wouldn't think would tickled replicating config files def looks related though i'll try around next time happens look slave files index slave plain 'index timestamp part timestamp.index. hi lot posts talk hardening /admin handler user credentials etc hardened considering fact could allow secure external access admin interface allow proper cluster work setting security admin/cores option thanks. errors persist complete index rebuild wouldn't done extensive checks far index seems work correctly need concerned thanks. hi noticed use removeduplicatestokenfilter query time consider term positions really anything e.g query 'term term term far see term positions make difference simple non-phrase search built-in way deal know write filter feel like would something quite basic query wouldn't think it's even anything weird normal users consider e.g searching music verified least according anecdotal evicende search really slows repeat term enough. thanks chris yes makes sense i'll experimenting along lines hitcollector solr standalone programs well yield meaningful resolution would benefit community it's interest i'll concept patch mentioned. could create list field might well create add fields schema.xml original point able refer field username string define explicitly solr reread schema file post add action starup solr reads schema.xml file indexschema object builds schema.xml used pervasively document adds searches understand use field general wonder adding suffix dynamic fields posing think user programmer it's intuitive think integer therefore enter searching think it's definitely tradeoff dynamic fields make easy add arbitrary fields information infered naming convention use names cleaner names create explict fields advance. defaulting behavior since forever changing behavior going happen making fit new version correct change behavior every application specified default behavior it's a-priori reason expect words equal fewer docs easily argue words return docs expect depends mental model providing default request handlers allows implement whatever model application chooses best replaced email.addr.es wrote. would use bq parameter boost question_source==0 documents first similar. look http//replaced.url it's pretty decent explanation memory mapped files wouldn't believe default configuration solr use mmapdirectory even understanding entire file wouldn't forcibly cached solr it's filesystem cache control it's actually ram eviction process depend thanks""",905,"53,24","33,15","59,34","15,03","5,08","1,77","0,99","0,11","0,66","0,00","0,00","3,31","0,11","15,69","6,63","0,99","10,94","2,76","2,21","1,10","0,66","1,10","4,31","0,99","0,00","5,86","0,44","0,11","0,11","7,51","6,19","1,22","0,00","0,11","0,00","24,86","7,85","5,86","3,98","3,54","1,66","2,10","0,88","0,77","2,32","1,44","0,33","0,55","0,99","0,00","0,55","0,00","0,44","10,50","3,31","2,87","3,76","1,66","3,98","0,77","0,11","0,77","0,11","0,00","0,55","0,33","0,77","11,93","3,09","0,00","0,11","0,00","0,00","0,00","1,22","0,22","5,52","0,00","1,77"
"""b25f329d7482d6ec4cbac4b7b82804b156f42ac3601340af82dce72c0b989f25""","""omri need indicate solr at_location field accept multiple values add field declaration see reference information options. makes next i.e still answered question next previous really means already know next page another query get next record. hi henri make sure container running solr set utf-0 example tomcat server.xml file connector definitions include join conversation like us facebook follow us twitter. oops sorry hijacking thread put real subject place. need include unique field fl parameter needed anyways match highlight fragments result docs highlighting working join conversation may even get ipad nook like us facebook follow us twitter. hi anyone ever set things figured good way access solr filter chain java code specifically would like tokenize data search strings possibly even facet values way solr filter chains additional pre-search processing e.g tokenizing way produce better fuzzy search query post-search processing find matches within facet values thanks advance. well sort depends mean previous next record sequencing built concept solr lucene indexes sequential isi.e use case data available support use case. hi mark used dih shall need leave comments set others done another question initial index create delta run commit run optimize without optimize deleted records still show query results. actual query look hl.snippets parameter join conversation may even get ipad nook like us facebook follow us twitter. could add standard field shard populate distinct value shard facet field look facet counts value corresponds shard hey-presto done. hi michael well stock answer it depends example would able search filename without searching file contents would always search together copy file parsed file content pdf single search field set default search field kind processing normalizing data case insensitive accent insensitive word contains camel case e.g theveryidea split case changes watch things like ipad word contains numbers left together separated stemming searching stemming would find stem stemmed sort thing always english languages involved text processing indexing vs searching able find hits based first characters term ngrams able highlight text segments search terms found probably read various tokenizers filters available prototyping see looks basically one fits part power solr lucene configurability achieve results business case calls part drawback solr lucene especially new folks configurability achieve results business case calls anyone got anything else suggest michael. hi nizan realize replying thread email client would get back isinfo thread since replication replicate top-level solr.xml file defines available cores dynamic cores requirement custom code wasted. hi sorry duplication seems like sent yesterday never made troubles solr spellcheck response running version overview search something really ugly like get back response suggestions list rck suggestions list two words book fine spelled correctly i.e got hits word suggestions ugly thing though hits problem handling result tell difference suggestions correctly spelled term suggestions something odd like happening searches obviously garbage i.e words real words show index suggestions illustrate point setup running multiple shards may part issue example book might found shards another think anything schema since really search suggestions returned us bits pieces would really like see response coming back indication word found suggestions hacked around code little bit wondering anyone across approaches taken created new classes extend indexbasedspellchecker spellcheckcomponent follows package imports excluded sort brevity methods taken overridden classes changes noted sd comments modification allows correctly spelled words returned suggestion modification working tandem sirsidynixspellcheckcomponent allows words suggestions returned spell check component even sharded search changes marked sd comments modification designed may work tandem sirsidynixindexbasedspellchecker return mispelled words suggestions flipped false suggestions index true thesuggestions null //sd removed thesuggestions.size shardrequest allow misspelled words suggestions thesuggestions.size always true hence removal continue //if shardrequest word mispelled add list mispelled words else extendedresults suggestions.size word misspelled added suggestions freqinfo isxml getting back search applying modified code thanks. looks like specified value pdfy reflected results query query searching vpn hence matches query yield. using unique solr index sounds like may value solr index unique bears resemblance unique derived data another way put makes two records solr index the unique istwo entries solr index isrelated original data unique immutable i.e update row database unique derived row would update otherwise nothing solr recognize duplicate entry delete insert instead insert. oops sorry missed well multivalued setting explicitly allow multiple values actual use case i.e multiple values field multivalued problem trying solve. without speaking directly indexing searching specific fields certainly possible retrieve xml file solr db allow binary field associated index document store gzipped xml file binary field retrieve certain conditions get original document information found solr handle much faster db regularly index large portion documents xml files prone frequent changes keep blob solr index make sure retrieve field really xml files relatively static i.e change rarely new ones still might make sense use real db store keep primary key db row solr index join conversation like us facebook follow us twitter. reading wrote originally wrote think misunderstanding based architecture code code server level solrj indexing calls meant server solr instance mean client thinking without thinking server sorry hopefully someone else chime specific view message context http//replaced.url sent solr user mailing list archive nabble.com. suppose something silly like fact indexing chain includes words= stopwords.txt query chain register february get time viewing three course circulation basics self-paced training suite. hi downloaded file unzipped see inside file tried unzipping errors look inside file see java code example ended unzipped .java extension example path apache-solr-0.0\lucene\backwards\src\test\org\apache\lucene\analysis\tokenattributes see two files ideas specific tool using expand windows xp thanks join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. use replication call inexperience really early working fully understanding solr best way approach various issues mention prototype non-production code covered though look replication feature thanks. possibility consider really need documents specifically empty non-defined values oxymoron control values send indexing could set special value means no value done similar vein using something like empty given field meaning original document actually value field i.e something unlikely real value easily select documents querying field empty instead negated form select however considered things like index relatively rare us value gut feel impacting indexes much size-wise performance-wise. thing think post-process snippets i.e pull highlighting tags strings look match result description field looking match find replace description original highlight text i.e highlight tags still place join conversation may even get ipad nook like us facebook follow us twitter. hi indexed various types documents fields author already able use facet choose values narrow given use case runs something like example list authors matching steel count number documents associated user chooses entries gets document results author present sideways essentially present facets first results choose facet show results facets show match fashion based analysis chain based fuzzy search search term entered know get list author facet values documents steel matches author field problem author multi-valued field returns facet values match steel values author field really ugly approach work time hoping someone better idea read facet parameters searched various places across anything like use facet.prefix looking facets begin search term looking facets contain search term could show anywhere fuzzy handling may exact matches anyways masochists know approach search something like searching documents author_boost field internal author field search term mark twain proximity distance somewhat arbitrary return hits really kludgy bits hoping enough hits hits would include mark twain however specify fl fields return field never exists getting back empty elements xml highlighting author_boost field tells us value search terms found sort document kind random sort try get many distinct highlighting results possible i.e score sequencing would cluster highlight values post processing build set strings highlighting results removing highlight elements intent becomes set mark twain strings chug facet_field list author_facet preserve entry set strings built highlighting results present result back users along counts facet really ugly usually work help visualize isexcerpts response join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. hi hoping someone thoughts running solr patch solrqueryparser.java getluceneversion calls use lucenematchversion directly running solr cores schema use solrj run searches java app running jboss jboss tomcat solr index folders server case relevant using jmeter load test harness running solaris processor box 00gb physical memory run successful load test user load rate solr searches second solr search responses coming 000ms tried ramp far tell solr hanging logging statements around solrj calls log long query construction takes run solrj query log search times getting number query construction logs corresponding search time logs tomcat jboss processes show well cpu still top processes cpu states show around idle usage two java processes around 0gb lwp state shows sleep jboss still alive get piece software talks jboss app get data set things use log0j logging solr log showing errors exceptions indexing searching back january load testing prototype problems though solr time ramped beautifully bottle necks apps solr benchmarking descendent prototyping bit complex searches fields schema basic search logic far solrj usage ideas else look ringing bells send details anyone wants specifics. hoping sort work multivalued field normally trying makes sense example two authors document would expect document sort issmith jones probably specific rule choose least generic sense wanted example able sort first author could index first author separate non-multivalued field purely sort still authors multivalued field join conversation like us facebook follow us twitter. set new field concatenation field category group facet many combinations would talking field run query something bit similar wanted author search author field documents field set based author search well field based author faceting search author field return results facet values display facet values counts users select issue new query return documents author facet value. setting reading thread seems could search macman hit macman maecman since seems could map single replacement mapped multiple times generating multiple tokens thanks view message context http//replaced.url sent solr user mailing list archive nabble.com. make field multi-valued field indexing time split text sentences putting sentence solr document values mv field think normal highlighting code used pull entire value i.e sentence matching mv instance within document i.e put overhead index step rather trying search time. put default value group_id field solr schema would work e.g something like unknown shall get original group_id value still grouped together figure display time""",1752,"58,40","33,45","53,82","13,98","3,82","1,88","0,80","1,08","0,00","0,00","0,00","1,94","0,06","9,13","2,11","1,77","5,48","1,43","3,48","1,08","0,40","0,51","3,48","0,80","0,00","6,79","0,06","0,00","0,06","5,88","4,57","1,31","0,00","0,29","0,29","19,86","4,62","4,34","2,23","4,22","1,77","0,57","0,86","1,77","1,71","1,26","0,17","0,29","0,46","0,17","0,29","0,00","0,00","11,76","3,48","2,34","4,91","2,05","2,34","1,26","0,06","1,08","0,00","0,06","0,00","0,29","0,86","6,28","3,82","0,00","0,00","0,00","0,00","0,00","0,91","0,11","0,00","0,00","1,43"
"""f9624337619765efae4f99a7c5ab852f2712306cb84c17cf189f4aa7afb8bad9""","""well could magnitude notation approach depends complex strings based examples would work identify series integers string assumes lengths series insert number integers string integer series sorting would sort string00 string00 string000 use original strings displays. hi relatively special case parent child relationship trying model currently using solr lucene example parent documents represent information e.g bibliographic information parent document contain children child representing physical copy book information e.g think library multiple branches child documents represent books format available given branch think special set information need solr lucene make use facet based need facet counts child facets example harry potter last crusade j.k. rowlings upcoming block novel following information etc etc bit simplified actually fields involved child document five fields used individually easy combination much harder refine result set relatively straightforward approach location format ordinary everyday facet fields certainly get results although ignore facet counts search book without facets applied get back facets like narrow things work though logically hole trying fill example suppose user chooses narrow location branch format dvd still get hit back child record values user looking dvd isbranch library dvd main completely controlling indexing searching side code i.e formulate document content indexed parse results presenting users approach thinking brute force method accomplishing using facets using facet.prefix parameter query could generate facets like narrowing single facet e.g location branch would usual facet search something like would request back facets like parse values returned location-format_facet retrieve follows branch- prefix would facet values format facet presented users book remains value fields pretty straightforward could somewhat simplified two facet fields instead four keeping paired facets using singleton facets retrieving paired fields limiting taking place parsing pairs location format facets limiting element use facet.prefix limiting choose facets look concatenated value however gets complex ramp fields generally requires individual facet fields number underlying fields i.e two fields two facet fields needed solr/lucene index support three fields could facets required fields would required facets getting bit much hmmm little scribbling ok fair bit scribbling actually reduce facet fields cover fields maybe bad interesting actually coded anything yet paper-napkin level exercise thing done perused various archived threads upcoming functionality regarding parent child hierarchical document strategies found anything would help much least directly saw jira lucene-0 nested document query support looks slides overview structurally would indicates query parser support place yet i.e solr query able relate child level queries either within base query clause question finally logical problem seem resolvable approach brute force outlined willing dive solr lucene code would like indication people think would good possible approach get level e.g way providing indexer tuple found combination values something searching facet queries thanks. dumb question time using bit java bit java. hi working upgrading troubles unit test code custom filters wrote tests extend abstractsolrtestcase reading thread test-harness elements present distributables checked branch code built ant generate-maven-artifacts found lucene-test-framework-0-xxx.jar however contain lucene level framework elements none solr solr test framework actually get built embedded solr jars somewhere way build jar contains solr portion test harnesses thanks. hi went apparently normal build install using instructions http//replaced.url get finally trying http//replaced.url get plenty searching looking information couch.ini anything useful mochiweb looked couchdb wiki troubleshooting forth near tell complaint made something somewhere host request header undefined sure know. nice thought trying suggestions inspired jan brad commenting ipv0 hosts /etc/hosts monkeying hostname along way near tell solved problem tests test suite passed sake reference actual build/install complaints debian make ebin directory lib/common_test may manually created something along lines anyhow problem actual location erl_rx_driver.so start playing earnest couch 0.0a000000 well whatever latest version etch let people know coming along. hello already code builds files yeah already document collection change values fields documents indexing possibility think instead modifying documents sending solr could write updateprocessor tha runs direclty solr gets access documents solr already parsed xml even documents someplace else like dih csv file make changes. contains information extracted document-file question part api document file path returns java object gives us way modify inbetween sending object indexing think otis gave answer api instead go external java xml apis completion task sorry description really making things complicated thanks. indy artefact use jdk0 sadly use invokedynamic default gpars wrt groovy/jdk0 situation sadly currently jdk0 version gpars mess jdk0 version works suspect people days work could fix almost hackathon happen groovy grails. 'it's fair wouldn't ready yet earliest release. hi steve indexed stored among field default properties listed specifiable -s. hi guys guys commit permission help commit thanks much. think increase acceptable yes think i'll try although case like groups documents smallish number planning variety different index sizes aiming sweet spot around docs. seems like able reproduce issue try investigate/fix. unfortunately wouldn't find commits. looked parameter. hmmm maybe need define mean server mean client view message context http//replaced.url sent solr user mailing list archive nabble.com. hi sorry spam testing posts actually seen sent queries past couple weeks single response anyways two would respond would appreciate let know ignored vs unseen thanks join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. second query clause instead otherwise mixing apples themes_raw oranges themes. ok figured solr really surprised prototype benchmarks used different instance tomcat using production load tests prototype tomcat instance maxthreads value set using default value production tomcat environment maxthreads value running threads getting connection refused exceptions thrown ramped solr hits past certain level thanks considering yonik others waiting see reply made others said listserv great. guarantee best algorithm isuse final static helper methods set characters strings solr treats special meaning query corresponding escaped versions note actual operators show escape characters wherever occur escapes special characters search terms get confused. hmmm maybe understanding getting jonathan say there good way solr run query across multiple solr indexes shards parameter allows searching across multiple cores instance shards across multiple instances certainly implications like relevance consistent across cores shards works pretty well us thanks. unfortunately wild card search terms get processed analyzers suggestion fairly common make sure lower case wild card search terms issuing query. use solrj dynamically created core capability know advance cores require almost always complete index build previous instance index needs available complete index build two cores index switch required indexing run issummary re early prototype implementation right production quality code tell voluminous methods identify core exists create method instantiates two solrserver objects solr indexcore requires create two cores indexname already exist live core used searching second core used indexing indexing two switched just-indexed core become live core way core works live core always named indexname indexing core always named indexname _index datadir core alternate indexname indexname create core already exists returns true new core created false otherwise solrj provides direct method check core exists getstatus index clearing first complete rebuild various logic submit batches could adjust core index complete reindexes need index always searchable hope helps. good answer may depend wanting restrict 000k documents seeking reduce time spent solr determining doc count wanting prevent people moving far result set case display digits return count solr performing adequately could always artificially restrict result set solr actually return 0m documents returns number specified query well cache next results anticipation subsequent query total count returned exceeds 000k report 000k number results similarly restrict far user page results sounds like sort results descending post date fact get recent ones coming back first. hi bill others post worth specialized resolution wrote similar requirement may help similar requirements running wanted able return facets matched actual search rather facets entire result set example user searches author twain present list facets match twain exclude facets twain found tell users facet values present alpha-sorted list author names count associated documents search author search field identify matching documents get facets i.e normal solr processing point filter facet set include match original search added extra facet parameter facet.sirsidynix.filter.facets instruct solr special facet filtering modified simplefacets method gettermcounts right final return counts like added method filtercounts basically wrapping things run search facet value setting instances based schema inserting facet value running original query anything matches score ones keep filters counts entries match original query using lucene isfast in-memory single document index queries run string value count create run original query anything score means hit value matches original query retain score means hit i.e facet value associated document matched query facet value match query param field field facet values came original search would initial_author_srch_boost well string time shove single-values initial_xxx fields good enough query able correctly bit explanation schema order suffixed facet fields _facet hence first statement matching searchable facet fields names basically differ suffix strip _facet append _boost _fuzzy two field types searching possibly applying boosts fuzzy matching shall see exactly hopefully modify version match schema basically idea derive field original search issued facet field shall read see works rather re-iterating anything date faceting ranges anything facet prefix handling may may work need prefixes anything else facets handle least test say special case us way intended general solution fit prime time submission solr enhancement""",1554,"57,56","34,94","54,63","12,16","2,25","0,71","0,39","0,32","0,00","0,00","0,00","1,54","0,00","9,59","2,38","1,80","5,60","1,54","3,02","0,90","0,51","0,26","3,60","1,29","0,06","4,95","0,26","0,00","1,09","7,59","5,73","1,80","0,06","0,26","0,19","20,33","5,98","4,18","2,70","3,35","1,67","1,99","0,39","0,90","1,35","0,84","0,26","0,19","0,51","0,00","0,51","0,00","0,00","10,62","2,57","2,25","4,70","3,28","3,60","1,16","0,19","1,03","0,13","0,00","0,45","0,51","0,64","6,69","3,35","0,00","0,00","0,00","0,00","0,00","1,16","0,13","0,32","0,00","1,74"
"""3d7a0f2243e5c6521e64c397be8cb6dc90793503a65a8e13b1c9ba6abcfa57a9""","""trick learned marketing look say product reverse adjectives ask would competition say things product answer drop terms endorsement product definition vacuous empty stick facts whit cleaner clear fluff might include metrics project health releases committers pmc additions etc info board would really find useful fluff-free shall weigh sure. aapche oodt use soley test ensure imap handling works. oozie-0 oozie cleanup failed actions fills namespace quota oozie-0 applied launcher job oozie-0 fork/join workflow fails oozie.action.yarn.tag must null oozie-0 oozie restart required oozie metrics graphing tool broken oozie-0 spark-opts value workflow.xml parsed properly oozie-0 minioozie work outside oozie oozie-0 introducing new counter instrumentation log distinguish reasons launcher failure oozie-0 oozie mask passwords logs logging command arguments oozie-0 ssh action succeed exists command fail oozie-0 rerun failed/killed/timedout coordinator actions rather specifying action numbers oozie-0 specifying coordinator datasets logical ways oozie-0 queuedump command display queue information server oozie-0 loginfo uses action instead oozie-0 workflow get failed state kill control node resolve variable message oozie-0 instrumentation configuration rest api web ui include oozie servers oozie-0 cache list available timezones admin oozie-0 workflowgenerator package tar.gz file though seems creating intention oozie-0 docs explicit multiple sub-workflow definitions possible may edit subscription. make possible easy use content delivery network. didn't change i'm afraid forgot mention sorry ndoc finally reached point wouldn't work codebase anymore nullreferenceexception 've created docs sandcastle wouldn't got option creating static html files far tell please help us fix uses official approach still works. hi matthew i'm glad getting ids deleting separate query need it's dozens thousands ids delete it's strategy delete. something open-source like http//replaced.url instead. shown query ment please excuse tested lot little bit confused right query course select/ q=titleprocessed life start=0 rows=0 indent=on view message context http//replaced.url sent solr user mailing list archive nabble.com. cool. hi nicolas wouldn't intentional course let see based traces. thanks guys would looking around tb total index million documents period days purge indexes.i estimated slightly higher side things it's feel would thanks. hello hope running jboss run solr simpler containers e.g. jetty oom things look better replicate less often e.g every minutes instead every seconds all/some -x__ jvm params actually help. working write tests feature get done soon send review thanks""",406,"31,23","33,00","52,46","12,56","2,71","0,99","0,74","0,25","0,00","0,00","0,00","1,72","0,00","10,10","4,68","1,23","5,91","2,71","2,71","0,74","0,25","0,99","2,96","5,67","0,00","4,93","0,00","0,00","0,25","8,37","4,68","3,69","0,49","0,99","1,72","18,23","3,45","4,93","2,71","3,20","2,46","0,99","1,23","0,74","2,22","1,23","0,49","0,49","0,99","0,00","0,74","0,00","0,25","10,34","3,94","2,96","2,96","6,90","5,91","0,49","0,49","0,49","0,00","0,49","0,25","0,00","0,25","18,23","5,91","0,00","0,00","0,00","0,00","0,00","5,67","0,49","2,46","0,00","3,69"
"""4b889711d92465ada1f34e529edb774180412496e385322b9b33f40ec907edbd""","""hi potential likely extension couchdb contract working need win00 work done would rather even time similar couchdbx rather specialized weeks effort based experience similar thing osx preliminary enquiry see anyone suitably capable available interested order get client approval would need completed march rfn would fine currently deploy couchdb platform-specific ruby gem binary libraries required erlang icu spidermonkey etc binaries nginx packaged individual gems requirements managed using gem dependencies package ruby knows extend command line environment include binary resources path/env vars/libpath etc given command line environment starting couch case couch ruby allows configure multiple instances run/manage ruby binaries package location-independent done munging rakefile builds gem means package application isolated machine environment relative given baseline done osx cool although flaws always terminating process concerned getting running system deployed windows need done windows nginx could either must configurable managed via ruby wrapper must native code compilation required gem install e.g cygwin required would conflict existing cygwin installation use private gem repository reason needs work windows xp vista corresponding server versions simple osx wrapper application provides server coupled merb-based app present purely html interface similar osx finder itunes part contract manages updating gems compaction using xmpp talk presence server etc none contract need gui portion installer done windows xp vista although must require additional downloads e.g .net must use shared ruby install gui manages/starts/stops/log admin app points embedded safari mozilla automatically selected port nothing else fails start app must raise explanatory dialog obviously functions handled admin app must fully this-is-just-a-windows-app e.g box help link pretty trivial clear tradeoffs embedding gecko windows app rather ie concerned tracking ie version different client installs maybe issue part really nothing couch would person parts single contract copyright would vest linkuistics company bsd license credit author linkuistics including admin although gui wrapper may required hence finished stage gauging interest/possibility open suggestions/ quotes/offers/enquiries done non-local subbing would need way verifying competence presuming list reaches couchdb-aware talent. netflix switched site search solr two weeks ago it's great walter could persuade add notes. ok whole dbq thing baffles heck may totally base would committing help least worth ths wouldn't dbq specifically said olddeletes map used dbi problem acording heap dumps looked suspect correct root cause ooms perhaps wouldn't using hard/soft commits effectively enough uncommitted data it's causing oom hard say w/o details confirmation exactly looking claim heap dump thanks replying using solr version even saw bounded 0k count looking heap dump amazed keep 0k entries yes see around 0m entries according heap dump around 00g memory occupied bytesref exactly looking say see 0m entries sure wouldn't confusing keys olddeletes instances bytesref jvm. in-reply-to starting new discussion mailing list please reply existing message instead start fresh email even change subject line email mail headers still track thread replied question thread gets less particularly difficult see http//replaced.url. hi francesco mostly covered difference additon virtual machine cloudservice would support availability sets therefore autoscaling article0 brief azure thinking cloud services http//replaced.url. thanks jack it's wouldn't multiple entities invocation two simultaneous invocations dih different entities thanks http//replaced.url. discussion irc regarding collecting data items performance regression start draw conclusions intention understand needs release i'll reply inline issues release run i'make check make distcheck assume see hang others yet everybody wouldn't comment bigcouch different interesting 0.x wouldn't hang i'make check 0.x r00b hangs sometimes different places i'm currently trying gather information question whether i'make check passing r00b release requirement vote considered happy go community decision emerges addition wouldn't question investigate happens address issue hence couchdb-0 insight would appreciated well assume mean tests wouldn't supposed work 0.x i'm happy backport changes master 0.x make work refrained wouldn't bring much change release branch i'm happy reconsider wouldn't think release vote good place discuss feature backports explaining away think warranted chrome broken attachment_ranges wouldn't know reported upstream robert wouldn't release blocker replicator_db test try running understand best situation hence move cli test suite master get test pass least wouldn't problem holds 0.x make absolutely clear report performance regression seriously i'm rather annoyed information ends dev understand irc it's shared understanding scenarios performance regressions shown asked three times posted mailing list i'm asking comprehensive report anything really found robert newson it's simple test irc ran test suspicion posted earlier mail tiny docs slower bigger docs faster nobody else bothered post see discussion observed expected would acceptable release far list concerned know people claimed things slower it's real hold release i'm happy hold release figured things asked help figuring need something work understand voluntary project people wouldn't infinite time spend least message you're collecting things report done would great start far hold horses might something going please let know request unreasonable whether overreacting sorry rant anyone looking performance regression please send list info comprehensive analysis awesome ran machine send us let it's collect data get situation solved need help it's three issues hand robert i'd release artefact understand needs happen make release includes assessing issues raises squaring release vote it's vague far dev concerned report performance regression need get behind it's non-dev discussion performance regression referenced influence dev decision need discussion it's information dev proceed make absolutely clear performance regression issue grateful people including robert newson robert dionne jason smith look it's need treat issue get info onto dev jria. record nightly builds test building automatically. hi sorry flood created filter jira shows blocking issues subscribe whatever means hope it's track matching issues using link. bob it's latest commits fixed replication issue i'd love hear things mentioned best. field solr.sortabledoublefield i'm actually migrating project solr process trying update schema solrconfig stages updating field thanks yonik. oh yes workarround sure needs added. see changes went wrong run stacktrace option get stack trace run info debug option get log output. see changes jfarrell thrift-0 thrift :framedtransport sometimes calls close undefined value fixes thrift :framedtransport module sometimes ends calling close method undefined value inside close. question sentence data model things stored retrieved came across o'reilly book data model chapter cassandra index subcolumns load super column memory columns loaded well mean exhaustive list column names values super column map keys contain two columns max wouldn't really performance concern correct becomes issue lots subcolumns i'm reading correctly i'm looking using super column good way cluster data say storing home addresses might use super column cared mostly accessing data logical area instance thanks help. could use vdu function fixes structure target db replicate old new db""",1103,"68,94","38,26","61,65","15,41","4,35","1,54","1,36","0,09","0,09","0,00","0,00","2,81","0,09","16,23","7,07","2,18","9,97","3,45","2,45","0,82","0,45","1,72","2,63","1,18","0,09","6,07","0,00","0,00","0,45","6,80","4,71","1,72","0,18","0,54","0,18","23,03","5,62","3,72","5,26","2,90","2,54","1,72","1,45","1,18","3,17","2,18","0,45","0,45","0,36","0,09","0,18","0,09","0,00","9,97","2,63","2,54","4,17","3,81","3,81","0,54","1,45","1,18","0,00","0,00","0,73","0,18","0,00","10,24","2,54","0,00","0,18","0,00","0,00","0,00","1,36","0,18","4,17","0,00","1,81"
"""5d25b0725f67b2ca1a4ec8d521523ff5cf9d08ae453584bfbd6acd705fb1a197""","""hi hi furkan ahmet thanks reply last email solr search proposal sent last sunday 0-mar-0 announce solr search simple html interface searching documents indexed apache solr tm actually developed last two months spare time small html interface solr far complete mature project features options might found useful users solr glad share code hosted http//replaced.url page quick overview link solr search found well link downloading thanks best regards ahmed replaced email.addr.es. hi ahmed egypt spent last two months developing custom web-interface solr using html called solr search actually basic idea inspired ajax solr solr search provides different approach terms usability available options users know might interesting information already better search interfaces send email hope sharing solr search community know exact steps share kindly please guide appropriate please find attached quick overview document solr search. github user benkeen commented diff pull request. github user rhtyd commented issue abhinandanprateek still wip let know help please rebase/fix conflicts. nitin-maharana please rebase currently merge conflicts thanks. github user blueorangutan commented issue karuturi trillian-jenkins test job centos0 mgmt vmware-00u0 kicked run smoke tests. github user blueorangutan commented issue. currently ui allow migration different versions changing ui change elegant purpose change user intentionally migration lower higher via api ui way around hypervisor allow point looks good ordering could defined. zhongneu syntax dynamic checkbox example text syntax value dynamic drop menu drop_down_label=default value|value 0|value |value. github user astroshim commented issue zjffdu you're right usually users wouldn't make production using docker received many question running zeppelin production environments think pr gives hints users solve problems making production environments. github user tbouron commented issue thanks aledsage indeed it's great test confirm said i.e wouldn't need mark configkey annotation able use top level config. github user rhtyd commented diff pull request. github user markncooper commented issue yeah i'll abort change it's implemented others. pr user need invoke print make output displayed notebook behavior natural consistent notebooks pr make pyspark interpreter zeppelin behave notebook main changes use single mode compile last statement evaluation result last statement printed stdout consistent notebooks like jupyter make sparkoutputstream extends logoutputstream see output inner process python/r helpful diagnosing pr tested tested manually following text pyspark paragraph get following output breaking changes older versions user wouldn't need call print explicitly needs documentation yes merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. github user astroshim commented issue tinkoff-dwh sorry problem. may concern breaking current ux change change many benefits comparing current embedded spark wrote pr description tae-jun mentioned comment http//replaced.url well thanks always kind big change brings downside well e.g breaking current ux wan write address major cases think would better share opinion get feedback merging quite easy cover already handled updating related docs pages definitely case harder handle user already expectation local mode works surely wouldn't read docs resolve i'll update 'bin/download-spark.sh print sth like wouldn't local-spark/ download embedded spark 'get-spark option user run './bin/zeppelin-daemon.sh start sentences removed future zeppelin users getting accustomed 'get-spark option hard handle user might assume spark works would suggest start applying change first step since zeppelin-provided official docker since bzz raised concern issue let answer make sure reason removed '-ppyspark '.travis 'pyspark profile existed it'spark-dependencies/pom.xml 'pyspark profile wouldn't anymore pr merged actually pyspark testcase astroshim added recently conflict change solved simply adding 'export spark_ver-bin-hadoop hadoop_ver '.travis.yml travis run running issues especially concerning removing it'spark-dependencies related build profiles. pull request adds support annotation disable emit code operator printto allowing custom streaming output format provided consuming application merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. thanks granthenke wouldn't heard anything madrob i'll look merging. sync-processes 'get-valid-new-worker-ids checks topology files launching workers it's issue corrupts topology code topology code sync-processes wouldn't launch worker next runs sync-supervisor try download topology code. could close pushing dummy commit yes agree sending pr weird communication form. isolated network network vm already created setupclass adding verification steps separate test case could run dvs setup port group verification code already tested dvs setup tested remaining steps advanced kvm setup ensure issues merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. github user koushik-das commented diff pull request. github user madrob commented diff pull request. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. github user bzz commented issue looks great thank 0ambda valuable build improvements merging master discussion. github user syed commented diff pull request. github user arunmahadevan commented diff pull request. github user jfarrell commented issue tylertreat-wf pushed tag http//replaced.url mirror picks github. streamoperatortask need final merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. barthel think need rebase feature branch mng-0 upstream/master something like assuming git clone upstream configured point apache/maven origin configured point barthel/maven. github user felixcheung commented pull request looks good merging comments. github user joshelser commented diff pull request. github user swill commented issue shared 0/master testing coordination thread simon weller team run latest master hardware labs testing master preparation rc still getting 'addhost issue periodically showed lot bigger issues concerned production honest long running production client. thank contributing apache opennlp order streamline review contribution ask ensure following steps taken changes x jira ticket associated pr referenced x pr start opennlp-xxxx xxxx jira number trying resolve pay particular attention hyphen character x pr rebased latest commit within target branch typically master x ensured full suite tests executed via mvn clean install root opennlp folder written updated unit tests verify changes adding new dependencies code dependencies licensed way compatible inclusion asf http//replaced.url applicable updated license file including main license file opennlp folder applicable updated notice file including main notice file found opennlp folder x ensured format looks appropriate output rendered please ensure pr submitted check travis-ci build issues submit update pr soon possible merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. github user vesense commented diff pull request. github user roshannaik commented diff pull request. rdowner aledsage review merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. followup ticket kafka-0 improve windowsize calculation quotas 've made following changes calling rate.windowsize clientquotamanager return exact window use computing delay time changed window calculation subtly current calculation bug wherein used number elapsed seconds lastwindowseconds recent sample object however lastwindowseconds time sample created causes issue implies current window elapsed time always sample created incorrect demonstrated testcase added metricstest 've fixed calculation count elapsed time oldest sample set since gives us accurate value exact amount time elapsed merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. github user commented issue confirm download mirror synced merge discussion. fixes issue assembly plugin created source package without 'flume-shared project maven compile failed merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. 'it's good undesrtand details yes install vs verify notice it's good thing tm check releases vote verify instead install local repository wouldn't get polluted local build release later official build central change jankins configuration verify instead install regards. thanks let put way tutorial going cost 0k. talking friend attended javaone year apparently major keynote featured interviews executives behind nhl.com anyone see apparently sun claiming java made site success however nhl.com runs tapestry came keynote""",1358,"31,58","36,52","53,02","6,92","1,40","0,44","0,29","0,07","0,07","0,00","0,00","0,96","0,00","8,25","2,21","1,40","5,08","1,33","1,62","0,59","0,15","0,74","2,14","0,74","0,00","7,81","0,00","0,07","0,00","5,74","5,01","0,66","0,00","0,00","0,22","16,35","2,58","5,60","1,77","1,62","2,43","0,29","1,10","0,29","1,25","0,66","0,15","0,15","0,37","0,00","0,29","0,07","0,07","16,42","8,32","2,58","5,01","3,90","3,53","1,33","0,52","0,88","0,07","0,00","0,37","0,22","0,15","12,30","4,42","0,00","0,00","0,00","0,00","0,00","2,43","0,15","2,65","0,00","2,65"
"""574ca75cae0d6cee843ca843d83fba69638211430d175ff62ef30fd1c06dfc89""","""files uploaded users deleted save space server mobile sorry typos. debian. hello maxim obvious red0 maybe right tested upload jpg file avatar conference room upload .ppt office file extensions. hello alvaro works flawlessly server starts image/office doc upload nothing common. ok. time ago something similar testing another question upload files properly use refer upload office files know debian could please url server. have visited right url works right uploading files files uploaded users drop whiteboard. hello hermann using different start/stop http//replaced.url necessary tags. verifyerror subclass uses pageactivationcontext extends uses pageactivationcontext. added new getrootexpressionclass method ognlexpressioncompiler interface impl handle special corner case casting previously component classes. page moved typecoercer service. -curious see real tapestry application live wish fulfilled booking demo available 's based well-known seam demo updated make use tapestry including fancy ajax updates code available github curious see real tapestry application live wish fulfilled booking demo available.it 's based well-known seam demo updated make use tapestry including fancy ajax updates code available github -project website basics homepage project.apache.org project naming descriptions use proper apache forms describe product etc website navigation links links included link http//replaced.url included license security links missingtrademark attributions attribution asf marks included footers etc logos graphics include tm use consistent product site tm missingproject metadata doap file checked date 's still referencing tapestry need update -we 've series beta releases tapestry voted due problem voted release december 00th first release almost months -a live demo tapestry available http//replaced.url -we deployed new improved web site authored confluence exported static web site includes brand new improved addition 've much community work non-committers access confluence organizing rewriting documentation tapestry component-oriented web framework java.branding requirements implementation progress project website basics homepage project.apache.org project naming descriptions use proper apache forms describe product etc website navigation links links included link http//replaced.url included license security links missingtrademark attributions attribution asf marks included footers etc logos graphics include tm use consistent product site tm missingproject metadata doap file checked date 's still referencing tapestry need update that.we 've series beta releases tapestry voted due problem voted release december 00th first release almost months.a live demo tapestry available http//replaced.url deployed new improved web site authored confluence exported static web site includes brand new improved addition 've much community work non-committers access confluence organizing rewriting documentation binary files diff available modified websites/production/tapestry/content/getting-started.html getting started tapestry easy lots ways begin watch video browse source code working demo app create skeleton app using maven step tutorial.watch short videofor fast-paced introduction watch mark w. shead 's minute demo video shows set simple tapestry application complete form validation hibernate-based persistence ajax video provides preview development speed productivity experienced tapestry users enjoy.play working demo appyou play tapestry via live demonstration applications start look booking demo source code provided download play it.create first tapestry projectthe easiest way start new app use apache maven create initial project maven use archetype kind project template create bare-bones tapestry application you.once maven installed execute following command getting started tapestry easy lots ways begin watch video browse source code working demo app create skeleton app using maven step tutorial.watch short videofor fast-paced introduction watch mark w. shead 's minute demo video shows set simple tapestry application complete form validation hibernate-based persistence ajax video provides preview development speed productivity experienced tapestry users enjoy.play working demo appyou play tapestry via live demonstration applications start look booking demo source code provided download play it.create first tapestry projectthe easiest way start new app use apache maven create initial project maven use archetype kind project template create bare-bones tapestry application you.once maven installed execute following command maven prompt archetype create tapestry quickstart project exact version number e.g. asks group artifact version number use staging uri get archetype not-yet-released version tapestry.you see following transcript modified websites/production/tapestry/content/the-tapestry-jail.html -ccordenier uli deploy manage webapps ccordenier thiagohp uli log restart tomcat -only use 've got jail tapestry.zones.apache.org running tomcat deploy demo applications booking demo running there.the jail replaced vm tapestry-vm.apache.org booking app moved document needs updated reflect new server ccordenier uli deploy manage webapps ccordenier thiagohp uli log restart tomcat.restarting tomcatonly use -if need restart tomcat anything else fail -if jail lost java tomcat +if need restart tomcat anything else fail.reinstalling jailif jail lost java tomcat -if n't exist yet +if n't exist yet -some files need manual download placed /usr/ports/distfiles follow instructions screen -some files already live http//replaced.url downloaded time zone update utility n't downloaded oracle directly -afterwards +some files need manual download placed /usr/ports/distfiles follow instructions screen.some files already live http//replaced.url downloaded time zone update utility n't downloaded oracle directly.afterwards -follow instructions manually install unresolved dependencies might encounter -check env packagesite point tb.apache.org +follow instructions manually install unresolved dependencies might encounter.install tomcat pre-built package preferred check env packagesite point tb.apache.org -this really discouraged +from ports discouraged really discouraged -tomcat resides /usr/local/apache-tomcat-0/ may wish set users conf/tomcat-users.xml note tomcat onwards manager role split separate roles manager gui manager-gui default tomcat listens port listen though edit conf/server.xml change port -tomcat0_enable= yes +configure resides /usr/local/apache-tomcat-0/ may wish set users conf/tomcat-users.xml note tomcat onwards manager role split separate roles manager gui manager-gui default tomcat listens port listen though edit conf/server.xml change port.add +tomcat0_enable= yes -to /etc/rc.conf start tomcat system startup tell use diablo jvm -the sudoers file may lost entry allowing members tomcat-restart group restart tomcat add back using visudo +to /etc/rc.conf start tomcat system startup tell use diablo jvm.the sudoers file may lost entry allowing members tomcat-restart group restart tomcat add back using visudo. fixes tapestry-0 component n't rendering clientid tag rendering enabled cycle rewinding particular form nothing n't even render body get condition work field necessary tapestry.form.validation.validateform form. touch documentation simplify export project http//replaced.url commit http//replaced.url tree http//replaced.url diff http//replaced.url. removed auto-injection service ids string backward incompatible decorators""",1073,"63,12","37,84","48,37","7,92","1,30","0,56","0,00","0,28","0,28","0,00","0,00","0,75","0,56","7,74","0,75","0,93","6,15","0,56","1,68","0,28","0,65","0,28","2,80","0,84","0,00","3,08","0,00","0,00","0,19","4,66","3,36","1,30","0,00","0,00","0,84","15,19","1,77","5,41","1,86","2,42","1,21","0,84","1,86","0,84","2,05","1,40","0,37","0,09","0,65","0,47","0,19","0,00","0,00","12,49","2,70","2,14","6,80","7,64","4,75","2,52","0,09","0,19","0,19","0,00","0,28","0,19","0,00","19,57","6,99","0,00","0,00","0,00","0,00","0,00","4,66","0,19","1,58","0,00","6,15"