Source (A),Source (B),WC,WPS,Sixltr,Dic,funct,pronoun,ppron,i,we,you,shehe,they,ipron,article,verb,auxverb,past,present,future,adverb,preps,conj,negate,quant,number,swear,social,family,friend,humans,affect,posemo,negemo,anx,anger,sad,cogmech,insight,cause,discrep,tentat,certain,inhib,incl,excl,percept,see,hear,feel,bio,body,health,sexual,ingest,relativ,motion,space,time,work,achieve,leisure,home,money,relig,death,assent,nonfl,filler,AllPunc,Period,Comma,Colon,SemiC,QMark,Exclam,Dash,Quote,Apostro,Parenth,OtherP
"""861fd45532e720b19456ed7a5ae138ed94d62927fa268077f96c42b8f0c4d5b2""","""offer couchwiki project named gorsvet.urlyss tiny url-shortener service written flask using dare may become dares""",17,"17,00","35,29","58,82","11,76","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","23,53","11,76","5,88","11,76","5,88","0,00","0,00","0,00","0,00","0,00","0,00","0,00","5,88","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","17,65","5,88","5,88","0,00","5,88","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,76","0,00","11,76","0,00","11,76","0,00","0,00","5,88","0,00","0,00","0,00","0,00","0,00","0,00","23,53","5,88","0,00","0,00","0,00","0,00","0,00","5,88","11,76","0,00","0,00","0,00"
"""1cc23fe0afdfd7abeb62bba9ce9deacde4fc680cf034895076d3c0eae42c8320""","""sorry delay getting back investigating angles using tableau drill create views drill access parquet files even though parquet files hold schema create views cast columns correct data bigint float timestamp etc updates suggested parquet-avro add support timestamp urgent originally thought since cast everything anyway. anyone please help following sure done something wrong downloaded zip http //replaced.url warning problems encountered building effective model org.apache.parquet parquet-scala_0.00 jar:0.0-snapshot warning problems encountered building effective model org.apache.parquet parquet-scrooge_0.00 jar:0.0-snapshot warning highly recommended fix problems threaten stability build warning reason future maven versions might longer support building malformed projects error see full stack trace errors re-run maven -e switch error information errors possible solutions please read following articles error correcting problems resume build command. attaching patch put together decimal includes added fixed bytes try make generic based branch avro logical support avro classes missing would need copy parquet fix schema getlogicaltype accessor need use instead think pretty straight-forward questions let know. thanks sounds need go trial local copies code would good check delayed contact http //replaced.url discuss. ok tried http //replaced.url seems build fine. written additional parquet-tools helper access files hadoop environment along lines existing parquet-schema etc scripts provided part parquet-tools distribution designed live location scripts hopefully others find useful determine path 's directory else""",230,"38,33","40,43","51,30","10,00","1,74","0,00","0,00","0,00","0,00","0,00","0,00","1,74","0,00","10,43","2,17","1,74","6,96","1,30","2,61","0,87","0,43","0,00","3,04","1,74","0,00","3,48","0,00","0,00","0,00","10,43","6,96","3,48","0,00","0,43","0,43","20,00","5,65","4,78","4,35","3,48","1,74","1,30","1,74","0,43","2,17","1,74","0,43","0,00","0,00","0,00","0,00","0,00","0,00","10,87","3,91","2,61","3,91","3,04","3,04","0,43","0,00","0,87","0,00","0,00","0,43","0,00","0,00","15,65","5,22","0,00","0,87","0,00","0,00","0,00","4,78","0,87","0,43","0,00","3,48"
"""2558c44d40a4e4abe2b563f51229232dc151c6ff22a82b24fb53a6c28a9fcb31""","""hi think line dependencyinformationreport.java needs fixed replacing could someone update please let know need additional information kind regards""",19,"19,00","31,58","73,68","15,79","5,26","0,00","0,00","0,00","0,00","0,00","0,00","5,26","0,00","31,58","10,53","0,00","21,05","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","10,53","0,00","0,00","0,00","10,53","10,53","0,00","0,00","0,00","0,00","42,11","15,79","0,00","15,79","5,26","0,00","0,00","5,26","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","10,53","5,26","0,00","5,26","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","15,79","5,26","0,00","0,00","0,00","0,00","0,00","0,00","10,53","0,00","0,00","0,00"
"""637258698cb8be15b9d856019f6325312df39b6028e1a87e846b26d2dd4dfbc8""","""hi list requirement producers wait indefinitely kafka broker comes case maintenance network interruptions disk caching front producer producer waits drains producer eventually reconnects functionality working fine kafka producer settings used acks=all would like acknowledge gwen shapiras slideshare http //replaced.url setting derived""",43,"43,00","53,49","53,49","6,98","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,63","2,33","2,33","6,98","2,33","2,33","0,00","0,00","0,00","2,33","0,00","0,00","4,65","0,00","0,00","0,00","6,98","4,65","2,33","0,00","2,33","0,00","30,23","2,33","13,95","2,33","2,33","2,33","6,98","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","9,30","6,98","0,00","4,65","18,60","16,28","0,00","0,00","2,33","0,00","0,00","0,00","0,00","0,00","13,95","2,33","0,00","0,00","0,00","0,00","0,00","0,00","4,65","0,00","0,00","6,98"
"""61631242ce3a1b35bb227cfd729b9570d47c8ee74a208766faa6fcc599faaeab""","""hi regexreplaceprocessorfactory line means use match groups replacement string reasoning behind missing something groups used making hard write simple solution training exercise students need clean incorrectly formatted dates thanks. right looking snapinstaller seemed though deletion index directory would deleting files indexreader thanks. tested impact certainly would cause index files read thus cached depending much ram available came big commercial user lucene think running something like fc0. hi got fields contain embedded xml two questions relating appears though shall need xml-escape field data otherwise solr complains find start tag embedded tags finds tag field expected constraint xml-escaping data best way handle kind related question would easiest way ignore xml tag data indexing types xml-containing fields seems like could define new field e.g set associated tokenizer something new create though would un-escape data ick parsing skip tags thanks. hi christian encoding using pushing documents solr specified xml post request separate issue mime-type use post using latest scripts solr characters look like xml pushing example encoded two surrogate characters instead code point extension b set xml parsers handle correctly source similar issues seen potential issue xml parser used updated handle extended unicode code points older parsers still failed handle example. documentation http //replaced.url specify names/positions fields csv file ignore fieldnames seems like would solve requirement different layout could specify mapping import could handy provide map versus value map updatecsv supports could use header provide mapping header fieldnames schema fieldnames. large index would recommend separate solr installation use update/commit changes use snappuller equivalent swap live search unlikely switch lucene tune new parameters control memory usage updating. sort answering question seems like need get current core use instantiate new solrcore exact config documentation solrcore 's constructor says core already exists stopped replaced unclear whether graceful swap like hard shutdown old core thanks hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor 's datadir gets used construction changing coredescriptor 's datadir effect since would go changing datadir core multi- thanks. hi emmanuel spot room think would issues would useful able leverage solr 's. hi uses solr generate terms mailing list text analysis extract good features things like classification similarity clustering last part cover using solr implement real-time similarity engine maybe recommendation engine well undoubtedly things unclear even incorrect please comment regards. hi robert confuses suggester says based spellchecker supposedly work shards issue got configuration shards already trying leverage auto-complete quick dirty work-around would add custom response handler wraps suggester returns results fields needs merge. grabbed latest greatest trunk make main.css .query-box height tall enough least mac 0/ff config character descenders get bumped fixed issue solr looks like contains anchor text missing constraints see .constraints css added main.css ianawd probably best way fix issue see css used intent set character gray seems silly open jira issues types things add noise list approach preferred thanks. url work please include full stack trace runtimeexception version tika using thanks. hi yonik ah explains little seen resin parameter used inside init method typically specified using tag know resin-specific yes would easier update resin runs unpack .war kind pain way knew get working versus jindi approach got special reason ask ongoing support init-param technique. turn facets query facet=true facet.field= shall get back distinct values though might play settings e.g facet.limit=0 get results need. hi upgrading solr noticed filtercache hit ratio dropped significantly previously enabled recording entries debugging looking seems edismax faceting creating entries sharded setup distributed search search string bogus text using edismax two fields get entry shard 's filter caches looks like expected similar situation happening faceted search even though fields single-value/untokenized strings using enum facet method shall get many many entries filtercache facet values look like item_ net result even big filtercache 0k hit ratio still thanks insights. providing maven0 build would useful us currently jar in-house maven repo works clean would favor changing directory layout selfish reasons would favor changing build rely maven0 use maven internally krugle sometimes works well times royal pain butt option would nice handy personally would hate foist maven everybody else. fwiw krugle started using moinmoin switched confluence general good change us especially scale scope wiki activity increased integration jira handy far different classes pages easiest way would create spaces assume possible w/the confluence config apache would set space different access rights based groups users quickly hacked app used convert moinmoin pages markup syntax confluence switch confluence happens look around. hi got pattern document call xy turn two tokens xy approach could use patterntokenizer extract xy custom filter returns xy next call caches next result could extend patterntokenizer return multiple tokens match though figuring specify schema seems another approach would require custom code thanks. hi using simpletextcodec past noticed something odd running solr enable posting format via something like resulting index expected text output noticed files standard binary format e.g .fdt field data based simpletextcodec.java assuming would get simpletextstoredfieldsformat stored data holds true files e.g http //replaced.url adding simple text format docvalues walk code figure hoping need change configuration setting thanks. using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks. csvloader user found/fixed bug involved active development/maintenance piece code james make progress merging support csv dih great. intermediate approach use generate separate hashes quantized bands find potentially similar files using apply accurate slower comparison determine true similarity data common get files due small text changes frequency term moves quantized bands changes uber hash get combining terms bands still get matches hashes individual number matching fingerprint values. hi list anybody know suggester component designed work shards asking documentation implies since suggester reuses much spellcheckcomponent infrastructure spellcheckcomponent documented supporting distributed setup make request get exception looking querycomponent.java:0 code see assuming docs variable null would happen response element solr response make direct request request handler core e.g http //hostname:0/solr/core0/select qt=suggest-core q=rad query works see element named response unlike regular query wondering configuration borked work fact suggester return response field means work shards thanks. hi uses character separate elements e.g state|city solr 0.x worked fine query gets distributed across shards get solrexception severe org.apache.solr.common.solrexception use fieldcache field neither indexed doc values state problem appears fl= state|city parameter getting split functionqparser tries use state field actually exists ignored field since q=state|city ca| find entries california known issue way disable parsing field names field list thanks. general yes subset terms occur frequently remove terms easy longer search either part query phrase combine common terms following terms works well bit complex significantly grow index either requires data analysis generate target set common terms. moving solr-dev solr-user quick impression given scope described page feels like boil ocean problem spent afternoon looking could use solr code getting much love somehow leveraging solr would seem like win three attributes solr interesting context caching though critical things live noticed described issues automatic doc- server mapping calc stable hash quick exploration use hadoop rpc talk guts solr assumes query processing happens search server level versus master currently nutch way request summaries document via subsequent post-merge call master immediate problem ran notion solr running inside container currently penetrates deep bowels code even core level calls made extract query parameters url step going try clean manner would define side/solr core api layer would relatively easy least first cut hooking solr core nutch master via hadoop prc. thing bit previously using apis area solr call corecontainer.getcore increments open count balance call close call naming could better think common expectation calls get something change state maybe. two cases think cases two characters variant forms unicode tried unify still exist gb tons wanted support phonetic pinyin zhuyin search might collapse syllables commonly confused course would storing phonetic forms words. hi sandhya would post question replaced email.addr.es mailing list include details document confidence level often low enough tika assume good match thus report language. answering question use corecontainer.reload core force assume got embeddedsolrserver running time everything happen correctly covers need find programmatically change settings using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks. shamed taking position vote earlier real experience slf0j indirectly via use jetty know would _something_ logging hood use log0j everywhere finishing earlier work creating jul logger bridged log0j another option would rather avoid. hi trying morelikethis support getting odd results realized unless fields used similarity lucene re-analyze field using standardanalyzer case quite different using solr schema first note anybody using morelikethis make sure specify termvectors=true solr schema fields passed query mlt.fl parameters second note wiki page example schema might include reference termvectors field attribute example sample schema says made think initially attributes http //replaced.url make mention termvectors termpositions would edit page currently section talks attributes common ones thanks. hi tommaso slaves configured use vip talk master easy dynamically change master use via updates. hi geert-jan thanks ref good stuff think close understand correctly could get using top two versus top simplicity results looked like actual faceted field value/hit counts would top hit facet field followed facet field used field collapsing would improve probability asked top hits would find entries top faceted thanks. idea thought given document/field set would contain text single language write special token language e.g analyzer add my-special-token-prefix-esperanto token field query time assuming know language make required term. different schemas would combine results two schemas define third core different conf separate conf/solrschema.xml set request handler dispatches two real cores. available cores dynamic cores requirement custom code wasted. believe mistake recent email thread list. hi arno need add boilerpipecontenthandler tika 's content pretty sure means would need modify solr e.g trunk tikaentityprocessor.gethtmlhandler method would try something like return new boilerpipecontenthandler new contenthandlerdecorator though quick look code curious use. could background gc depending got jvm configured use though stay long. pros/cons using cache-control no-cache response header avoid problem tracked entire thread proposal would use response codes along xml body would client deal container response would typically html versus actual thanks. noted lot different approaches could search email list archive discussion using solr subject solr support integration providing compass-like mechanism keeping solr index sync db via hibernateeventlistener. issue ran daily rolling log files maybe missed find functionality jdk logging package however log0j advocating change noting worked around leveraging resin 's support wrapping logger set daily. right baking fine-grained level security information bad idea example worked pretty well code search krugle project number groups granted access rights file project would inherit list groups user logs get authenticated via ldap set groups belong returned ldap server becomes fairly well-bounded list terms query acl-groups field file/project document set boost portion query. hi creating combo field searching straight-forward way boost contribution fields used create combined field would read past threads seem anything built solr simple hack copy field multiple times e.g schema field-to-boost effective boost 0x since highlighting display multi-value field seems work ok long course level boosting 0x 0x etc acceptable something continue work future issues approach run yet thanks. general notice http //replaced.url uses solr search project data store index user-generated notes code finally went live last week openly post thanks solr community useful tool great. hi quick note caveat using slightly older version solr without fragile sense relies current working directory resin home directory b path must ./webapps/ anyway worth solrsevlet.java 's init method change beginning see instance resin specifying explicit location configuration data note relative original source least version getinitparameter .getinitparameter parameter solr.configdir lower-case build deploy solr resin called solr-notes example 's /webapps/solr-notes/web-inf/web.xml a. uncomment first three system-property lines noted comment work around bug resin b. add init-param section copy contents solr config directory /webapps/solr-notes/conf part edit specify location data directory repeat steps second different. ran minor problem clicked facet tried search would get error think problem fqs velocity macro vm_global_library.vm missing else insert url macro fqs p foreach p velocitycount else without url becomes /solr/browsefq=xxxx instead /solr/ completely new world velocity templating got low confidence right way fix. hi 've got ancient lucene tokenizer code 'm trying avoid forward-porting n't think 's equivalent solr specifically 's applying shingles output something like worddelimiterfilter e.g mysupersink gets split super sink shingled 're using shingle mysuper super supersink sink n't follow wdf single filter shingles n't created across terms coming wdf 's pieces generated wdf actually way make work solr thanks. must missing something 's schema exactly original example schema look index using latest luke would need rebuild luke current lucene code since format version tried using solr/admin/analysis see query returning get server error numerictokenstream support chartermattribute looks like happy analysis triefloatfield searches index using solr/admin results single value e.g ideas might going thanks. started using regular dom parser coding hand switched xstream help handling solr pojo mappings seems work fine get past encoding. included bit information configure top-level sharding request handler use suggest component get npe get response results completeness pieces puzzle suggest-one suggest-one suggest-two thanks. think change token frequency would cause jump quantized levels order change md0 hash obviously happen point quantizing frequencies make much less likely using tech page crawl seems working pretty well though done in-depth analysis seen different proximity hash functions ones referring thanks. code appears try maybe code date see /solr/conf used path code see created cwd/data data default specified work tried number different permutations maybe missed magic combination grepped source see pattern solr.solr.home used anywhere able control location config file using set location /data directory still gets created saw though seem use either see solr.datadir used code anywhere trying control command line seem work could uncomment edit tag file worked makes worried missing something wrong sources tell ways control location data directory used index a. change cwd sure inside resin b. edit tag file lets change directory ways control location config directory lets change directory thanks. hi marcus look new project called katta first code check-in happening weekend would wait monday look. hi shawn different use case ones covered response robert wanted call currently use embedded server building indexes part hadoop workflow results get copied production analytics server daily basis writing multiple embedded servers reduce task gives us maximum performance proven reliable method daily rebuild pre-aggregations need analytics use case regards. hi especially yonik grins trying solr resin following copied solr-nightly.war /webapps/ cloned/edited resin.conf file include mapping solr adding started resin created /webapps/solr-nightly directory expected tried use solr complained finding see stack trace email realized solrconf folder included default .war added inside /webapps/solr-nightly directory result tried moving uncommented lines file yonik indicated necessary get resin work properly solr change anything figured would ask correct way configure solr resin rather continue thrash would help resin jock far usage limited futzing resin config file add simple thanks. well ultimately heading towards single multiple embedded solr cores case could .jsp-based gui/admin functionality peacefully co-exist use embedded cores description roadmap solr gui example assuming files still exist going forward become much gui layer top new/beefed plan eventually get html using json responses request handlers thanks. hi dave think pushing solr space probably going non-starter solr 's focus index management serving side things different multitude issues faced looking good free would try ibm 's uses lucene hood fact wonder could reverse index create solr schema. hi erik missing change needed thanks much. hi ryan dug client code natural inclination would go latter fits better would write test use test request object. hi ken given comments seemed describe using nrt opposite use case set solr use solr.mmapdirectoryfactory bother test whether nrt would better use case mostly sound like advantage focused things relating solr would love hear results someone testing batch indexing use case tested various xxxdirectoryfactory implementations please let know results testing. karsten said providing detail actually trying usually makes better helpful/accurate answers guessing search key value right create multi-value field custom indexed stored indexing add entries custom set analyzer strip index key e.g. never tried sending anything utf-0 solr comment issues shall run based experience date would strongly suggest converting utf-0 post solr. actually tagsoup 's reason existence clean html wild tika 's html parser wraps uses generate stream sax events consumes turns normalized. seemed work thanks suggestion though using case anybody else reads shall need run tests check performance improvements hadoop workflows output always fresh unless interesting helicopter stunts yes default index always rebuilt scratch thus long primary key used reduce-phase key easy ensure uniqueness index thanks. hi michael build meant gui whereby user use special characters say quoting collection clauses programmatically build query without using query parser code wound write seemed like simple escaping quickly got complex convoluted e.g allow term get processed specially query parser ok case sounds like stuck escaping. useful use morelikethis support see http //replaced.url attached patch. hi frederik figure solution problem asking recently ran similar problem similar setup shards server occasionally query long time occasionally see timeout exceptions http requests e.g restarting jetty seems clear problem temporarily looking code solr handles distributed requests got interesting smells would surprised issue related using regards. hi jack thanks included details original question issue got index 000m records 00m unique value prefix characters long adding another indexed field would significant hoping way via grouping/collapsing query time possible thanks. hi otis sorry missed reply thanks trying find similar report wondering file jira issue might get attention. hi assuming use first characters specific field grouping results thing possible out-of-the-box would next best option e.g custom function query thanks. hi checking seems indexed fields specified sorting purposes query string least seeing date field correct miss info wiki someplace case would handy standardrequesthandler return error invalid request sorting non-indexed field requested thanks. describing web mining web crawling extract price data web pages put specific field solr using nutch would need write custom plug-ins know extract price page add custom field crawl results topic nutch mailing list since solr downstream consumer whatever nutch provides. hi lucene index generated solr optimizing able view using limo download mac try use luke fails luke complains /index/_jdi0.f0 file directory files downloaded follows known issue luke solr-generated lucene indexes references found file directory involve index corruption far tell index fine thanks. note shall need ant compile top lucene directory first trying solr-specific builds inside /solr sub-dir least ran trying build solr dist recently. would use copyfield copy contents new field uses string use faceting. use characterencodingutf0 think shall get back stream utf-0 bytes db know mysqlxmlfield 's start array bytes returned jdbc call create string array using utf-0 encoding use bytes directly writing xml best thing make sure xml send solr starts line make sure converted text xml fields wrap fields. hi renee mike right question post users list yes separate setconnectiontimeout used though familiar possibility ping response handler responding connection established getting data back. think need create separate background process least case web crawling challenge efficiently use samza process simultaneously fetch many urls increase complexity process 's code wind manage either multi-threaded async fetch state hadoop-based crawlers limited number parallel reduce tasks fetching see nutch bixo examples e.g fetchbuffer another project involved past. hi looking using embedded solr lets extract ranked results state publish part task 's operation methods defined problematic though specifically range methods return iterators iterating lots results solr feasible newer paging support still abuse architecture wondering whether need support methods called internally tasks e.g task thus optional assuming state automatically restored changelog samza system calling putall list repeatedly dug details would example required method thanks. experience resin definitely explicit character encoding. related item might check jmaki http //replaced.url jsp-based amount jsp-generated content pretty easy add support dynamic. would parse html extract text first use index data nutch tika projects examples using html. believe code support handling form data sent put request logic make sure either unspecified application/x-www-form-urlencoded read body byte array use request convert string typically specified e.g use curl tool post data sent header convert key/value pairs using urldecoder.deccode string utf-0 since key/value pairs url-encoded believe standard assume us-ascii utf-0 would work well us-ascii viewed sub-set utf-0. think ran would like run two copies solr inside resin since two sets searchable data distinct schemas/usage patterns specify location configuration directory least using either solr.configdir solr.solr.home subscribed dev list shall post question timing fix versus something quick needs fwiw quick fix pass directory location init thanks. finally got around looking short field values returned java.lang.short xmlwriter.writeval textresponsewriter.writeval missing check val instanceof short thus bit code used thing happens binary field since val case byte get b b anybody else run seems odd known issue wondering something odd schema especially true since binaryfield write methods xml json via textresponsewriter handle base00-encoding data wondering normally binaryfield.write methods would get used whether actual problem lies elsewhere. hi would started using embedded solr back via patched version in-progress code base wondered paragraph said given current state solrj expected roadmap solr general would guidelines special circumstances warrant use solrj know back namely multiple indexes run multiple webapps handled multi-core generating lots http traffic handled dih maybe solr search system since integrated system hands customers restart container option anything got wedged might still issue commonly compelling reasons use solrj thanks. got situation data directory needs live elsewhere besides inside solr home b moves different location updating indexes setting symlink /data great option best approach making work solrj low- level solution seems create solrcore instance specify data directory use update corecontainer wrong would like avoid mucking around low-level solrcore approaches thanks. interested need sorted output faceted browsing alternative output formats something along lines merge xml responses w/o schema proposal would fine much better would use hadoop prc versus http call sub-searchers assuming better performance might fewer connectivity issues leveraging work done embedded jetty example anybody data points relative performance master schema main search server could get distributed remote searchers would part thanks. works excellent trying build distribution trunk use prototyping noticed things fresh check-out build trunk/solr sub-dir due dependencies lucene classes done top-level ant compile cd /solr ant builds noticed run-example target trunk/solr/build.xml description show ant -p. tried ant create-package trunk/solr got error near see contrib/velocity anywhere lucene trunk tree recommended way build solr distribution trunk meantime shall use example/start.jar solr.solr.home thanks. hi running problem queries distributed among multiple shards return binary field data properly hit single core xml response http request contains expected data hit request handler configured distribute request shards xml contains b b looks like wind getting .tostring data actual data anybody else run done fair amount searching hits yet next step create unit test solr nobody raises hand walk thanks. time text. hi moving towards embedding multiple solr cores versus using multiple solr webapps way simplifying build/deploy getting control startup/update process would hate lose handy gui inspecting schema importantly trying queries explain turned anybody tried dual-mode method operation thoughts whether workable issues would taken quick look supporting java code ideas would needed hoping easy approach whacking admin support code thanks. could use lengthfilterfactory restrict terms least character long believe patternreplacefilterfactory creates patternreplacefilter says. use prng mix ids form auto-incrementing field yes. sounds like hitting max url length 0k common default http web server using run solr web servers know let bump limit via configuration settings. hi got situation key result initial search request let 's say list values faceted top faceted field values need get top hit target request restricted value currently total requests requests following initial query made parallel still questions magic query handle solr as-is best solution create request handler case developing custom thanks. another trick described andrzej another field days write token e.g number times matches age document starting reasonable base date add +days:0 query lucene winds boosting results recent. problems knowing possible set case seems like minor tweak call solranalyzer though understand mark 's comment setanalyzer method says required using like docnum method call tell analyzer either default standardanalyzer whatever gets set explicitly still get used case term vector. hi otis exactly something similar nutch searches using ehcache http //replaced.url store rewritten query string serialized xml response way dependencies stable searcher/doc ids nutch reference get remote searchers depending entries cache hit docs xml representation storing xml might. hi list working improving performance solr scheme cascading supports generating solr index output hadoop job use solrj write index locally via embeddedsolrserver mentions using overwrite=false csv request handler way improving performance see http //replaced.url removed support solrj deemed dangerous mere mortals question whether anyone knows much performance boost really provides hadoop-based workflows straightforward ensure unique key field really unique thus performance gain significant might look figuring way trigger lock re-enabling support solrj thanks. clarify issue using actual user search traffic seo content expose example people commonly search java hint url static content page language part generating static pages based search traffic though might decide content favor see based popularity yes need automate content generation regular e.g weekly basis big challenges ran dealing badly behaved bots would hammer site wound putting content separate system would impact users main system generating regular report user agent ip address could block robots.txt ip necessary figuring structure static content look like spam many links page much depth constrains many pages reasonably expose project scores based code activity usage used rank content focus exposing early low depth good stuff could based popularity search logs anyway lot topic feel solr specific apologies reducing signal-to-noise ratio. looked briefly passing analyzer use morelikethis fields lot analyzer used given filters play performance really stunk use stored term vectors would nice yes thanks. hi otis working project speak stefan friends going live separately something independent solr/nutch view search plumbing usable multiple environments makes sense view replicating core solr nutch functionality sucks sure outcome. hi frederik directly run issue solr experienced similar issues related context case custom made solrj requests generated aggregated/analyzed results load testing ran different issues load test software issue scaling assuming case seen happen e.g limit max parallel connections client used talk solr needed tune solrj settings httpconnectionmanager heavy load running free connections given got shards request going spawn http connections know top head manages connections whether possible tune stack trace sure looks like blocked getting free http connections needed optimize configuration jetty jvm gc etc lots knobs twiddle better worse. might look nutch handles issue nutch stopwords wants keep around generates combo terms like the- index query parser thing query phrase common terms wind searching across much smaller slice comes course expense larger index lot unique terms due combo terms big win example site http //replaced.url index source files without optimization searches could several seconds got 000ms lots breathing room. difference free speech free beer see http //replaced.url. way mean configuring current extractingrequesthandler fundamental issue solr uses tika prevents extractingrequesthandler modified work way seems like useful configuration regards. low qps multi-core servers believe reason multiple shards server provide better parallelism request thus reduce response time. noticed prices showing even though got think issue line hit.vm number.currency function needs get passed something looks like number return could list values square brackets confuse number.currency get price think line needs since returns single value without brackets. hi way explicitly add record using solr add fail record already exists value unique field specified schema 's uniquekey field course pre-flight query see record unique exists multi-client environment solr multiple users reliable thanks. depending complexity solrj could solution see section talks solrj provides apis create queries instead hand coding query http //replaced.url. would great facing issue rolling code keep solr index sync mysql db access via hibernate thanks. others noted currently updating field means deleting inserting entire document depending use field might able create another core/container field plus key field use join support note http //replaced.url improvement looks like 0.x code line though see fix version. area might issues date range queries many docs run oom errors recent thread yonik others good suggestions ways avoid problem know impact would merging results use date ranges guessing low yonik would know best well 0-server configuration would work would 000m docs/server large number even data/doc small 0k real performance going heavily impacted nature data types queries shall need think distribute data avoid performance constrained worst case searchers nutch wound add termination logic avoid long-running queries clog things primarily dealing load best first step create single solr representative data see well performs issues going around limits box 000m docs versus distributed nature though keeping servers alive happy significant ops task finally would decide early whether search query words ok result set happens missing doc server timed looking query-type solution solr would less interesting. hi looking snapinstaller seems following copy new index directory master slave 's solr data directory giving index.tmp rename temp index directory index commit send post /solr/update service new index gets use feel like must missing something seems like request middle processed step successful swap could fail due index changing underneath insights thanks. sounds like roysolr running embedded jetty launching solr using start.jar app container newrelic installed. hi ian public terabyte dataset project would good match need course means actually finish crawl finalize avro format use data free collections data around though none know target top-ranked pages. hi sorry noise finally realized running using java code enwikicontentsource lucene benchmark explicitly set fields push results solr. mitch right looking recommendation engine understand question properly yes mahout work though taste recommendation engine supports pretty new owen robin anil mahout action book early release via manning lots assuming list recommendations given user based past behavior recommendation engine could use adjust search results waiting jump best handle. hi savannah comments scattered in-line store xpath expressions text file strings load/compile needed definitely yes using tagsoup clean bad html definitely yes needing per-site rules typically xpath optional regex needed extract specific details common sites powered back-end often re-use general rules markup consistent kind thing use bixo http //replaced.url requires knowledge cascading hadoop order yes would separate job field index though often job titles slight variants would probably work much better automatically found common phrases used otherwise get senior bottlewasher sr. hi navina thanks confirming putall list required method supporting changelog functionality hoping others confirm range _not_ used samza system i.e used internally needed tasks true adding notes methods required used samza system changelog support vs. optional used task-specific code needed would helpful thanks. ago similar issue least back think possible use solr 's copy-field support create boosted version field copied field multiple times. would first try setting post explicitly utf-0 matter appears resin tomcat issues properly handling form data without matter issue configuring tomcat see http //replaced.url patch applied may 0th year work around appears bug tomcat assume version solr patch. actual format extension b characters xml posted. typically run memory solr index update new index searcher getting warmed looking heap often shows ways reduce memory requirements e.g shall see really big chunk used sorted field see http //replaced.url http //replaced.url. took look search results seems like word red shows description tag every sure extracting color information smart color-specific keywords found associated text. given requirement break document separately controlled pieces would create fronts solr handles conversion could think ways using solr feel like unnatural acts general comment acls relatively easy way handle via group ids use restrict query document groupid list group ids authorized access user query converted query groupid xx groupid yy xx/yy groups user belongs. based experience using jira manner would agree mike automatically assign fix go path wind steadily growing wave deferred issues constantly getting pushed next. hi part interesting work creating custom query parser writing unit tests exercised extendeddismaxqparser first created extendeddismaxqparserplugin used create qparser via query something like complexphrase query expecting complex phrase query parser get used 's happening local param treated regular text makes think conceptual model local params processing wrong 's higher level code pre-processing step first hoping 'd get disjunctionmaxqueries queries complexphrasequery would mean processing happen inside extendeddismaxqparser code pointers poke around thanks. currently links seem wind pointing api-0_0_0-alpha versions expected e.g search streamingupdatesolrserver first hit streamingupdatesolrserver solr api follow link get page http //replaced.url. hi documentation wikipediatokenizer specifically wondering pieces source xml get mapped field names solr schema example seems going date field example schema got goes body way get example thanks. worked around way. actually tika htmlparser wraps tagsoup good option. hi robert -xx heapdumppath= something look versus gedankenexperiment. hi hector low query rates using shards approach improve performance multi-core cpus solr cores setup distributing requests effectively use cpu cores parallel request spread shards across spindles maximizing i/o throughput issues approach binary fields work results back b versus actual data short fields get java.lang.short text prefixed every value deep queries result lots extra load e.g 0000th hit shall get shards hits collected/returned dispatcher though unique score returned case followed second request get actual top hits shards something wonky way distributed http requests queued processed load see ioexceptions always n-0 shards succeed shard request fails good reproducible case yet debug. normally would say like getting swap based settings 0gb jvm space used 00gb box confirm nothing else using lots memory right top command showing swap usage right encounter slow search times top command say system load cpu vs. i/o percentages. auto-generated unique value would like. hi floyd typically would creating custom analyzer converts words pinyin zhuyin index would actual hanzi characters plus via copyfield phonetic version search would use dismax search fields higher weighting hanzi field segmentation error prone requires embedding specialized code typically license high quality results commercial vendor first cut approach would use current synonym support map hanzi possible pronunciations numerous open source datasets contain information note might performance issues huge set synonyms weighting phrase matches sufficiently high using dismax think could get reasonable results. run similar issue mix webapps non-solr guys use log0j rotating daily appender since supported default jdk logger mismatch log files produced would looked mention wrapping log0j logmanager big deal bit annoyance search reusable implementation find anything looked good since use resin resin support describing approach taking short-term embedded jetty usage shall back situation ryan arguing use apache commons logging 've enough fun nutch commenting discussion. hi chris would thought queries would allow thing supported currently standard query would recommend re-posting lucene user list. hi otis learned krugle approach worked us block bots search page expose target content via statically linked pages separately generated backing store optimized target search terms extracted search logs. hi geert-jan yup http //replaced.url modifications patch could work thanks info. yes came exactly conclusion could try use ram jvm load index ramdirectory mmapdirectory wound slower configuration smaller jvm relied os-level cache make index accesses really fast trick heard /dev/null force index data. docs need updated believe code wrote back note escape solr uses support using bug two tokens white- note idea still issue regular escaping work around bug parser last character escape build expression looks like xxx signal escape character wrong general escaping characters query gets tricky parser shall save pain suffering since code dismaxrequesthandler added solr iirc tries smart handling escaping. hi christian erik mentioned appears line standardtokenizerimpl.jflex needs updated include extension b character range. would use data files. sir christmas card list shall fire tomorrow morning let know goes. hi particularly uwe robert yes gist question specify use simpletextxxx e.g simpletextstoredfieldsformat solr currently possible thanks. hi martin lurker interesting thread would suggest talking solr committers experience merging lucene got many similarities discussing though solr mature happened seen externally ultimately win though without lot teething issues many seemed personality conflicts groups versus technical/admin/operational issues additional comment inline ps counter example fine separate project solr releases eventually came faster lucene release cycle improved slow-down everyone expecting certainly reduced continuous painful debates belonged solr vs. lucene. general issue months ago generate reports things like scm commit activity given day larger customers users multiple timezones timezone use wrote blog post http //replaced.url short answer ultimately decided use utc times server report api ui least heinous various options. code see seems using fsdirectory another layer wrapping going tom ever get useful results testing curious impact various xxxdirectoryfactory implementations batch indexing thanks. already 'fronts solr web service us pretty easy something similar use case map user list groups using ldap make required clause solr request field contains. confused answer assuming based page referenced url provided approach textprofilesignature would generate different md0 hash single letter change change resulted change quantized frequency word uncommon word would even show signature. hi especially yonik http //replaced.url page mentions duplicate field collapsing later allow duplicate see mention deduplication happens search time normally requires field stored indexed efficiency might need fieldcache wondering status support thoughts potential thanks. ok thanks confirming absolute easiest approach would support new init value codecfactory schemacodecfactory would use select different base codec use versus always using lucenecodec would switch everything different codec could extend schemacodecfactory support additional per-field settings stored fields format etc beyond currently available quick dirty hack specified different codecfactory factory hard-codes simpletextcodec works files simpletextxxx format segments.gen segments_xx files pluggable. processing html definitely use something like nekohtml tagsoup note tika uses tagsoup makes easy special processing specific elements give content handler gets fed stream cleaned-up html elements. hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor 's datadir gets used construction changing coredescriptor 's datadir effect since would go changing datadir core multi- thanks. believe solritas supports autocompletion box wondering anybody experience using lucidworks distro leveraging autocomplete plugin hooking solr facets curious tricks traps getting work thanks. response header and/or meta tags page see code used nutch could missing could wrong either/both issue determining right arbitrary web page easy way analysis advance know sure always x going simplify things soon possible means right processing page collation settings db change db interprets data change data. hi yonik thanks fast response ok curious made bit confusing api well-formed xml xml parser happy reserved xml characters need escaped using org.apache.commons.lang.stringescapeutils seems work pretty well great advice let give try thanks. hi would copied/pasted schema fields testbed schema based version solr using back dark ages version handy comment warning changing version fields multivalued default would checked docs realize behavior changed days would used http //replaced.url examine field would seen thanks. another approach add additional acl field contents would list customers ids access document query implicit acl actual query idea would work case use control access source code krugle enterprise product though using ldap-provided groups line mike 's suggestion. ah yes would explain oddity saw order hits matching document boost since querying field omit norms really short. hi james general immediate updating index continuous stream new content fast search results work opposition searcher 's various caches getting continuously flushed avoid stale content easily kill performance issue interesting topics discussed lucene bof meeting apachecon alone wanting ways clear hard problem relax need immediate updates index accept level lag time receiving new content showing index would suggest splitting two processes backend system deals updates slower interval update search index. hi olivier setting mime explicitly via stream.type parameter. error either schema.xml file messed might still need uncomment lines beginning file ones say uncomment trying use resin version even though using later version resin lots issues xml parsing. note nutch try solve concerns searchers would slow rmi faster hadoop rpc less issue well handle potential summarizer problems case example remote searcher goes away gets bogged times got hit server needs summary case ran load testing though would serious getting completely wrong summary remote index updated search request happened summary requested munge count might enough pretty simple part remember issues servlet-esque objects getting passed deep shall another look afternoon poking weeks ago thanks. got version index appears open luke reports problem grins crafted matching schema tried use index solr solr-trunk either case get exception startup startup logging says trying something destined fail would expected schema/index miss-matches show later right index opened would seen various posts error due corrupt indexes buggy version java obscure lucene none seem apply situation thanks. hi got field defined solr schema always contains two fixed values documents get added boost supplied varies max query field expecting ordering results would match boost values longer specifying omitnorms= true field still get seemingly random results use full search interface solr results two documents different boosts looks like expecting value different different document boost values missing thanks. hi feel like must missing something working customized version supports distributed searching multiple local cores assuming support searchcomponents handler needs create/maintain responsebuilder passed various methods responsebuilder finished list shardrequest objects requests received responses shards inside shardrequest responses list shardresponse objects contain things like solrresponse solrresponse field shardresponse private method set package private appear like easy way create shardresponse objects searchcomponents expect receive inside responsebuilder put custom package shardresponse call setsolrresponse builds run locally deploy jar code runtime get illegal access exception running jetty make work re-building solr.war custom pretty painful thanks. hi erik let us say grins different field besides used autocompletion would places would need hit change field besides terms.fl value layout.vm example asking trying use latest support index uses product_name auto-complete field getting auto-completes happening see solr logs requests made /solr/terms auto-complete look like would expect work seem generating results odd try curling thing use would consider minimum set parameters get expected xml response ideas wrong thanks. try uncommenting lines see fixes problem. exact version resin still confirm uncommented lines. curious mean utf-0 complaint mean able handle utf-0 encoded xml thanks. thanks fast response beginning worry nobody read posts see http //replaced.url attached test code issue plus simple fix json case regards""",7197,"39,54","32,82","55,61","10,89","1,38","0,21","0,06","0,15","0,00","0,00","0,00","1,17","0,07","13,73","2,90","2,49","8,45","2,29","2,20","1,18","0,65","0,25","3,57","0,82","0,03","4,86","0,00","0,01","0,06","7,35","5,38","1,97","0,22","0,47","0,49","22,01","4,93","6,73","3,54","3,52","1,33","1,15","0,92","1,14","2,13","1,31","0,35","0,32","0,61","0,19","0,29","0,04","0,10","11,85","2,90","3,08","4,77","2,75","4,10","0,65","0,22","0,67","0,04","0,10","0,35","0,18","0,51","9,20","4,22","0,00","0,04","0,00","0,00","0,00","2,07","0,03","0,56","0,00","2,28"
"""9a60aceacd1b17a0e6a9d5926fbc40ec9b2df5679a5c3c6c967c0d0fb8475e20""","""fixed hopefully generators shall peek/statsprocessor etc commits go branch. seems files yet uploaded apache dist server status 'beeing released staged change web page wait files available best regards. hi please check current master think problem reported solved http //replaced.url best. dense protocol fails tests support messages compact protocol fixed awaiting results merge. submited patch maybe shall get accepted. good idea ignore gcc failure. hi going travis builds submitted couple pull req failed g++ internal compiler error stop pull reqsts merged left still shame marks requests failed anyone able reproduce issue seems happen mostly compilingtransporttest.cpp maybe machine runs memory compilation single file takes 0.0gb machine maybe split smaller compile units. looks like :methodname compiler code look like :method_name definitely prefer first currently playing around compiler use 'lib assuming recent compiler code move toward look. hey keep old c++ lib least c+0 lib would break iface compatibility avoid confusion point agreed use cpp_v0 new lib user would ask c+0 perfect solution still least solution current c++ lib renamed cpp_v0 gen cpp alias cpp_v0 branch probably little bit outdated n't merged rebased contains full copy cpp_v0 lib std :shared_ptr std :thread instead boost counterparts boost usage reduced optional fields using boost :optional need check maybe compilers already support c+0 std :optional could switch c+0 probably released cpp_v0 become simplified threadmanager tested think last thing missing best regards. updated generators langs unfortunately fill new seems oneway message never fully supported. would go aliases including 'short 'long 'i0 keep everything consistent best. today links working idea shall forward dev somebody higher permissions might needed. hi randy think could somehow merge cpp0 efforts old branch put little hiatus yet think parts could useful best regards. hoped could find anything thrift something apache wide seems projects searching apache standards saw like mention design etc coding put bracket importantly put new feature less text code devs used read code specifications ever saw specs thrift langs design part standards could extracted general shared top level docs rules like keep functions short apply almost langs issues lib public api comments lies. favour keeping things simple possible add new flag current behaviour intuitive simple especially need ask someone add new assignees makes process complicated requires two persons get hold anyone able edit jira users leave assignee usually issue result loose information jira simplification suggested removal close transition hope commiters join discussion shall least try suggest update doc/commitets.md mention anything assignee. yes work vs easiest way use cmake generate vcxproj done automatically pretty sure changes properly reflected make build changes build system like adding new file done directly cmake files vcxproj regenerated still imho easier remembering apply changes build system least twice. somebody check missed lang. hi tried apply 'with common sense files many differences would incrementally would better easier apply small changes make mostly introduced help reorganize patches quickly agree improved need check clang-format versions available right new support something like //clang-format comment could check new opts available personally hate clang-format makes short yet believe consistent even perfect automatic tool better chaos regards. think easier store info jira reference jira issue commit additional info could jira especially simplifies providing info user list etc searching even change commit message personally like client part would prefer component still decide jira integrates workflow change information jira and/or commit message could leave original authors commits could make contributors happy authored commit apache repo. assuming aliases would present parsing time prefer ix notation like explicit yet imagine lot users bits bytes int make idl look like java/c would try detect 'typedef i00 int constructs next step would detect keywords supported languages might good idea would require lot work especially adding new lang maybe 'reserved members names doable stig errors think problem appears generated code langs typedef constructs like c++ idl leads something like typedef int00_t int tries redefine. shall try merge/rebase master include recent changes c++ lib c++ v0 lib probably weekend hope shall find time c+0 quick answer point thrift would introduce std00 std00 etc long answer started branch discussion lead following thrift c++ library best modern c++ updated along new compilers new language library features etc library named cpp v0 mix current new versions c++ standard thrift able make library compatible current newest standard std00 std00 point library become library kept common subset c++ compilers features library enable thrift older outdated environments keeping multiple libs cpp00 cpp00 cpp00 etc would add big maintenance cost thrift best regards. change way things encoded fixed decoding procedure protocol version change needed imho still old server old decoding procedures may support oneway methods sent new clients compact protocol. suspected something break submited pull req see travis say shall look hope explaining comments issue devs. somebody started need oneway calls half normal call pass-through server oneways really nice feature thrift would nice make work properly thrift already defines oneway additional message use. may remember working c+0/cpp_v0 gen/lib http //replaced.url yet put hiatus time ago maybe next month shall find time finish. unnecessary mostly disrupts 'git blame mean new parts keep old ugly inconsistent really believe importance constant refactoring maintenance means iterations code naturally changed 'new coding standards mean naming convention example lines functions untestable fork split split split introduce low level unit tests changes way code looks yet pretty confident move good direction developer never afraid making changes existing code tests ensure nothing gon break tests provide tests needs changed software rot became unmaintainable way tests changed provide future project although like live regards. i0 byte creates inconsistency. sounds great although prefer scons-type tools everything flexible/portable autotools good assume cmake replace automake completely support two building systems like compiler right huge best. cmake generate vcxproj files cmakelists.txt sure requires something specific cmakelists part visual projects could removed repo generated example part release keeping repo really makes leats separate build systems maintain compiler cmake could think releasing thrift.exe using visual would remove dependency mingw. able reproduce steven 's problem seems t_oneway encodes shall debug shifts masks etc hopefully programming message everything ok. oh really hope langs standards like please http //replaced.url added tasks consistency especially going roger 's suggestion lib/ lang/readme.md would nice consistent lib layout avoid mails dev like saw lang x guidelines keep standard ones regards. require lua support try adding without-lua. soo see two potential changes result discussion add dictionary reserved keywords validate variable names a. global dictionary containing merge keywords thrift supported langs make idl portable like level might little easier maintain yet might surprising users changing gen x lead thrift errors 0a 0b would expect error /warning like 'int used c++ java. got shall commit soon check impl compact protocol. tried compiler much different lib wanted sure lib 'future""",1144,"31,78","32,43","60,75","14,86","1,84","0,00","0,00","0,00","0,00","0,00","0,00","1,84","0,09","13,20","4,20","2,53","6,91","2,80","4,02","0,52","0,35","0,35","3,85","1,14","0,00","4,55","0,00","0,00","0,17","7,69","5,86","1,84","0,44","0,35","0,61","24,04","5,07","4,55","4,46","5,24","2,01","2,01","1,14","1,22","1,40","0,96","0,17","0,26","0,61","0,00","0,61","0,61","0,00","15,30","4,46","2,97","6,99","3,85","4,72","0,26","0,00","1,75","0,00","0,00","0,35","0,00","1,31","11,28","3,76","0,00","0,52","0,00","0,00","0,00","0,52","0,17","1,31","0,00","4,98"
"""e1081946adf1d4efd0c90d6d8b1923924184bdabb0f7c60c34facbf404c24e98""","""thought yet pretty busy studying learning basics curly brackets keyboard hint curly brackets italian keyboards love idea open source pretty sure skill-wise community let us see project progresses next weeks regards. exactly background accounting seems fact journal entries stored along original document makes incredibly easy audit track books turn enabled schema-less nature couchdb fact makes easy record data original document _attachment instance together happy see someone else thinking along line regards. hi everyone sort beginner trying hand couchdb-lucene followed instructions github various blog pages around kind stuck ok since playing around version couchdb welcome version 0.0b000000 installed following instructions github seems running curl 'http //replaced.url get seemingly satisfactory response couchdb-lucene welcome version build indexing view like document contain property suspect things fail hookup couchdb couchdb-lucene modified .ini file like must something wrong nothing happens even unable find couchdb-lucene logs supposed changed permissions /target/ snapshot/indexes folder still see changes must something really stupid need help btw love couchdb think tight integration lucene recipe great applications user-experience congratulations everyone involved regards bruno. hi robert thanks kind reply done advised removed update_notification restarting query db reference view continues hope mapping view etc correct get back hmmm couchdb apparently points right db pbo_documents stops somewhere road wrong thanks advance congrats fro great software regards bruno. yes works misinterpreted paul 's comment simply double quotes single quotes shows beginner thanks taking time point error regards bruno doc.member. great hear best luck couchdb ltd.. robert thanks help pulled git yesterday couchdb-external-hook.py assume latest fact download provided two files inside /tools folder home folder look tomorrow try put together list software versions use pretty late europe thanks regards. idiot restarted couchdb-lucene running everything works expected thanks patience regards bruno. hi everyone querying view continue getting invalid utf-0 json problem get away database contains two documents attribute view using attribute key like doc.member tinypic http //replaced.url contains view appears futon try rebuilt yesterday trunk 0.0b000000 strange thing aware tests failing changes oauth rev_stemming thought authorization issue read enacted everything found topic still admin party way mac snow leopard pointers help thanks lot bruno. hi krishna run json json validator http //replaced.url says invalid character rec.add doc return rec would try get json jsonlint first pasting expression view hope helps regards bruno. hi julian notice map function emits every letter author 's instance entry db yields following results key null value using group=true clause increased clarity yields key value key value key value key value key value key r value key x value could another factor contributing slowness view. thanks lot escaping works indeed single quoting however need study wiki understand going regards bruno doc.member. hi everyone intend work couchdb-lucene downloaded latest version yesterday everything ok initial setup able obtain correct responses examples read would like proceed complex indexing getting behaviour explain happens change index functions views and/or alter view names i.e properties object start getting code reason no_such_view messages getting messages changed/altered views querying unchanged views within _design document debug level logs bear explanation least noob eyes cleaning curl -x post http //replaced.url help tried deleting index files help either thing seems work starting afresh new _design document design picked unstable state yesterday would really appreciate help regards bruno. hi robert thanks expressing interest office right provide procedure soon back later day regards bruno. hi zhengji fact working something similar think gnucash far tested following concepts storing accounting entries array alongside original storing chart accounts database source entries together order obtain general ledger far good absolute beginner couch web done volume testing conceptual level couchdb looks like clean powerful solution problems imho regards""",628,"41,87","38,22","63,22","10,99","2,71","0,32","0,16","0,16","0,00","0,00","0,00","2,39","0,00","10,35","1,43","1,75","7,48","0,80","2,39","1,27","0,48","0,32","3,34","0,96","0,00","7,01","0,00","0,00","0,00","9,87","8,44","1,43","0,00","0,32","0,32","17,68","4,46","3,34","2,07","3,66","2,71","1,11","1,27","1,11","4,14","3,03","0,32","0,64","0,64","0,32","0,00","0,32","0,00","13,69","3,82","3,34","6,05","4,30","3,82","1,75","3,03","1,91","0,16","0,00","0,64","0,16","0,64","10,67","4,14","0,00","0,00","0,00","0,00","0,00","2,07","0,32","0,48","0,00","3,66"
"""25d53f5bed4ab97165996b0526a5a9bbc19600442faa72d962fff32cac01dd49""","""higher resolution path build would fail well regards. oh sure parent allow version range hourah thanks let see option ok developers case think reason fill jira entry mean iterating set possible resolutions fail set fails acceptable resolution found fact happen failing parent relationship replaced failing well dependency relationship understand root cause failing build pom considered error stops whole process whereas failing resolve transitive dependency marks particular resolution path unacceptable continue search imho behavior depend relationship parent vs. dependency behavior cases vote continuing version range resolution thanks. looks aether thought manage kind issue direct dependency dependency selector given chance exclude dependency graph hint possible configure aether ignore 'lib 0.0-snapshot resolving version range thanks. hi christian tried result build failure nice colored system seriously different imho misleading error message. point failing build lib 0.0-snapshot pom file result app failed build exists another lib 0.0-snapshot parent 0.0-snapshot abides app-to-lib version range note relationship parent lib dependency relationship instead parent relationship failing case build 'app correctly succeeds exists another lib 0.0-snapshot dependent parent 0.0-snapshot version range. fyi use-case eventspies sap exactly described igor use event spy instrument hudson/jenkins instance monitor builds maven project thrown hudson/jenkins monitored customization maven distribution dropping files lib/ext setting maven easy upgrade regards. hi says suggestions overcome issue explanation let projects three minimalistic sample projects http //replaced.url happens time versions bumped projects deployed remote repo 'parent 0.0-snapshot still remote repo 'parent 0.0-snapshot still remote repo 'parent 0.0-snapshot still remote repo 'parent 0.0-snapshot still remote repo maven tries build pom 'lib 0.0-snapshot fails 'parent 0.0-snapshot available anymore however imho build 'app fail another dependency resolution 'lib 0.0-snapshot 'parent 0.0-snapshot fact relationship parent lib dependency parent-child relationship build 'app succeeds even transitive dependency 'parent 0.0-snapshot resolved note 'app build successful day 'lib 0.0-snapshot turn evicted 'parent 0.0-snapshot still remote repo suggestions overcome issue builds failing every day build farm reason thanks ps content mail duplicated http //replaced.url. sap eventspies used well useless would welcome review convoluted incomplete listener interfaces""",353,"44,12","39,09","52,41","7,08","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,08","1,13","1,70","4,25","0,57","3,40","0,28","0,57","0,00","3,40","5,10","0,00","10,20","4,82","0,00","0,28","10,48","5,67","4,82","0,00","0,00","4,25","13,88","5,67","3,40","0,57","1,70","1,70","0,57","0,00","0,28","1,70","0,85","0,57","0,00","0,00","0,00","0,00","0,00","0,00","9,35","1,98","2,27","4,25","6,52","6,80","0,28","0,00","0,00","0,28","0,00","0,57","0,85","0,00","15,86","2,83","0,00","0,00","0,00","0,00","0,00","5,95","0,57","4,53","0,00","1,98"
"""0d57a6bd7080012afdf24dc0d5ff3095d0b7a90c793c4a35d2739de2dedcbf01""","""preparing patch submission maven-eclipse plugin fix bug enhance functionality setting context-root wtp0.0 project question pom.xml user specify right wtpcomponentwriter grabs webappsrc value maven plugin seems like logical place put contextpath/contextroot parameter time probably appropriate put property used plugin otherwise property could go eclipse plugin configuration feels bit odd reason. think concern could addressed set defaults fall back try following sources order get value found pom.artifactid think plugin always able override project-level value first common system property like contextroot would allow multiple plugins configured property finally currently default""",94,"47,00","42,55","50,00","7,45","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","12,77","3,19","2,13","7,45","1,06","2,13","0,00","1,06","0,00","1,06","2,13","0,00","1,06","0,00","0,00","0,00","3,19","3,19","0,00","0,00","0,00","0,00","18,09","8,51","5,32","3,19","3,19","1,06","0,00","0,00","0,00","2,13","0,00","0,00","2,13","0,00","0,00","0,00","0,00","0,00","19,15","5,32","3,19","7,45","2,13","3,19","0,00","0,00","3,19","0,00","0,00","0,00","0,00","2,13","9,57","3,19","0,00","0,00","0,00","0,00","0,00","3,19","2,13","0,00","0,00","1,06"
"""11411a35018100822513329188627157767176b867623a5d794fb4ba2b98edae""","""problem original poster two years ago solr using shard searching performing sharded query would get empty missing results documents querying shard individually worked anything shards parameter yielded result documents able get results back updating schema include problem seeing solr formulating queries go get records shard including square brackets around ids asking e.g delved solr code saw query string formed querycomponent.createretrievedocs simply calling tostring unique key field value document wanted get value objects somehow arraylists something like strings annoying square brackets showed via tostring emphasizing schema field single-valued lists would hopefully stop appearing think least brackets went away 's relevant querycomponent code check depending may need tha simple tostring comment seems fit theory""",115,"115,00","40,00","61,74","13,91","2,61","0,00","0,00","0,00","0,00","0,00","0,00","2,61","0,00","14,78","2,61","4,35","7,83","2,61","4,35","1,74","0,00","0,00","3,48","0,87","0,00","3,48","0,00","0,00","0,87","9,57","5,22","4,35","0,00","0,87","1,74","28,70","6,96","5,22","6,09","8,70","0,00","1,74","2,61","1,74","3,48","3,48","0,00","0,00","0,00","0,00","0,00","0,00","0,00","10,43","2,61","3,48","4,35","0,87","3,48","0,00","0,00","2,61","0,00","0,00","0,00","0,00","0,00","5,22","1,74","0,00","0,00","0,00","0,00","0,00","0,87","1,74","0,87","0,00","0,00"
"""11c430bae59cd170a1bd2f25110f9c28cc855382f89c010b23760d1af6263d8b""","""changed zkhost use root level works went using. thank much gets moving. hi using solr cloud zookeeper several shard setup try use solr cloud bring shard setup seems load fine without errors however go web interface click 'cloud exception thrown happens shard shard setup anyone seen thank. thank reply work around available could related number external zookeeper servers""",58,"14,50","22,41","60,34","12,07","1,72","0,00","0,00","0,00","0,00","0,00","0,00","1,72","0,00","29,31","1,72","5,17","22,41","0,00","3,45","3,45","1,72","1,72","3,45","0,00","0,00","5,17","0,00","0,00","0,00","8,62","6,90","1,72","0,00","0,00","0,00","20,69","3,45","10,34","1,72","3,45","0,00","0,00","1,72","1,72","3,45","3,45","0,00","0,00","1,72","0,00","1,72","0,00","0,00","13,79","10,34","3,45","0,00","3,45","5,17","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","10,34","5,17","0,00","0,00","0,00","0,00","0,00","0,00","3,45","1,72","0,00","0,00"
"""0c88527e76ab30d41d0b192c43dfa142bb2212fafffd51b02f40c3ce9ded574a""","""yes reason sometimes takes generate argument log.debug code looks something like would course benefit using instead since time-consuming generation message occur log string literals suppose point using. sounds good thresholds/filters distributed loggers log events thresholds/filters applied logging server injected remoteloggingserverplugin case need simple way distribute logger-specific parts config file distributed servers btw thanks excellent job support mailing list simply beyond comparison wrote. sure msdn library says fileshare.read allows subsequent opening file reading change fileshare.readwrite allows subsequent opening file reading writing might better luck hi quick question using rollinglogfileappender webservice logging works whenever try open log file reading another process get sharing violation log file open file would enable minimallock way use exclusivelock another process read log file kind regards. timestamp since start application creation logging event. interop component vb components use interop component error tracing logging interops handle config files well hence using xml file config file load set rollingfile appender properties understand like app.config file put log0net configuration application put log0net configuration using normal log0net configuration syntax separate file e.g logging.config configure application simple line main program xmlconfigurator.configure new make sure use configureandwatch absolutely difference log0net reading file code reading specially-formatted file besides course upside reinvent whole configuration code. browsed log0j 's documentation little came across file appender listens port certain signal comes rolls file i.e closes current file renames starts new file windows alternative sending hup signal make work would sort external log rotation tool complexity log rotation tool could arbitrary could simple windows think solution. would suggest try find locking file tool handle freeware tell split sec see another instance log0net someone/something trying read file latter case lucky specify reader open file read-only shared mode blocking gone case always good idea see problem really changes. hi richard would much like see best give two major log viewers today chainsaw http //replaced.url opinion focused log0j apache/java environment unfortunately freeware viewer definitely space fill. well ilog interface isxxxxenabled methods solve situations need address current log level. reference strong file assembly info assemblyinfo.cs use attribute assembly replace empty string file. thinking along new requests features rolling file appender comes really done logrotate unix tool specialized rotating log files today googled around little astonishment realized seems like made windows port logrotate really strange apparently logs files opposed event log database windows way servers log files like iis generally include rolling mechanism reducing need still store log files matter many variables think set limit rolling file appender leave fancy parts external solutions even means us build log rotating opinion rolling file appender already grown complex several configuration parameters whose side effects cooperation parameters hard foresee without reading source anyone knows windows port logrotate please shout. hi guys feel little stupid matter much browse sdk zilch application detect internal error occurred see case bad configuration file detected testing logmanager.getrepository .configured detect errors say fileappender reason unable open file vague memory seeing noappendersconfigurederror find let us turn question something general would think examples good practises implement personally would like series tests startup app make sure people running applications skilled necessarily developers get decent information something happens. dear question general kind use log0net system architecture involves remoting consider example enterprise system consisting webserver w companys dmz application server inside business logic components connects database d. sometimes business logic components need call components server hosts kind third-party software calls w using .net remoting calls using yes realize reveal person previously forced work dcom world question still still .net world several reasons wanting distribute companys software several servers consider ways go configuration file every server let every component logging locally means logging info single call spread machines passed merge changes configuration duplicated many machines next level might configure remotingappender machines logging persisted transmitting single machine hosts remotingserver sure flexible though local config files say deliver everything remotingappender much say logging server remotingserver picked event ugly solution might let every called component create instances loggingevent similar return array main app takes array logs dream solution might involve special remoting-aware loggermanager takes retreiving caching config files logging server transparently passes events shall logged repository logging server wish list looks like single config file rule darkness bind least place changes made place logging done appenders work net adonetappender netsendappender remotesyslogappender etc may course file logs windows event log machine code logs preferably know remoting scenario exciting problem decision wether log done logging level filters etc set config file another machine really ask would like must many ways think pros cons. sorry line became complicated necessary 's get say. ok urban understand think find looking inside framework though question since use short form format specifier r mean use old version using took find list format specifiers way solve problem would use timestamp r process log file separately would quite easy calculate differences using excel example really small note maintainers nicko text manual page mentioned says conversion specifier starts percent sign followed conversion character correct. well wrote log0net-0. really simple solution loggers create note logger longer static counter keep track instances could course replaced something sophisticated naming logger. rebuilt log0net library strong key message log0net missing strong project. since currently way conditional settings config file would say implement condition code clean way would use custom property e.g 'friendlyappdomain set startup either current appdomain predefined constant depending appdomain looks use property configure file would still make application fairly flexible comes configuring file future still change filename anyway without recompiling personal opinion yeah ask anyway cleanest solution today would different config process imagine someone else know log0net make changes app probably go source diving order figure mechanics behind file naming scheme two config files less elegant completely straightforward him/her understand future might able two appenders different configuration inherit common properties 'virtual appender something like discussed. philip nelson wrote use injection techniques add logging sounds eh interesting would elaborate. tested chainsaw java web start great thing liked production environment web application would much appropriate anyone experience thing favourite personally running apache/tomcat considered. using use standard xml escapes xml guru think values respectively course start wonder need put layout first place whole structure appenders filters layout stuff powerful infrastructure filtering routing files etc. give guys list shot helping""",1048,"43,67","41,22","57,54","13,84","2,19","0,48","0,10","0,19","0,00","0,19","0,00","1,72","0,00","12,60","3,24","1,43","8,30","2,29","3,05","1,34","0,29","0,19","4,68","0,57","0,00","6,30","0,00","0,10","0,38","6,30","4,96","1,24","0,00","0,38","0,19","22,61","6,01","6,11","3,15","4,10","1,62","0,48","1,05","1,72","2,77","1,05","1,05","0,57","0,00","0,00","0,00","0,00","0,00","12,60","2,96","3,53","5,06","3,24","3,34","1,43","0,86","0,48","0,10","0,00","0,48","0,29","0,48","5,63","3,63","0,00","0,00","0,00","0,00","0,00","0,67","0,19","0,38","0,00","0,76"
"""05c55dc3e1b707311232820199d85f66084dfaab49c54c6c4f82163ae468a27f""","""sent phone. sounds intent would work test confirmed working way impression get neither false positive false negative test test intentionally changing behaviour longer supports new syntax support new vote mark existing test range= add new test clone switched sent phone. apache maven team pleased announce release apache maven component provides abstract classes manage report generation run part site generation maven-reporting-api 's mavenreport direct standalone goal invocation maven-plugin-api 's specify version project 's dependencies configuration download appropriate sources etc download page make test. maven team pleased announce release maven plugin http //replaced.url maven plugin plugin used create maven plugin descriptor 's found source tree include jar used generate report files mojos well generic help goal specify version project 's plugin configuration like previous versions fun. apache maven team pleased announce release apache doxia doxia content generation framework provides powerful techniques sites consisting decoration content generated doxia documents like rtf pdf format generate page garbage skin generated content package o.a.m.doxia.siterenderer. apache maven team pleased announce release specify version project parent like following changes since version. maven team pleased announce release apache maven toolchains plugins allows share configuration across plugins example make sure plugins like compiler surefire webstart etc use jdk execution specify version project 's plugin configuration mtoolchains-0 add check plugin works expected. scale note xml schema anything syntax change operating pom instead pom postpone things forever let us find future-proof solution please endless discussions led nowhere far means need revert new import scope behaviour would mind see minor version increment model version far problematic others know else introducing new model building behaviour i'am let 's harm road better wrong course bad xml would make things like polyglot maven even harder since consumer pom something technical edited manually could keep xml forever xml parsers xslt processors available nearly every programming language xml makes sense solved way change semantics could used transform different syntaxes changes made bump model version syntax related fact diff pom-0.0.xml pom-0.0.xml difference would value model value would need deploy two poms summarize need find solution handling different syntaxes solution handle different semantics syntax going bump model versions must clear everyone increment means syntax different semantics minor version leaves patch version bug fixes like changing order elements combine.children attribute quite xml related better think model terms xml currently master would work sure model version increment without change syntax really issue however regards. apache maven team pleased announce release apache maven doxia sitetools extension base doxia component generates either html sites consisting decoration content generated doxia documents like rtf pdf download appropriate sources etc download page duplicate already available velocity tools copy resources call copyresources element string skin wish used internally. maven team pleased announce release apache maven site maven site plugin plugin generates site current specify version project 's plugin configuration download appropriate sources etc download page msite-0 use property items child module site msite-0 inclusion resources basedir velocity used. apache maven team pleased announce release apache maven plugin allows generate pdf version project 's specify version project 's plugin configuration download appropriate sources etc download page mpdf-0 docdescriptors locales default ignored document. development apache maven promotes use dependencies via use default super pom version requirement expect near future update minimum requirement maven jre permit make cleanup plugins fixed quarter maven-eclipse-plugin wagon surefire components yet preparing enable rat check every build next maven-parent pom ensure full compliance currently change cause real guide line definitive descriptor plugins changed accordingly means every new release contains appropriate download link every release reported quarter contains already issue closed report last status topic apachecon europe talk developers said attending expect meet discuss maven future plans users mailing list activity reduced little bit last six month comparison whereas developer list activity less constant within time period. maven team pleased announce release maven skins parent specify version project 's configuration. code maven-internal looked fine seems issue improving toolchains required signature change well able merge global user toolchains assumption maven-toolchains-plugin plugin using specific code adjusted make compatible signatures another thirdparty maven-plugin hitting issue think accept signatures world plugin side always extra handling done either refuse certain maven versions cope signatures since damage already done change gives info instead less would advice keep code as-is provide little code-example solve reflection yes still means need create situations. recently discussion within pmc discovered complete misconception release votes lot people feel voting releases useful pmc wrong digging issue found years ago improved legal requirements releases ensuring vote source distribution convenience binary minimum pmc votes introduced misleading wording voting template binding vs non-binding votes yes non-binding votes binding regarding legal requirements useful feedback looking vote release work real users conclusion decided change wording remove binding vs non-binding count still need find new wording explain issue whole community get feedback already saw experiments around recent votes need feedback change need feedback release votes short need involvement hesitate tell us missed something share ideas hope improve future evolutions beloved apache maven project help whole community regards. see embedded mode core run minutes objection merge embedded mode master regards. put build-pom vs consumer-pom place need publish build poms repository repository consume already- built artifacts dependencies consumer pom without newer maven version prerequisites let people consume key feature able generate consumer-pom good old pom semantics without build configuration useless consumers suppose new include scope could expanded dependencies/dependencymanagement consumer pom example build-poms need central parent poms ie poms pom packaging poms necessary build artifacts putting maven version prerequisite use issue notice consumer-poms parent poms useful imho parent poms build-poms published central pom consumer-poms published sustainable solution pom artifactid limitation generate build-pom consumer- pom since new semantics really equivalent case remember artifact consumption artifact build regards. maven team pleased announce release maven reporting api version. apache maven team pleased announce release apache maven doxia version used future version maven-site- doxia content generation framework provides powerful techniques download appropriate sources etc download page wish. maven team pleased announce release maven archetype available local nexus create-from-project command archetype-0 reading archetype-metadata.xml file would helpful know xml badly formed. maven team pleased announce release apache maven project specified project specify version project 's plugin configuration specified via pluginmanagement. apache maven team pleased announce release apache maven calls information standard jdeps tool please refer specify version project 's plugin configuration download appropriate sources etc download page first release plugin. apache maven team pleased announce release maven dependency plugin provides capability manipulate artifacts copy and/or unpack artifacts local remote repositories specified specify version project 's plugin configuration copies poms twice wish. apache maven team pleased announce release apache maven maven plugin tools contains necessary tools generate rebarbative content like descriptor help documentation specify version project 's plugin configuration broken using tags running twice skiperrornodescriptorsfound. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project 's reports configured pom specify version project 's plugin configuration msite-0 reportplugins should/could inherit information. maven team pleased announce release maven project info plugin used generate reports information project specify version project 's plugin configuration warnings non-integral time-zones plugins. maven team pleased announce release maven ant tasks find binaries find release notes. apache maven team pleased announce release apache maven plugin generates report regarding code used specify version project 's plugin configuration causes parallel build failures would nice see check together output violations wish. codehaus apache done week-end new jira projects ready use asf jira every migrated project codehaus turned read-only moved asf notice added reminder still issues accounts username codehaus already used apache please report issue infra-0 precise info regarding codehaus username expected username apache thank codehaus support migration generally great service given us many years thank apache infra huge work import great result hides problems encountered. apache maven team pleased announce release maven ant find binaries find release notes. maven team pleased announce release maven pmd plugin pmd plugin allows automatically run pmd code analysis tool project 's source code generate site report results specify version project 's plugin configuration corrupted copied target directory wish. maven team pleased announce release apache parent pom pom contains settings likely useful apache project building releasing code maven specify version project 's parent configuration wish parent asf-parent. apache maven team pleased announce release apache maven doxia version used future version maven-site- doxia content generation framework provides powerful techniques download appropriate sources etc download page switch parser pegdown flexmark. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project 's reports configured pom specify version project 's plugin configuration download appropriate sources etc download page comes settings.xml wish msite-0 much information maven logs site deploy. apache maven team pleased announce release apache maven plugin utility plugin allow publishing maven website supported scm primary goal utility plugin allow apache tested git scm example push content github specify version project 's plugin configuration. someone interested build fails run parallel mode -t0 could reproduce failure local machine someone interested discover goes wrong parallel investigate wanted false positive asf jenkins regards. maven team pleased announce release maven project info reports specified project specify version project 's plugin configuration mpir-0 create new report show include module different new feature. remarks sub-project taken experience working first see facts http //replaced.url complete list projects documented pmcs lot software described grouped pmcs came conclusion question semantic around project either talk tlps sub-projects trying visions http //replaced.url started tlp sub-projects vision tlp pretty much used us projects top level projects sub-projects bad impression puts -ones fact committees project top see commons logging committees really main project projects like extensions plugin see ant velocity imho talking committees projects best way avoid bad passion comes tlps sub-projects vision terms question merging tlps becomes merging committees ie communities putting projects management merged committee imho description verbose debate less passionated focused main question really community managed committee opinion kafka samza case hope explanations help discussion regards. migration start 00h every jira project codehaus migrated asf marked read-only remember check jira account email settings codehaus asf sure result asf everything tracked http //replaced.url. herve change look ones. maven team pleased announce release parent poms maven default config wish parent asf-parent default config wish default config wish. maven team pleased announce release maven project info reports specified project specify version project 's plugin configuration. maven team pleased announce release apache maven specify version project 's dependency configuration mshared-0 dependencies inside pluginmanagement taken account reporting defined. maven team pleased announce release maven archetypes see http //replaced.url. apache maven team pleased announce release maven site site plugin used generate site project generated site includes project 's reports configured pom specify version project 's plugin configuration output parameter msite-0 report inheritance work specified site wish. apache maven team pleased announce release maven project maven project info reports plugin used generate reports information project specify version project 's plugin configuration produce output changed. maven team pleased announce release maven archetype archetype-0 archetype downloaded pom placed working metadata.xml descriptors generated needed create-from-project goal. closed issue already fixed. maven team pleased announce release maven ant tasks maven ant tasks allow several features maven used ant deployment maven repository information available current version maven ant tasks downloaded project defined contrary documentation wish. maven team pleased announce release maven plugin plugin uses tool generate project specify version project 's plugin configuration project modules never installed/deployed. apache maven team pleased announce release apache either html sites consisting decoration content generated doxia documents like rtf pdf download appropriate sources etc download page get info report. maven team pleased announce release maven dependency dependency plugin provides capability manipulate artifacts copy and/or unpack artifacts local remote repositories specified specify version project 's plugin configuration. maven team pleased announce release maven archetype even redirection package instead packageinpathformat descriptor contains token project old artifact create-from-project command required property archetype-metadata.xml properties work due faulty ordering fileset archetype like done old 0.x archetype archetype-0 allow fields like scm developers licenses etc set generating archetype code archetype add-archetype-metadata. apache maven team pleased announce release apache maven component provides abstract classes manage report generation run part site generation maven-reporting-api 's mavenreport direct standalone goal invocation maven-plugin-api 's specify dependency project 's dependency configuration sub-task. apache maven team pleased announce release apache maven maven plugin tools contains necessary tools able produce maven plugins scripting languages generate rebarbative content like descriptor help documentation maven plugin plugin used create maven plugin descriptor 's found source tree include jar used generate report files mojos well updating plugin registry artifact metadata generating generic help goal specify version project 's plugin configuration wish metadata create new version descriptor. apache maven team pleased announce release apache maven maven-scm-publish-plugin utility plugin allow publishing maven website supported scm primary goal utility plugin allow apache projects publish maven websites via asf svnpubsub system plugin tested git scm example push specify version project 's plugin configuration seconds retry. unit test failing jenkins regarding jgit reproduce machine anybody clue special jenkins regards. apache maven team pleased announce release apache maven doxia sitetools extension base doxia component generates either html sites consisting decoration content generated doxia documents like rtf pdf download appropriate sources etc download page date date without precision created last modified siterenderer side rather velocity side modules found wish. development apache maven promotes use dependencies via shows plugins ready finished expect contest create new maven create mascot maven owl work progress integrate maven studying jira migration codehaus apache better end-users consistency since got feedback users lost requiring compliance issue closed report last status topic users mailing list activity reduced little bit last strongly increased within time frame http //replaced.url. sent replaced email.addr.es like jira issues told mailing list conf fixed days ago infra-0 regards. hi announced quarterly report january studying jira migration codehaus apache migration happen week- create exact jira projects asf codehaus copy whole content mark codehaus read-only example done years ago near empty mpom moved http //replaced.url username migration codehaus asf happen migrating existing content user accounts migrated 's way jira works avoid creating accounts already exist apache matching done account 's e-mail address mapping look something like e-mail/username pair codehaus content email associated user asf jira instance reuse else create new account asf jira instance exact suffix still needs defined sure username apache jira please look e-mail codehaus check matches e-mail apache question hesitate ask keep informed operation planned details. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project 's reports configured pom notice breaking changes know upgrading specify version project 's plugin configuration download appropriate sources etc download page sub-task generation jar menus point index.html ignored unless file 'generated-site/markdown/ distributionmanagement site url another defined child project property maven.site.skip set true available generated-site component wish""",2549,"41,11","44,13","49,23","6,67","0,75","0,20","0,04","0,16","0,00","0,00","0,00","0,55","0,04","6,39","1,10","2,16","3,49","0,43","1,02","0,90","0,31","0,31","4,79","1,02","0,00","5,14","0,51","0,00","0,20","5,73","4,71","1,02","0,16","0,04","0,35","16,40","2,75","5,96","2,12","1,14","1,92","1,18","0,71","0,94","0,82","0,51","0,08","0,20","0,16","0,08","0,00","0,08","0,00","9,14","2,16","2,86","3,73","9,18","5,10","2,24","0,08","1,02","0,00","0,04","0,08","0,12","0,55","9,57","3,33","0,00","0,00","0,00","0,00","0,00","3,49","0,08","1,77","0,00","0,90"
"""30b647497e524daa7f575690abd21920047511555b6fd6a469e4bd2a314579f1""","""hi jason refined proposal part user proposals page regards. hi compare maven dependency mechanisms home-brewn solution company among others major thing different maven know concept artifact life cycle least know mechanism refer build life cycle life cycle status information would allow extend dependency management new dimension could declare whether certain dependencies actually allowed used""",55,"27,50","41,82","60,00","16,36","3,64","0,00","0,00","0,00","0,00","0,00","0,00","3,64","0,00","9,09","3,64","1,82","3,64","1,82","1,82","1,82","1,82","0,00","7,27","0,00","0,00","3,64","0,00","0,00","0,00","1,82","1,82","0,00","0,00","0,00","0,00","21,82","9,09","7,27","3,64","0,00","1,82","0,00","0,00","1,82","0,00","0,00","0,00","0,00","5,45","0,00","5,45","0,00","0,00","9,09","0,00","1,82","7,27","3,64","1,82","0,00","1,82","1,82","0,00","0,00","0,00","0,00","0,00","7,27","1,82","0,00","0,00","0,00","0,00","0,00","1,82","3,64","0,00","0,00","0,00"
"""e5ef67ecbaaf1d8e8992ed3534836b7b1d76f86fb60f4e6a14fc65d14afa33c2""","""hello understand strictly concern blocker release maven scm-0 still blocking upgrade 0.x including 0.0-rc0 anyone using mercurial hg scm absolute paths remote server maven release plugin chance get fix included. think actually tracked defaulturlnormalizer module maven-model-builder indeed part maven-core defaulturlnormalizer following element scm/connection removes double slash hostname part url normalization occurs release plugin find plugin descriptor instead mavenproject.getscm .getconnection used retrieve scm url method returns scm connection url normalized important double slash removed somewhere lost maze maven-core sorry pinpoint issue better. right weeks ago made weak attempt look source code scm module found nothing obvious something wrong url component actually parses pom.xml extracts string scm/connection element maven use scm url mercurial contains absolute relative path handed hg mercurial looks project wrong place release build checkout fails mercurial special non-standard url annotation two consecutive slashes hostname separate relative paths absolute paths example scm url works fails 0.x double slashes 'localhost tells hg file path absolute file system located '/opt/foo double slash removed hg instead search directory 'opt/foo relative home directory user server perhaps '/home/luser/opt/foo. double slashes issue scm-0 opened mng-issue contains simplistic patch fix relevant test case fix might bit invasive affects url scm connection url alternative perhaps normal tests passes suppose original code removes double slashes reason avoiding http redirects perhaps anyone know oh documentation mentions profile integration tests -p run-its maven tells profile""",245,"61,25","38,78","44,08","10,20","2,04","0,00","0,00","0,00","0,00","0,00","0,00","2,04","0,00","5,71","0,41","1,63","3,67","0,41","3,27","0,00","0,00","0,41","4,08","2,45","0,00","2,45","0,00","0,00","0,00","4,90","1,63","3,27","0,41","0,00","1,22","16,73","2,45","2,86","0,41","4,08","2,45","2,04","0,82","0,41","0,82","0,82","0,00","0,00","0,41","0,00","0,41","0,00","0,00","9,39","2,86","4,49","1,63","3,67","2,86","0,00","0,82","0,00","0,00","0,00","0,41","0,00","0,00","13,88","3,27","0,00","0,00","0,00","0,00","0,00","4,49","0,82","1,63","0,00","3,67"
"""4ee8a6b0f4c6cbd020ffa14217436cc3226ba516a8fb32a15047fe61784dc626""","""hello quite dumb question new nutch/solr migrating web indexer commercial product nutch/solr yet understood internal need spending time problem installed nutch/solr everything works fine solr standalone mode even installed solr install_solr_service.sh everything fine go pass solr cloud mode found elegant way modify /etc/init.d/solr /etc/default/solr.in.sh order launch solr cloud mode succeeded start dirty point hints links cleanly remark reason trying migrate solrcloud able basic auth around documents index standalone enough us regards. thank john good start understand seems complicated though got way regards. hi john though launch solr -c option last tried""",104,"34,67","26,92","54,81","11,54","2,88","0,96","0,00","0,96","0,00","0,00","0,00","1,92","0,00","10,58","0,00","3,85","6,73","0,00","4,81","1,92","1,92","0,00","2,88","0,00","0,96","5,77","0,00","0,00","0,00","7,69","5,77","1,92","0,00","0,96","0,00","17,31","7,69","3,85","1,92","3,85","1,92","0,00","0,96","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","14,42","1,92","3,85","5,77","4,81","5,77","0,00","0,96","0,96","0,00","0,00","0,00","0,00","0,00","19,23","5,77","0,00","0,00","0,00","0,00","0,00","0,96","1,92","0,00","0,00","10,58"
"""eb4a21a7dedf4a3b309af5d9dc0a1012b0a3a4a736db77d1d539917f19d0c317""","""currently initial patch seems work would like know whether overall code ok. added documentation properties configdef suggests sure importance assigned properties normal properties find info totally sure validations correct tried figure code still might miss something finally mailing list right place ask questions submit patch jira ticket get review even sure quality thanks help patch used specifying set expected configurations defaults override topic created altered defaults.compact compact else delete configuration s\ .format s\ .format check given properties contain log config names values parsed check given properties contain log config names values parsed parse values unfortunately validate smaller number replicas since information newline file""",104,"52,00","40,38","55,77","8,65","0,96","0,00","0,00","0,00","0,00","0,00","0,00","0,96","0,00","11,54","1,92","3,85","5,77","1,92","2,88","0,96","0,96","0,00","0,96","0,00","0,00","4,81","0,00","0,00","0,00","13,46","10,58","2,88","0,00","0,00","0,96","24,04","6,73","2,88","2,88","4,81","4,81","1,92","0,00","1,92","0,00","0,00","0,00","0,00","0,96","0,00","0,96","0,00","0,00","7,69","0,00","2,88","4,81","2,88","3,85","0,96","0,00","1,92","0,00","0,00","0,96","0,00","0,00","7,69","3,85","0,00","0,00","0,00","0,00","0,00","0,00","1,92","0,00","0,00","1,92"
"""23f365c7cc67a9c7e0569e37f4dc4a7569f5320786759c5b92b12e887d7c5fa4""","""believe misreading specification continuation defined otherchar newline otherchar defined anything nul cr lf words even continuation line lf allowed. hi two months ago vote held releasing maven-changes-plugin see issues raised particular concerning maven-changelog-plugin opposed maven-changes-plugin conclusion defer release currently single issue open mchanges-0 never able reproduce called blocker anyways least imo question whether would possible release thing finally helps would glad required work. synchronized ibiblio normal write unfortunately belongs apsite group member imo apcvs committer access. thank. hi mjar-0 mjar-0 fixed suggested brett releasing maven-archiver-plugin maven-jar-plugin thanks. due release axiom thanks. following question particular considering extremely nasty release policy maven project maven-jar-plugin seen release eight months although plugin extremely important nasty bugs maven-changes-plugin seen release ui know long although plugin required migrating maven. thanks issue created. pops question expect move regards. already patch mjar-0 ready work someone volunteers work pull patches. running attached test case might well added plexus-archiver whenever fixed demonstrates plexus archivers issue tracker file issue attach test regards""",180,"16,36","40,00","51,67","11,11","1,67","0,00","0,00","0,00","0,00","0,00","0,00","1,67","0,00","8,33","1,67","2,22","4,44","1,67","2,22","0,00","2,22","0,56","2,22","3,33","0,00","6,67","0,00","0,00","0,00","7,22","5,56","1,67","0,00","1,11","0,00","22,22","3,89","2,78","2,22","3,89","3,33","2,78","0,56","0,56","1,67","1,67","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,67","3,89","0,56","7,22","5,00","5,00","0,56","0,00","0,00","0,56","0,00","0,00","0,56","0,00","17,22","5,56","0,00","0,00","0,00","0,00","0,00","10,56","1,11","0,00","0,00","0,00"
"""3875138fac55dea58c60afbfb915d2f3c3cdd2304c980682a90a6b1811544513""","""try use temp view couchdb response 'temporary views supported couchdb status_code see temporary views supported couchdb0.0 documents changelog page broken http //replaced.url""",25,"25,00","44,00","64,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","20,00","0,00","8,00","12,00","0,00","0,00","0,00","0,00","0,00","4,00","4,00","0,00","0,00","0,00","0,00","0,00","8,00","8,00","0,00","0,00","0,00","0,00","16,00","0,00","8,00","0,00","8,00","0,00","0,00","0,00","0,00","16,00","16,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","12,00","4,00","0,00","8,00","0,00","4,00","0,00","12,00","0,00","0,00","0,00","0,00","0,00","0,00","28,00","4,00","0,00","0,00","0,00","0,00","0,00","0,00","8,00","4,00","0,00","12,00"
"""7a6ddb98f89bc8f6a761a842d9297a7972e1ff5e70a7321b7610bc6f65472b87""","""use case fwiw using couple block contributions used components used ajax stopped working introducing main really handy component automatically rendering really clue fix shall love working thanks denis bringing discussion. projects taken extreme allowing index pages context enforcing rule code workaround trick pagerenderdispatcher skip index pages adding new dispatcher pipeline forces empty_context index pages module http //replaced.url btw talked like months ago http //replaced.url. hi heard wrails project sorry senro project senro http//www.senro.org like wicket rails check http //replaced.url glad chris nelson copyright note still comes perfect timing complete beautiful week open pd sorry bother stupid things needed catharsis action. problem maven latest versions think using maven use maven build tapestry0. tag fix issues clientdataencodertest merged back branch component fixed broken tapestry.png 0c000af tag fix tests broken recent change 0b0badd use application root package application commit looks like could related issue experiencing""",152,"30,40","27,63","49,34","5,92","0,66","0,00","0,00","0,00","0,00","0,00","0,00","0,66","0,00","12,50","1,32","4,61","6,58","0,66","2,63","0,00","0,00","0,00","1,97","0,00","0,00","3,29","0,00","0,00","0,00","7,89","3,29","4,61","0,00","1,97","0,66","16,45","1,32","7,24","1,97","0,00","1,97","1,32","0,66","1,32","2,63","1,32","0,66","0,00","0,66","0,00","0,00","0,66","0,00","12,50","4,61","1,32","6,58","5,92","1,97","0,00","0,00","0,66","0,00","0,00","0,00","0,00","1,97","13,82","6,58","0,00","0,00","0,00","0,00","0,00","0,00","1,32","0,00","0,00","5,92"
"""d3bbe7127318e1711f6fb3991caba157662051ce4fb2400f6db294afca2bb977""","""indeed mngeclipse jetty gone years codehaus support assitance thank much. assumption yes. probably query resolution unresolved. done. hi folks would like another bug squash like last year currently unresolved bugs updated three years hardly believe updated ever maybe resolved new versions automatically please look whether see still anything else object go ahead close fix let know week. hi devs possible push fluido skin work happens completed fixes time ago would like another bootstrap 0.x based released modification work bootstrap 0.x begins lot stuff time. think special idea mind could read snippet retrieve bytes specific encoding compare guessing encoding tricky would require icu0j course user supplies wrong encoding lost. think must tandem packaging zip finally. would favor move java make strong use nio0 file operations lot pain go away. course piss lot people would course several reasons people n't popularity old collisions etc even enforce happen maven added plugin dev center. though minor doc issues related msite-0 downgrade advise velocity noted caution guarantee velocity request tools properly work additionally filtering section updated context variable makes easy display dotted maven properties. non-binding. quite annoying took quite time figure correct settings git cmd egit even know whether got right subversion made way easier stackoverflow good help place anyone better cross-os setting. hi herve seems like entire process requires another sign fortunately already jira account xircles account wrote codehaus support asked transferring account intend recreate jira account created many tickets current thanks. hi brett thanks promotion seems like edit tickets old created ages ago able drop completely thanks. always minutes. hi release preps constantly failing build maven mvn -preporting site help simply means actually tried read jar file skin received classes directory target traced proceed debugging debug symbols match source code anyone clue system tries resolve instead local repository highly assume sonatype eclipse aether issue insights welcome. think credit goes github eases participation non-committers tremendously though need improve pr process mirrored repos. thank much bringing personally disgusted stupid names proposed like boom shotgun anything else related rifle arming. updated good possible. considered running versions display-plugin-updates. question obviously. located lifecycle phase 'maven-plugin. hi folks in-house project jcl-over-slf0j nothing special far module dependends net.sf.michael-o.dirctxsrc dircontextsource jar:0 compile slf0j-simple test scope mvn dependency tree properly gives master gives questions another fix master relied erratic behavior core previously depmgmthave influence transitive deps direct really say right wrong two slf0j bindings slf0j simple logback obviously wrong thoughts. vote please period already due. glad found documentation hard would like repeat information serveral places need update maybe interpolation issue please open jira issue thanks. might added http //replaced.url. would like fluido reference parent released yet ok actually moreover found another bug fluido would like report fix. opened issues least fixed. working sample project attached already please check. simply list dependencies report. infact added introduce maven.conf m0.conf mng-0 make conf modification /etc /usr/local/etc snap. hi previously suggested makes sense retire skins updated long time resources maintain properly last releases therefore propose retire skins vote successful make final release skin making clear skin site retired source code moved retired area subversion repository process retiring skin described though plugins process universal vote open hours yes time would ask kindly already cast vote revote make. thanks fix added purpose resembles output site tool generates modules menu looks awkward outputs uniform. hi folks would like patch several issues jira unfortunately right change tickets e.g target version component forth anyone able promote thanks. opposing previous proposals really see urgent need animal link look meaning maven clearly see refers well-educated human think needs evaluated though know represent graphic. possible ran uts twice check dep tree mediated. dependency different poms dependency think like inject annotation guice thank. could announce least five days advance would give least timeframe merge prs handle issues. http //replaced.url. file jira ticket. hi paul looks way better thing though alread fixed issue bound distinct maven version change e.g. mng-0. hi changes since last release. given companies/folks react something discontinued would move java baseline christman first release java e.g. policy make apps run java months even compatible n't concern. open issue infra inquire default pr procedure look like mirrored repos github think need common approach entire asf. fact mentioned front page skin addtionally maven. hi robert back october result retirement vote positive plugin still without notice issues still open jira repo github simply. hi barrie docs definitively correspond poms moment chapter 'staging latest documentation says release prepared simply wrong release prepared performed documentation staged due missing next problem made push pom back forth simply work scm publish bound parent site-deploy phase site stage-deploy goal scm-publish publish-scm works know whether correct verification kinda awkward says wait sync linked site say anything sync given timeframe sync occurs path '/www/maven.apache.org/pom exist '/www/maven.apache.org/content/pom moreover directory skins updated skins-0 directory exists pom-archives skipped steps stuck lost help would greatly appreciated. already. terms contributions zero someone keen enough code prepare patch simply lazy need fork clone push create pull request even work see benefit. hi still couple issues left jira. hi michael staged site http //replaced.url. pages missing apt files remains empty 'maven actually fixed. big question would like clarify first commons compress gone java minor release people requesting. proverb n't make think rm mono-module multi-module project he/she needs know perform steps fine relase docs scattered several pages references would like avoid another exception. horribly afraid right documentation says b defined b 's parent 's dependency management section since dependency management selected referenced 's pom b compile scope thanks. hi folks anyone already noticed plugin http //replaced.url might automate test pr jason wants thoughts. nobody retiring fluido skin subject. hi still couple issues left jira note release depends release skins parent. yes resposibility always good simply make build fail instead log collision happens. file issue. note maven-0 repo probably typo. thanks announcement actively providing several years finally able contribute officially directly thank. currently running patched version maven offensive commit reverted surefire master freebsd 0-stable shall see wether cause hour. able restage tomorrow working patch doxia-0 would like commit. done http //replaced.url completely disassembled tarball build maven theirselves non-canoncial build likely going support modifications known us rather untar offical tarball rebuilt package though grab tarball internet somehow. mshared-0. think looking maven embedder. would drop altogether go. confirm tried wagon 0-snapshot maven master. hi vote passed following result promote artifacts central repo pmcs please promote source release zip file add release. thanks turns something wrong repo r.a.o able browse yesterday try browse nexus ui get http repos broken file issue infra. insane works thank much need fix fail w/ maven 0.x due aether advise must raise mpir msite mksins-0 work anymore may others. hi folks herve quite busy right preparing doxia sitetools along maven site plugin several tickets would like fix changes impact skins provide need adapted close look skins updated years active improvements happen fluido skin given fact probably work none changed html0 would like retire skins keep two active would ultimately mean two updated later proposal please share opinion agree start vote retire. looks overengineered types zips jar either. following services unavailable upgrade. quick search central shall even apache project adhere convention receive. yes works stupid fault simply noticed use secured url used read-only url sorry noise. done r0000000. yes shame simply forgot agenda already ok. hi folks performed another cleanup jira last couple days old issues right open issues manageble point view noticed several plugins touched years status following plugins anyone working planning release version thoughts objections would like retire clean jira plugin plugin. hi paul good idea done already pre-0.0 versions going think strictly follow no-fix-version-for-incomplete/invalid/wontfix able assign real versions great. read mind idea weeks option work strongly support remove build files ant maven maven along maven ant tasks need removed need free use aether ant tasks. aware change intentional conforms poms procedure described docs nothing nothing less. think fully correct leave goals defaults distributionmanagement/site element perform site-deploy default check m-relase-p docs say. parse.vm lives next template.vm call parse 'parse.vm macro available rest though tried probably create custom skin really create. unreachable two different carriers. canceling vote due problems pom files herve found shall respin vote today. yes maven mvn -prun-its install build.log files say updated msite mpir projects touched subversion. thank offer best time go channel. period new issues asf jira someone found issue already codehaus jira migrated course. fine would like note first announce publically users mailing list think deserves major bump plugin version wdyt. thanks guys look http //replaced.url next couple days process required. help becuase repos located github apache. n't course solve problem pre-existing files give clean way things right new files. hi folks seems like full access maven directory see anyone able check error correct permissions username michaelo thanks. model would require maven cleanup env like m0_home .m0 m0.conf forth otherwise break current limitations without. hi folks recently proposed mskins-0 downgrade 0.x two reasons updated site.vm custom.css bootstrap yet knows real changes model change must happen patch version received feedback neither positive negative would like perform downgrade site rendering work expected additionally made several improvements css rendering stuff locally parent project would like merge back plugin bring site plugins let us postpone bootstrap 0.x fluido skin 0.x. think variations depenencies must go either classifiers build qualiafiers start allow arbitrary elements people start ask custom elements x. therefore custom elements allow plugin 's. avail still error leave profile reporting. vote please. ok thank process sites later day. noticed work therefore asking last 0.x release branch intended able cut release permissions promoting release thanks. hi devs promote staged repos skins parent fluido skin gone happened auto-dropper active rao source releases already promoted herve thanks. thanks change poms accordingly. please reply promotion http //replaced.url would like manage release like codehaus jira thanks. hi folks actually proceed third-party plugins comply naming pattern given plugin created document beeen first published simply add disclaimer legacy reasons plugins central created date. rather add plain text release notes html junk unreadible. first thing came mind. convention way omit last zero done maven plugins/components. cause problem dependency transitive dpmgt section read/compared mdep would least expect warning anyone else. already known issue year http //replaced.url. guys agreed reuse tags apply procedure tomcat team staged build fit abandoned tag remains relased added change log http //replaced.url see version opinion release notes shall mention skipped skipped. hi tibor currently running full test bed various maven versions surefire master pass log files along target directory shall join channel. forgot always use relative path docs otherwise could misinterpreted. ultimately mean plugins run maven someone still uses older version java build projects. cloned tested upgrade msite pain go away parse/ include. aware retirement plan procedure go anyawy post list first figure plugins really dead sorry part. yes http //replaced.url obstained merge patch version minds course""",1881,"15,68","33,12","59,22","12,44","1,49","0,27","0,00","0,11","0,00","0,11","0,05","1,22","0,05","11,00","2,60","2,07","6,59","2,07","3,24","0,69","0,64","0,53","3,93","1,97","0,05","5,79","0,85","0,00","0,37","7,81","5,74","2,07","0,32","0,43","0,48","17,65","4,09","3,46","2,87","2,92","2,13","0,90","0,90","0,80","2,23","0,90","0,48","0,80","0,80","0,64","0,16","0,00","0,00","13,82","3,77","2,76","7,60","5,32","4,84","1,01","0,37","0,58","0,00","0,05","0,58","0,05","0,48","14,04","8,51","0,00","0,05","0,00","0,00","0,00","2,45","0,11","0,74","0,00","2,18"
"""05e13208ea5297172044cb524152d78290e0894d7aa7ed77a8b683ef1833f8b9""","""way query table based binary column bytes proofpoint inc. sql query looks part like sometimes returns port values ip column ip values port column delete group first/last sections seems work fine run query cores runs fine fail part number cores effecting bug makes pretty sure 's race condition along fact printing values changes bug 've looked client code n't seen anything obvious occurred suppose could server code anyone else seen anything like proofpoint inc""",75,"37,50","20,00","64,00","12,00","4,00","0,00","0,00","0,00","0,00","0,00","0,00","4,00","0,00","10,67","1,33","4,00","5,33","0,00","0,00","1,33","0,00","0,00","4,00","1,33","0,00","2,67","0,00","0,00","0,00","13,33","10,67","2,67","0,00","0,00","1,33","25,33","6,67","5,33","1,33","9,33","4,00","0,00","1,33","0,00","9,33","9,33","0,00","0,00","1,33","0,00","1,33","0,00","0,00","10,67","4,00","0,00","6,67","5,33","5,33","0,00","0,00","0,00","0,00","0,00","0,00","0,00","1,33","9,33","1,33","0,00","0,00","0,00","0,00","0,00","0,00","2,67","4,00","0,00","1,33"
"""b88e5ab8b78d81d5d2641e4ea1e017560904a5a481c38692833f255be2bee640""","""deb package rpm. thinking frequent example theory using tokens dc0 dc0 significantly affect key distribution specifically two keys move next much however seems unexplained requirement least could find explanation nodes must unique token even put different circle networktopologystrategy otherwise data evenly distributed. opposite true requests fast slow case percentile fast slow except order samples opposite direction usual. yes column stored value every key may matter switch compression afaik advantages default worried storage space mysql table intend move cassandra columns long column names average characters column values mostly byte integers hand many colums empty specifically null afaik mysql able simply store column value default table data without indexes mysql gb millions rows cassandra gb without compression gb compression includes single index row compression switched specific case storage requirements roughly cassandra mysql sorted key sorted column. cassandra testing using old server core celeron processor 0gib ram another 0gib cores two consumer sata hard disks works i.e memory error etc writes reads second maybe database extremely small even days megabytes configuration absolute stock configuration changed anything except noticable node small server remember somewhere hand noticable interesting disk higher log hard disk contained system data disk grain salt intention test setting cluster two distant datacenters performance test. appearance old rows caused old timestamps set columns turn caused threadlocals cleaned since fixed timestamp rows returned corresponds latest saved state every case. noticed strange phenomenon cassandra would like know something completely impossible see log extract new versions row written reads returns obsolete data read version even already written single cassandra node cluster client local network rows written read seconds would think test environment see obsolete data actually thousands log entries hours test say row read match latest data written checked detail history another node seems eventually receive up-to-date row took minutes specific case fyi started evaluate cassandra without significant. thank mail restarted affected server noticed mail likely related leap second introduced today. requirement nodes unique token still global cluster/ring node needs unique logically seperate rings networktopologystrategy puts rest code. played test cluster stopping cassandra node updating row another noticed delay delivering hinted handoffs know rationale node originally received update noticed server waited started pushing hints log endpoint. copy init.d datastax package. use propertyfilesnitch networktopologystrategy create multi-datacenter setup two circles start reading page http //replaced.url moreover tokens must unique even across datacenters although pure curiosity wonder rationale behind way someone enlighten first line output nodetool obviously contains token nothing else seems like formatting glitch maybe role""",416,"34,67","39,90","56,73","15,62","1,20","0,24","0,24","0,00","0,00","0,00","0,00","0,96","0,00","7,93","1,44","2,64","3,85","1,20","3,12","2,40","0,72","0,96","5,53","2,16","0,00","1,92","0,00","0,00","0,00","3,12","2,40","0,72","0,24","0,00","0,24","18,51","5,53","3,37","1,68","3,37","2,40","2,40","0,24","1,44","4,81","2,40","0,72","1,20","0,48","0,48","0,00","0,00","0,00","15,38","2,64","2,88","8,89","3,61","2,16","1,20","0,24","0,96","0,00","0,00","0,24","0,00","0,24","5,29","3,37","0,00","0,00","0,00","0,00","0,00","0,72","0,48","0,00","0,00","0,72"
"""ddca80046db3358a64436ad45a3fcd95c3e42bd773e997bc5acc2d6892cb3795""","""open issue related matters seem received much attention regards""",9,"9,00","55,56","88,89","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,11","0,00","0,00","11,11","0,00","0,00","0,00","0,00","0,00","11,11","0,00","0,00","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","66,67","33,33","0,00","0,00","11,11","0,00","0,00","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","22,22","11,11","11,11","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","22,22","0,00","0,00","0,00","0,00","0,00","0,00","0,00","22,22","0,00","0,00","0,00"
"""4ce4268ef0b844ab722cf9bdcc579e412df2a4bf8d392d4743e1b92839384627""","""trying understand relationship emitted transferred values spout trident topology multiple instances topology executing expected ratio emitted transferred would least similar instances topology differ quite bit example 0x another unit measured tuple tree storm topologies trident batch. trident topology processes tuples opaque kafka spout three bolts write different forms processed output form different record parallelism rotation policy applied three outputs results two streams writing small files hdfs 'm thinking setting different parallelism outputs reason different parallelism settings across output bolts. read storm-0 http //replaced.url question work hdfs state trident topology ensure record ends written hdfs states none. thanks helps answer question batch fails could records batch get written disk hdfs state another""",113,"28,25","45,13","39,82","10,62","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,08","1,77","1,77","3,54","0,88","0,00","0,88","0,00","0,88","6,19","3,54","0,00","5,31","0,00","0,00","0,00","3,54","2,65","0,88","0,00","0,00","0,88","11,50","5,31","1,77","2,65","2,65","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,08","0,88","1,77","2,65","5,31","2,65","0,88","0,00","0,00","0,00","0,00","0,00","0,00","0,00","8,85","3,54","0,00","0,00","0,00","0,00","0,00","0,88","1,77","0,88","0,00","1,77"
"""804a7d6bac6f39e9ab931b81ed7b0092838d087a9ab4a1ba46520d1a84533188""","""possible solution might simple create virtualhost apache0 points particular documentroot file /etc/apache0/sites-available/couchdb.example.com allowoverride none sudo /etc/init.d/apache0 equivalent system /var/www/couchdb.example.com/htdocs/ following content. hi edward message would like accomplish following use flash access data couchdb requires couchdb serve file called 'crossdomain.xml flash application load first verify allowed flash requires file located root http server e.g http //replaced.url accomplished using virtualhost url rewrite options couchdb offers described earlier nicolas orr blitz.io neat app describes follows tried works beautifully edit domain.com zone add record blitz.domain.com create blitz.txt file put content blitz told put upload blitz.txt attachment design document get content blitz.txt attachment ok test couchdb hit root couchdb reports version however put rewrite couchdb complains many security issue fair enough 0x thus design doc looks like instead blitz course need figure hostname couchdb database would like access e.g flash.example.org make sure hostname actually resolves machine access i.e runs couchdb restart couchdb afterwards think even necessary add entry futon use couchapp way like create design document called _design/flash contains crossdomain.xml file attachment rewrite statements much like mentioned curl 'http //replaced.url provide crossdomain.xml added attachment second rewrite points root database question allowing get documents database http //yourserver:0/flash/doc0 accessible http //replaced.url setup allow flash application connect couchdb without problems hope helps. curl http //replaced.url couchdb welcome version congratulations violence last refuge incompetent. hi sam android apps created internal hobby projects connect couchdb via replication preload database remote instance documents add local instance replicated back remote instance use basic add authentication hardcoded considering use accountmanager make couchdb listen interfaces mount sdcard edit local.ini would normal pc would rely apps though since expect people installing app. hi looking amount replies wrt topic seems much interest full text searching really hard tell would expect feature implemented couchdb way would supersede nice couchdb-lucene combo said _really simple_ probably bad solution performance wise search implementation look couchdb lists decide _view sent _list function within _list function implement full text search inspecting document data setup least allows replication functionality might enough. hi 0cts although seem see nails everywhere couchdb hammer better storing xml xml database like sedna allows versioning documents xpath searches documents sent mobile""",391,"65,17","39,39","55,24","9,72","0,26","0,26","0,26","0,00","0,00","0,00","0,00","0,00","0,00","9,46","2,05","2,05","5,37","2,05","2,30","0,77","0,77","0,51","5,12","0,77","0,00","6,14","0,00","0,00","0,26","6,91","5,88","1,02","0,00","0,26","0,00","16,37","2,81","6,14","3,07","2,05","0,51","1,02","1,02","0,77","2,30","1,53","0,51","0,26","0,00","0,00","0,00","0,00","0,00","9,21","4,09","2,30","2,56","4,86","4,35","0,26","5,37","0,26","0,00","0,00","0,26","0,00","1,02","17,39","7,67","0,00","0,26","0,00","0,00","0,00","0,51","0,51","0,51","0,00","7,93"
"""178f713589e948dbd7b2f430525f28805f89a3fd393d8f0f76882a0dc432f5d5""","""hi barrie nice hear back guys indeed jira account last time tried able assign issues could somebody please look working version maven-indexer dropping plexus polishing examples re-working certain bits api believe tamas still vacation surely explain thanks. oh right seems broken recent merge believe 0d0ed0af0eff000ab000ae0c00d00f000e00bf0a safe revision rollback try things look get back. thanks accepting merging olivier. hi kenneth version maven trying build building fine maven kind regards. hi writing concerning bug misleading maven locks around quite preventing us able use distributed build system properly use hudson although possibility run build private repository reasonable solution large number builds due amount space need node hitting lot lately hudson running nodes using different operating systems maven problem described follows build snapshot module install locally work days commit code meantime changes module however maven-metadata-local.xml contains line tells maven update snapshots even -u using older snapshots locally solution remove artifact 's local metadata file edit manually comes regular dependencies written small plugin works around issues locked maven-metadata-local.xml files info however comes parents situation much trickier projects get loaded interpolated long would like ask proposed patch accepted/reviewed confirmed bug exists maven proposed patch david rousselie applicable maven copy five lines changesto defaultrepositorymetadatamanager.java another 0.x release could included could bug patch reviewed writing workaround plugins good idea using modified maven company therefore would like request issue fixed next version maven quite critical thanks advance looking forward replies regards. hi would downsides like bind process need download remote repository 's index least usually small recall correctly maven central's index something like mb .gz talking top head recent work maven-indexer downloading index central order run tests took would people start saing oh maven slow build tool resolve million dependencies even slower switch some-next-cool-build-tool-goes-here referring maven slow lot people feeling takes ages build larger projects due resolution dependencies need update local index regular basis day least quite convinced would accepted well everyone could option dependencies could resolved mixed basis index use/update option specified ignore option specified saying current model best trying illustrate downsides switch kind regards. hi created account long time ago probably early late sure need re-register something. okay registered xcircles username seemed work. questions stackoverflow.com relating maven-indexer created tag re-tagged appropriately. hi furthermore feeling adding option artifact bound lead headaches hand could specify groupid includes/excludes repositories things could much better terms resolution times frankly think remote repository 's job first place people otherwise two different places handled making things complicated track later artifact routing rules exist. hi herve username carlspring documentation site needs serious work lack thereof developer unfamiliar indexer must sources try figure deprecated currently tricky business unless willing invest lot time pray using right thing site documentation non-existant created separate module examples currently based tamas examples github great starting point need extended examples much illustrate bits pieces scattered across different sites gather things place make useful possible would like open issues jira chase guys filed issues order polish requirements close obsolete ones schedule relevant ones know need extra permissions gotten latest indexer work spring need using project would like add examples needs done order able fully drop plexus replace wagon code proper started working went vacation pressing work soon get chance look well kind regards. could always clone repositories fixes locally submit pull requests seems problem clearly questions lists right place ask. works fine thanks herve. hi would like become maven committer lately contributing fixes maven-indexer current area interest working together tamas cservenak deeply familiar writing maven plugins large part core plugins see pulls currently working things maven-indexer well contribute knowledge free time project written quite plugins hosted github familiar contribution process pulls work look project already read stephen 's blog becoming committer senior build release engineer agree approach believe fixes live high standard would privilege able join team kind regards time yearly committer school announcement become maven committer http //replaced.url committer.html let us know. hi thinking whatever animal owl would really cool wearing sort super-hero outfit letter maven brought super-powers order proposal""",700,"46,67","36,71","63,14","12,71","2,00","0,29","0,00","0,29","0,00","0,00","0,00","1,71","0,00","12,29","3,43","1,86","7,43","1,57","2,86","0,57","0,71","0,00","4,14","0,71","0,00","5,86","0,14","0,00","0,71","8,14","6,86","1,14","0,00","0,43","0,00","20,71","5,14","3,43","4,43","3,86","2,14","0,86","1,00","0,86","1,86","0,86","0,29","0,71","0,57","0,29","0,29","0,00","0,00","17,14","3,00","5,57","8,43","5,14","6,00","1,00","0,00","0,71","0,14","0,00","0,86","0,43","0,14","8,00","3,14","0,00","0,00","0,00","0,00","0,00","3,14","0,29","0,71","0,00","0,71"
"""ba513f1638042a57f5f9c8cd0c995142ba498abd14ddab3ae6ae0e6b2dfc6fee""","""p.s volunteer help kip sent protonmail http //replaced.url encrypted email based switzerland""",14,"14,00","35,71","35,71","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,14","0,00","7,14","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","21,43","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,14","0,00","7,14","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","14,29","14,29","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","42,86","14,29","0,00","0,00","0,00","0,00","0,00","0,00","14,29","0,00","0,00","14,29"
"""b851c6f2472149ec2d0802860acf21ea1087b80e19565dd8a0567ba75429e6c0""","""hi current implementation maven-eclipse-plugin 's to-maven given following two bundles according maven conventions create two artifacts groupid artifactid version using classifier source second bundle instead creates two completely unrelated artifacts creates problems using -ddownloadsources=true trying ago attached patch could please shed light wheter reasoning correct thanks. hi martin well aware maven default build.finalname unfortunately look eclipse packages source artifacts corner case example snippet list bundles plugins directory eclipse problem maven-eclipse-plugin 's to-maven intended serve default tool create maven artifact eclipse correctly recognizing bundles source suffix source artifact corresponding java artifact instead creating new java artifacts contain compiled java classes java sources assumption ask eclipse guys completely revamp builds account maven 's build.finalname defaults would possible fix maven-eclipse-plugin generate correct maven provides possibile fix would possible apply thanks""",138,"69,00","48,55","47,83","10,87","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,25","2,17","0,72","4,35","1,45","3,62","1,45","0,00","0,00","1,45","2,90","0,00","3,62","0,00","0,00","0,72","9,42","7,25","2,17","0,00","0,00","0,00","24,64","3,62","10,87","3,62","2,90","4,35","0,72","0,00","0,00","1,45","0,72","0,00","0,00","0,00","0,00","0,00","0,00","0,00","3,62","0,72","0,72","2,17","1,45","4,35","0,00","0,00","0,72","0,00","0,00","0,00","0,72","0,00","13,04","2,17","0,00","0,00","0,00","0,00","0,00","6,52","1,45","2,17","0,00","0,72"
"""0f1757469d1c549990a866d08ed0b74dd6f447db8d5aa88eb06ed24ef95413af""","""please send mail. website looks great""",6,"3,00","16,67","83,33","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","16,67","0,00","0,00","16,67","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","33,33","0,00","0,00","0,00","33,33","33,33","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","16,67","16,67","0,00","0,00","0,00","0,00","0,00","0,00","0,00","16,67","16,67","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","50,00","16,67","0,00","0,00","0,00","0,00","0,00","0,00","33,33","0,00","0,00","0,00"
"""cbbc1cfe5bddc6a69e0c3ba62cd14e45b039d4ecd166998f460cd9ca82369338""","""newbie solr done everything solr tutorial section using latest versions jdk solr see solr admin page http //replaced.url hit search button receive http error invalid value 'explicit echoparams parameter use 'explicit 'all' tried run solr tomcat unsuccessful solutions document links appreciated thanks help""",44,"44,00","34,09","56,82","9,09","2,27","0,00","0,00","0,00","0,00","0,00","0,00","2,27","0,00","13,64","2,27","4,55","9,09","0,00","0,00","0,00","0,00","0,00","9,09","0,00","0,00","6,82","0,00","0,00","0,00","11,36","6,82","4,55","0,00","2,27","2,27","20,45","6,82","6,82","0,00","0,00","9,09","0,00","0,00","0,00","2,27","2,27","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,36","6,82","2,27","2,27","2,27","6,82","0,00","0,00","2,27","0,00","0,00","0,00","0,00","0,00","20,45","2,27","0,00","0,00","0,00","0,00","0,00","0,00","4,55","9,09","0,00","4,55"
"""ee79066e5e393cd954df45326b189c7d93967ef5152edd52354d057256b8eb66""","""hi hilmi tez jars required typically configured though tez.lib.uris config property thanks regards kuhu missing tez jars likely missing custom setup please follow instructions setup client hadoop environment http //replaced.url""",33,"33,00","30,30","39,39","3,03","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","6,06","0,00","0,00","6,06","0,00","3,03","0,00","3,03","0,00","0,00","0,00","0,00","3,03","0,00","0,00","0,00","12,12","6,06","6,06","0,00","0,00","6,06","9,09","0,00","0,00","0,00","6,06","0,00","3,03","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","9,09","6,06","3,03","0,00","6,06","3,03","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","21,21","9,09","0,00","0,00","0,00","0,00","0,00","0,00","6,06","0,00","0,00","6,06"
"""af1c54dfaff10f453ee8a71ab9a5e42d8a7daa76351d7ae06b906e249a2637b6""","""new api allows cloudstack copy deltas two snapshots addition cloudstack longer needed plugin scripts plugin scripts mainly try backport code prior even plain vanilla work api cloudstack source code refer xenserver combination xenserver0.00 indicate significance change""",37,"37,00","37,84","37,84","5,41","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","5,41","0,00","2,70","2,70","0,00","2,70","0,00","0,00","0,00","0,00","5,41","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","18,92","2,70","8,11","2,70","2,70","0,00","0,00","2,70","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","10,81","2,70","0,00","8,11","2,70","5,41","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","5,41","0,00","0,00","0,00","0,00","0,00","0,00","0,00","5,41","0,00","0,00","0,00"
"""2d42c4ecc5cfc3ffed8d308fce4b7673b599cefabe78b400cb31223353b3875f""","""hi remove lock file solr//data/index. set proper permissions tomcat. hi use wild cards autocompletion lucene far better tools making good autocompletion since wild card term query passed configured query time analyzer comments use porter stemmer use german specific stem index time tokenizer defined possible behaviour undefined far know. hello majisha codepoints content field however stripping method built invalid middle byte exception mind seen even solr 0.x upgrading parts infrastructure solr 0.x got struck confirm content field sent nutch causes. n't elect leader. either upgrade tika manually use pdfbox. well somewhat problem url 's uniquekey contain exclamation marks idea allow escaped thus ignored compositeidrouter. hi far know never good idea run lucene openjdk either oracle java higher openjdk. pkill work""",123,"13,67","29,27","45,53","6,50","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,32","0,00","2,44","4,88","0,00","2,44","0,81","0,81","0,81","3,25","1,63","0,00","4,88","0,00","0,00","0,00","4,88","3,25","1,63","0,00","0,00","0,00","21,95","5,69","8,13","0,81","1,63","1,63","1,63","0,00","1,63","0,81","0,81","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,01","4,88","4,07","3,25","1,63","2,44","0,81","0,00","0,00","0,00","0,00","0,00","0,81","0,00","13,82","8,13","0,00","0,00","0,00","0,00","0,00","0,00","1,63","1,63","0,00","2,44"
"""45845ce8346c2adfd741afb3553eaabad2c155fe4153e168f3ebb4d99f47f1a4""","""hi alex think line wrong use implementation ilog wrapper implementation checks implementation sets locationinfo onto position point _log instance code logging change ilogger log messages like copied code logimpl.cs loggerwrapperimpl.cs damm good peace code fun reading. hi yes mamanged kick live following problem app-domain definied powershell workingdir powershell-snapin path located path way specify location configfile let know still need see lines code. hi someone else might application would like send even though application may send fatal different logger first post makes sense configure logger 'invoicing later configure logger different properties access 'two loggers code logmanger.getlogger invoicing please first version two different loggers app using appender set level logger perhaps define another root logger invoicing configure global settings invoicing loggers every single fun. hi shouldnt becourse created instance logger stored repository aplication ends use loggername get instance problem use logmanager repositorie's getlogger member create instances wich result multiple instances logger need implement iloggerfactory interface create custom subclassed logger configure hierarchy use factory use logmanager repository create loggers instance created. see example projects copied example remember perhaps find usefull. research checked log0net sources error thrown appenderskelleton base appenders m_closed m_closed set reads code docu programming error append closed appender perhaps really bug log0net appenders post simple possible config reproduces error system try reproduce debug log0net see whats going wrong appender. perhaps simply use threadcontext store additional infos could store additional infos like. see point building sources coding log0net sources simply use log0net projects way add reference release like debug log0net.dll included inside zip thats. know set properties depending appender use declaration follows generic description given docu reading docu know discover possibilies appender provides knowing follow docu setting properties config given example tag tag set top level config declare custom levels extensions provide said found docu maybe tags best thing would xml-schema config reads 'issue tracking config follow strict schema searching complete plain list xml tags config like etc. hallo able reproduce problem config runs without errors system time study log0net sources tell totaly way configure loggers reason repository stores logers hashtable logger used key table impossible store two loggers happens xmlconfig like loops root xml elements every logger element calls getlogger get logger wich create new logger return allready created exists level set/overriden appenderrefs erased recreated logger allways configured like last definition configfile profe simple hard say fileappender crashes using target file processes perhaps log0net able open filehandle runs write closed appender. see internal log0net logging see. message local queue service host processes time beeing bit specific service independed application holds references logging application 's dlls words using generic service remote applications customized version every single kind objects log queue strings perhaps xml real bussiness message objects service hold logic getting msgs queue without looking inside forwarding ado far see thing win kind async logging price pay extra level design service app depends additional tech msmq think non-blocking app ok msgs logged possible wrong order long timestamp shows real date simply build depender following decorator pattern new appender holds property referencing real appender ado-appender case every time appender asked logg new message creates new thread starts leave inside new thread recieved msg passed real appender less lines code whole appender every logginrequest aync creating new thread even faster writing msmq plus add additional logic inside decorator wait writing db long x msgs recieved using threadpool store msgs long network unavailable whatever use apps becourse totaly independend rest app would put inside independed course appender reused appenders ado simply adds asynfunctionality every appender choose. hi basic log0net things named logger using loggername allways work instance logger configured named logger need repository stores reference configured thought workflow like configure logger use custom logfile bad bad softwaredesign yes would affect threads use logger use two explicit loggers every location messages calculation lets say following locations location0 used logger could new logger inherits level appender message calculation like configured configfile every message goes child loggers logfile location see simple custom logmanager providing new members getmessagelogger getcalculationlogger location parameter return configured logger ilog precise log0net.ilog. hallo perhaps provide hints get complete list log0net studied great docu examples googling found important tag listed docu ok least beside checking source parser anywhere full list available looking something special know possibilities. well first code filters without toughing log0net code simple add fqdn config simple prevent log recieve specified level use buildin filter levelmatchfilter configfile possible know filter log0net defining custom loglevels easy without toughing code see trace example provided sources. thx pointing deliver messages appender fatals delivered. problem use every message needs checked matchs filter perhaps set filterlevel inside definition would speed things""",776,"45,65","41,49","52,96","11,21","1,29","0,13","0,00","0,13","0,00","0,00","0,00","1,16","0,00","9,79","1,68","1,29","7,09","1,03","2,58","1,68","0,26","0,64","4,38","0,90","0,00","4,12","0,00","0,00","0,26","5,15","3,48","1,68","0,00","0,00","0,00","20,23","3,22","6,19","2,19","2,71","2,19","0,77","2,06","0,77","2,06","1,42","0,39","0,26","0,00","0,00","0,00","0,00","0,00","13,02","2,58","4,12","4,77","2,71","2,96","0,77","0,00","1,16","0,00","0,26","0,52","0,13","1,03","4,77","2,71","0,00","0,00","0,00","0,00","0,00","0,64","0,26","0,64","0,00","0,52"
"""b13fd8f0636336d4fe72610e907dd976f348a4f2a632d66b0ce15063e2391ff9""","""hello everyone hope email finds well hope everyone excited apachecon would like remind couple important dates well ask assistance spreading word please use social media platform get word visibility better apachecon link main site found http //replaced.url planning attend apachecon north america may add-on option registration form join conference discounted fee us available apache big data north america attendees please tweet away look forward seeing vancouver groovy day""",70,"70,00","32,86","72,86","17,14","4,29","1,43","0,00","1,43","0,00","0,00","0,00","2,86","0,00","12,86","2,86","1,43","8,57","2,86","2,86","2,86","0,00","0,00","4,29","0,00","0,00","12,86","0,00","0,00","0,00","14,29","14,29","0,00","0,00","0,00","0,00","17,14","4,29","1,43","4,29","5,71","0,00","0,00","1,43","0,00","2,86","2,86","0,00","0,00","0,00","0,00","0,00","0,00","0,00","17,14","4,29","11,43","5,71","4,29","4,29","0,00","0,00","2,86","0,00","0,00","0,00","2,86","0,00","8,57","1,43","0,00","0,00","0,00","0,00","0,00","1,43","2,86","0,00","0,00","2,86"
"""f8edcfb93545238bb180cd8e2deec494f9b41dc5bc16de388bc5e0a6a542cc71""","""hi first sorry form bad english new user apache solr read documentation check find solution problem need words order sensitivity query example two something two something three something four four something three something two something run query query two result something two something three something four two like query four something three something two something need result disturb returned query two result four something three something two something two like query something two something three something four need result disturb returned possible yes regards""",84,"84,00","35,71","91,67","52,38","21,43","0,00","0,00","0,00","0,00","0,00","0,00","21,43","0,00","3,57","0,00","0,00","3,57","0,00","0,00","0,00","0,00","0,00","2,38","28,57","0,00","2,38","0,00","0,00","0,00","5,95","0,00","5,95","2,38","0,00","0,00","42,86","9,52","5,95","4,76","22,62","0,00","0,00","0,00","21,43","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","7,14","1,19","0,00","4,76","1,19","2,38","1,19","0,00","1,19","0,00","0,00","1,19","0,00","2,38","2,38","0,00","0,00","0,00","0,00","0,00","0,00","0,00","2,38","0,00","0,00","0,00"
"""8d813ba09858cc9de6303df077cde873630baa77238f6ea9e7a073808a56522a""","""read various pages used curl lot figure correct command line add document example solr instance tried things however seem file server solr case pushing document windows machine solr indexing. 's complete example. clearer rich text document xml make work example docs folder read solr book tried samples could make work. read solr book book way read need help mean time using example solr system send word document using curl system full path document tried various commands gives stream errors documents server solr second server would like experiment home indexing documents server giving experience outside b trying grasp issues design read books struggling grasp ideas yet using .net connector recommended webpage connects solr adds extra information solr query client see later returns data normal design using shopping trolley design orders created comments added order processed search order information many indexing requirements index many database tables proposing use solr instance indexes data use example text field copydata extra fields tables solr book gave short shift database indexing tool good tutorial using solr sqlserver many tables thinking might join lot might easier create xml output send pros cons way restricting searches client signed good idea secured behind application adding extras thinking would index order notes separately order alternative value note field order keep deleting best way keep replacing whole order 'join notes search like feature suppose solr index normal map url code using return data solr hope help""",235,"58,75","28,51","55,32","11,91","0,43","0,00","0,00","0,00","0,00","0,00","0,00","0,43","0,00","12,34","2,13","2,13","8,09","1,70","0,85","0,85","0,43","0,00","7,23","0,43","0,00","3,83","0,00","0,00","0,00","5,53","5,11","0,43","0,43","0,00","0,00","20,85","5,11","5,96","2,98","2,98","0,85","2,13","0,43","0,00","1,28","0,43","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,49","1,70","2,13","3,40","6,38","4,26","4,68","0,85","1,28","0,00","0,00","0,00","0,00","0,43","3,40","1,70","0,00","0,00","0,00","0,00","0,00","0,00","0,85","0,85","0,00","0,00"
"""999076a19d33ea80f5d6380fe64bf5094611a510bd64fd29ef8ff26fb467c42e""","""set apache reverse proxy couchdb problems replicating idea setup apache later handle authentication however even disabled get following failure trying replicate apache runs port forwarding couchdb error get error part database actual document occurs seems vary unable reproduce behaviour smaller database well mb already tried dumping reloading database contents nothing changed proxy configuration follows hope fairly standard proxyrequests allowencodedslashes normal requests couchdb futon interface seem working fine proxy ideas problem could thanks""",72,"72,00","48,61","59,72","9,72","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,89","1,39","2,78","9,72","0,00","4,17","0,00","1,39","1,39","2,78","0,00","0,00","1,39","0,00","0,00","0,00","11,11","5,56","5,56","0,00","1,39","1,39","20,83","5,56","2,78","6,94","6,94","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,89","8,33","2,78","4,17","2,78","6,94","0,00","5,56","0,00","0,00","0,00","0,00","1,39","0,00","2,78","0,00","0,00","0,00","0,00","0,00","0,00","0,00","2,78","0,00","0,00","0,00"
"""35d689bead6b31c10cbf7e8bd33da99ac9e6dd77ca7675f3226cb4a4faa88f56""","""ok shall try answer would correct give data need indent=on f.createddate.facet.date.start=now/days-0months f.createddate.facet.date.end=now. hello quite new solr trying get grip currently reading enjoying solr enterprise search server book trying failing need advise following given following schema subscriptions would like group accountid facet isclosed accountid addition would like see number created subscriptions since given date would possible figure grouping something like advise would highly appreciated regards. yes see question bit confusing thanks answers try clarify bit query date field validtodate value field present documents would like get number documents given date range r0 value validtodate i.e documents number documents given date range r0 value validtodate question really possible query need two queries facet.range.other=all help way. hello following faceting parameters gives unwanted non-null dates result set way query index give non-null dates return i.e would like get result set contains non-nulls validtodate faceting non-null values validtodate would like get non-null values faceting result response example gives results non-null validtodates would like get results non-null validtodate facets write start wonder possible facets dependent result set might better handle application layer extracting help would appreciated""",205,"51,25","31,71","68,78","12,68","2,44","0,98","0,98","0,00","0,00","0,00","0,00","1,46","0,00","17,56","5,85","1,95","9,76","5,85","1,46","0,98","0,00","0,00","2,44","0,49","0,00","7,32","0,00","0,00","0,00","12,20","9,76","2,44","0,49","0,00","0,49","25,37","6,34","5,37","6,83","5,37","0,98","0,49","0,49","1,46","1,46","0,98","0,00","0,49","1,46","0,00","1,46","0,00","0,00","13,17","1,46","0,49","11,71","2,44","4,39","0,98","0,00","2,44","0,00","0,00","0,98","0,00","0,00","14,63","7,32","0,00","0,00","0,00","0,00","0,00","3,90","0,98","0,00","0,00","2,44"
"""e1e776cd2f7efc4f5b2ac35594d0c991db9b41bceceb2e089500d784af1c2c65""","""hi using solr store time series data log events etc right use solr cloud collection cleaning deleting documents via queries would like know approaches people using way create collection receiving post inexistent inded could use date part index cleanup process would delete old collections""",44,"44,00","29,55","61,36","11,36","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","18,18","6,82","0,00","11,36","4,55","0,00","0,00","0,00","0,00","6,82","0,00","0,00","6,82","0,00","0,00","2,27","4,55","4,55","0,00","0,00","0,00","0,00","22,73","4,55","11,36","6,82","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","18,18","4,55","2,27","9,09","0,00","2,27","0,00","4,55","2,27","0,00","0,00","0,00","0,00","0,00","4,55","0,00","0,00","0,00","0,00","0,00","0,00","0,00","4,55","0,00","0,00","0,00"
"""94fb5e656357db4b2bbc7f9ce3f56241293b797ffc31e5c3c0773e5ac3e33c07""","""trying use archiva proxy maven central instance configured following configured local maven installation use repository see lot checksum validation failed issues warning could validate integrity download maybe use see archiva secure tls go directly maven central issues tried different combinations like maven central group using maven central group makes insane slow still validation errors manually download respective sha0 hash file see expected hash see maven suddenly think value start anyone ever seen anyone help kind regards""",76,"76,00","36,84","48,68","9,21","2,63","0,00","0,00","0,00","0,00","0,00","0,00","2,63","0,00","19,74","1,32","2,63","15,79","0,00","3,95","0,00","0,00","0,00","1,32","0,00","0,00","6,58","0,00","0,00","0,00","5,26","3,95","1,32","0,00","0,00","1,32","19,74","1,32","6,58","2,63","5,26","2,63","1,32","0,00","0,00","6,58","6,58","0,00","0,00","0,00","0,00","0,00","0,00","0,00","11,84","2,63","1,32","7,89","1,32","3,95","0,00","0,00","1,32","0,00","0,00","0,00","0,00","1,32","2,63","0,00","0,00","0,00","0,00","0,00","0,00","0,00","2,63","0,00","0,00","0,00"
"""f2a3b32a84d8e62507db6d4ca3c640fc3d6c267dfd5a9eb829266482f859f9fd""","""hi still problem projectbuilder followed api change projectbuildingrequest see following setup gist problematic code works version beta-0 gets npe version beta-0 later hints welcome better regards kristian. plugin use maven0 api maven-beta-0 gives maven-rc0 gives maybe used projectbuilder maybe part public api maybe writing plugin maven0 api never mind fix side regards kristian""",58,"29,00","25,86","48,28","10,34","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","12,07","0,00","3,45","8,62","0,00","6,90","0,00","0,00","1,72","5,17","5,17","0,00","10,34","0,00","0,00","0,00","6,90","3,45","3,45","0,00","0,00","0,00","18,97","0,00","5,17","3,45","5,17","1,72","0,00","0,00","0,00","1,72","1,72","0,00","0,00","0,00","0,00","0,00","0,00","0,00","12,07","5,17","1,72","5,17","1,72","3,45","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,79","1,72","0,00","0,00","0,00","0,00","0,00","8,62","3,45","0,00","0,00","0,00"
"""b25f329d7482d6ec4cbac4b7b82804b156f42ac3601340af82dce72c0b989f25""","""hi henri make sure container running solr set utf-0 example tomcat server.xml file connector definitions include join conversation like us facebook follow us twitter. hi relatively special case parent child relationship trying model currently using solr lucene example parent documents represent information e.g bibliographic information parent document contain children child representing physical copy book information e.g think library multiple branches child documents represent books format available given branch think special set information need solr lucene make use facet based need facet counts child facets example harry potter last crusade j.k. rowlings upcoming block novel following information etc etc bit simplified actually fields involved child document five fields used individually easy combination much harder refine result set relatively straightforward approach location format ordinary everyday facet fields certainly get results although ignore facet counts search book without facets applied get back facets like narrow things work though logically hole trying fill example suppose user chooses narrow location branch format dvd still get hit back child record values user looking dvd 's branch library dvd main completely controlling indexing searching side code i.e formulate document content indexed parse results presenting users approach thinking brute force method accomplishing using facets using facet.prefix parameter query could generate 'facets like narrowing single facet e.g location branch would usual facet search something like would request back facets like parse values returned location-format_facet retrieve follows branch- prefix would facet values 'format facet presented users 'book remains value fields pretty straightforward could somewhat simplified two facet fields instead four keeping paired facets using singleton facets retrieving paired fields limiting taking place parsing pairs location format facets limiting element use facet.prefix limiting choose facets look concatenated value however gets complex ramp fields generally requires individual facet fields number underlying fields i.e two fields two facet fields needed solr/lucene index support three fields could facets required fields would required facets getting bit much hmmm little scribbling ok fair bit scribbling actually reduce facet fields cover fields maybe bad interesting actually coded anything yet paper-napkin level exercise thing done perused various archived threads upcoming functionality regarding parent child hierarchical document strategies found anything would help much least directly saw jira lucene-0 nested document query support looks slides overview structurally would indicates query parser support place yet i.e solr query able relate child level queries either within base query clause question finally logical problem seem resolvable approach brute force outlined willing dive solr lucene code would like indication people think would good possible approach get level e.g way providing indexer tuple found combination values something searching facet queries thanks. hi sorry duplication seems like sent yesterday never made troubles solr spellcheck response running version overview search something really ugly like get back response suggestions list 'rck suggestions list two words 'book fine 'spelled correctly i.e got hits word suggestions ugly thing though hits problem handling result tell difference suggestions 'correctly spelled term suggestions something odd like happening searches obviously garbage i.e words real words show index suggestions illustrate point setup running multiple shards may part issue example 'book might found shards another think anything schema since really search suggestions returned us bits pieces would really like see response coming back indication word found suggestions hacked around code little bit wondering anyone across approaches taken created new classes extend indexbasedspellchecker spellcheckcomponent follows package imports excluded sort brevity methods taken overridden classes changes noted sd comments modification allows correctly spelled words returned suggestion modification working tandem sirsidynixspellcheckcomponent allows words suggestions returned spell check component even sharded search changes marked sd comments modification designed may work tandem sirsidynixindexbasedspellchecker return mispelled words suggestions flipped false suggestions index true thesuggestions null //sd removed thesuggestions.size shardrequest allow misspelled words suggestions thesuggestions.size always true hence removal continue //if shardrequest word mispelled add list mispelled words else extendedresults suggestions.size word misspelled added suggestions freqinfo 's xml getting back search applying modified code thanks. hmmm maybe understanding getting jonathan say 'there good way solr run query across multiple solr indexes 'shards parameter allows searching across multiple cores instance shards across multiple instances certainly implications like relevance consistent across cores shards works pretty well us thanks. hi bill others post worth specialized resolution wrote similar requirement may help similar requirements running wanted able return facets matched actual search rather facets entire result set example user searches author 'twain present list facets match 'twain exclude facets 'twain found tell users 'facet values present alpha-sorted list author names count associated documents search author search field identify matching documents get facets i.e normal solr processing point filter facet set include match original search added extra facet parameter facet.sirsidynix.filter.facets instruct solr special facet filtering modified simplefacets method gettermcounts right final return counts like added method 'filtercounts basically wrapping things run search facet value setting instances based schema inserting facet value running original query anything matches score ones keep filters counts entries match original query using lucene 's fast in-memory single document index queries run string value count create run original query anything score means 'hit value matches original query retain score means hit i.e facet value associated document matched query facet value match query param field field facet values came original search would initial_author_srch_boost well string time shove single-values initial_xxx fields good enough query able correctly bit explanation schema order suffixed facet fields _facet hence first statement matching 'searchable 'facet fields names basically differ suffix strip '_facet append '_boost '_fuzzy two field types searching possibly applying boosts fuzzy matching shall see exactly hopefully modify version match schema basically idea derive field original search issued facet field shall read see works rather re-iterating anything date faceting ranges anything facet prefix handling may may work need prefixes anything else facets handle least test say special case us way intended general solution fit 'prime time submission solr enhancement. looked parameter. oops sorry hijacking thread put real subject place. hi anyone ever set things figured good way access solr filter chain java code specifically would like tokenize data search strings possibly even facet values way solr filter chains additional pre-search processing e.g tokenizing way produce better fuzzy search query post-search processing find matches within facet values thanks advance. oops sorry missed well multivalued setting explicitly allow multiple values actual use case i.e multiple values field multivalued problem trying solve. hi hoping someone thoughts running solr patch solrqueryparser.java getluceneversion calls use lucenematchversion directly running solr cores schema use solrj run searches java app running jboss jboss tomcat solr index folders server case relevant using jmeter load test harness running solaris processor box 00gb physical memory run successful load test user load rate solr searches second solr search responses coming 000ms tried ramp far tell solr hanging logging statements around solrj calls log long query construction takes run solrj query log search times getting number query construction logs corresponding search time logs tomcat jboss processes show well cpu still top processes cpu states show around idle usage two java processes around 0gb lwp state shows sleep jboss still 'alive get piece software talks jboss app get data set things use log0j logging solr log showing errors exceptions indexing searching back january load testing prototype problems though solr time ramped beautifully bottle necks apps solr benchmarking descendent prototyping bit complex searches fields schema basic search logic far solrj usage ideas else look ringing bells send details anyone wants specifics. hi nizan realize replying thread email client would get back 's info thread since replication replicate top-level solr.xml file defines available cores dynamic cores requirement custom code wasted. good answer may depend wanting restrict 000k documents seeking reduce time spent solr determining doc count wanting prevent people moving far result set case display digits return count solr performing adequately could always artificially restrict result set solr actually 'return 0m documents returns number specified query well cache next results anticipation subsequent query total count returned exceeds 000k report 000k number results similarly restrict far user page results sounds like sort results descending post date fact get recent ones coming back first. setting reading thread seems could search 'macman hit 'macman 'maecman since seems could map single replacement mapped multiple times generating multiple tokens thanks view message context http //replaced.url sent solr user mailing list archive nabble.com. dumb question time using bit java bit java. hi downloaded file unzipped see inside file tried unzipping errors look inside file see java code example ended unzipped .java extension example path apache-solr-0.0\lucene\backwards\src\test\org\apache\lucene\analysis\tokenattributes see two files ideas specific tool using expand windows xp thanks join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. well could magnitude notation approach depends complex strings based examples would work identify series integers string assumes lengths series insert number integers string integer series sorting would sort string00 string00 string000 use original strings displays. possibility consider really need documents specifically empty non-defined values oxymoron control values send indexing could set special value means 'no value done similar vein using something like empty given field meaning original document actually value field i.e something unlikely 'real value easily select documents querying field empty instead negated form select however considered things like index relatively rare us value 'gut feel impacting indexes much size-wise performance-wise. well sort depends mean 'previous 'next record sequencing built concept solr lucene indexes sequential 's i.e use case data available support use case. second query clause instead otherwise mixing apples themes_raw oranges themes. thing think post-process snippets i.e pull highlighting tags strings look match result description field looking match find replace description original highlight text i.e highlight tags still place join conversation may even get ipad nook like us facebook follow us twitter. set new field concatenation 'field 'category group facet many combinations would talking field run query something bit similar wanted 'author search 'author field documents field set based 'author search well field based 'author faceting search author field return results facet values display facet values counts users select issue new query return documents author facet value. hi mark used dih shall need leave comments set others done another question initial index create delta run 'commit run 'optimize without optimize 'deleted records still show query results. hmmm maybe need define mean 'server mean 'client view message context http //replaced.url sent solr user mailing list archive nabble.com. unfortunately wild card search terms get processed analyzers suggestion fairly common make sure lower case wild card search terms issuing query. omri need indicate solr at_location field accept multiple values add field declaration see reference information options. using unique solr index sounds like may value solr index unique bears resemblance unique derived data another way put makes two records solr index 'the unique 's two entries solr index 's related original data unique immutable i.e update row database unique derived row would update otherwise nothing solr recognize duplicate entry 'delete 'insert instead 'insert. use replication call inexperience really early working fully understanding solr best way approach various issues mention prototype non-production code covered though look replication feature thanks. hoping sort work multivalued field normally trying makes sense example two authors document would expect document sort 's smith jones probably specific rule choose least generic sense wanted example able sort first author could index first author separate non-multivalued field purely sort still authors multivalued field join conversation like us facebook follow us twitter. guarantee 'best algorithm 's use final static helper methods set characters strings solr treats special meaning query corresponding escaped versions note actual operators show escape characters wherever occur escapes special characters search terms get confused. hi sorry 'spam testing posts actually seen sent queries past couple weeks single response anyways two would respond would appreciate let know ignored vs unseen thanks join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. suppose something silly like fact indexing chain includes 'words= stopwords.txt query chain register february get time viewing three course circulation basics self-paced training suite. reading wrote originally wrote think misunderstanding based architecture code code 'server level solrj indexing calls meant 'server solr instance mean 'client thinking without thinking 'server sorry hopefully someone else chime specific view message context http //replaced.url sent solr user mailing list archive nabble.com. without speaking directly indexing searching specific fields certainly possible retrieve xml file solr db allow binary field associated index document store gzipped xml file binary field retrieve certain conditions get original document information found solr handle much faster db regularly index large portion documents xml files prone frequent changes keep blob solr index make sure retrieve field really xml files relatively static i.e change rarely new ones still might make sense use real db store keep primary key db row solr index join conversation like us facebook follow us twitter. use solrj dynamically created core capability know advance cores require almost always complete index build previous instance index needs available complete index build two cores index switch required indexing run 's summary 're early prototype implementation right production quality code tell voluminous methods identify core exists create method instantiates two solrserver objects solr indexcore requires create two cores indexname already exist live core used searching second core used indexing indexing two switched just-indexed core become live core way core works live core always named indexname indexing core always named indexname _index datadir core alternate indexname indexname create core already exists returns true new core created false otherwise solrj provides direct method check core exists getstatus index clearing first complete rebuild various logic submit batches could adjust core index complete reindexes need index always searchable hope helps. ok figured solr really surprised prototype benchmarks used different instance tomcat using production load tests prototype tomcat instance maxthreads value set using default value production tomcat environment maxthreads value running threads getting connection refused exceptions thrown ramped solr hits past certain level thanks considering yonik others waiting see reply made others said listserv great. put default value group_id field solr schema would work e.g something like 'unknown shall get original group_id value still grouped together figure display time. hi working upgrading troubles unit test code custom filters wrote tests extend abstractsolrtestcase reading thread test-harness elements present distributables checked branch code built ant generate-maven-artifacts found lucene-test-framework-0-xxx.jar however contain lucene level framework elements none solr solr test framework actually get built embedded solr jars somewhere way build jar contains solr portion test harnesses thanks. could add standard field shard populate distinct value shard facet field look facet counts value corresponds shard hey-presto done. make field multi-valued field indexing time split text sentences putting sentence solr document values mv field think normal highlighting code used pull entire value i.e sentence matching mv instance within document i.e put 'overhead index step rather trying search time. hi michael well stock answer 'it depends' example would able search filename without searching file contents would always search together copy file parsed file content pdf single search field set default search field kind processing normalizing data case insensitive accent insensitive 'word contains camel case e.g theveryidea split case changes watch things like ipad 'word contains numbers left together separated stemming searching 'stemming would find 'stem 'stemmed sort thing always english languages involved text processing indexing vs searching able find hits based first characters term ngrams able highlight text segments search terms found probably read various tokenizers filters available prototyping see looks basically 'one fits part power solr lucene configurability achieve results business case calls part drawback solr lucene especially new folks configurability achieve results business case calls anyone got anything else suggest michael. looks like specified value pdfy reflected results query query searching vpn hence matches query yield. hi indexed various types documents fields author already able use facet choose values narrow given use case runs something like example list authors matching 'steel count number documents associated user chooses entries gets document results author present 'sideways essentially present facets first results choose facet show results facets show match fashion based analysis chain based fuzzy search search term entered know get list author facet values documents 'steel matches author field problem author multi-valued field returns facet values match 'steel values author field really ugly approach work time hoping someone better idea read facet parameters searched various places across anything like use facet.prefix looking facets begin search term looking facets contain search term could show anywhere fuzzy handling may exact matches anyways masochists know approach search something like searching documents author_boost field internal 'author field search term mark twain proximity distance somewhat arbitrary return hits really kludgy bits hoping enough hits hits would include mark twain however specify fl fields return field never exists getting back empty elements xml highlighting author_boost field tells us value search terms found sort document kind random sort try get many distinct highlighting results possible i.e score sequencing would cluster highlight values post processing build set strings highlighting results removing highlight elements intent becomes set 'mark twain strings chug facet_field list author_facet preserve entry set strings built highlighting results present result back users along counts facet really ugly usually work help visualize 's excerpts response join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. need include unique field 'fl parameter needed anyways match highlight fragments result docs highlighting working join conversation may even get ipad nook like us facebook follow us twitter. actual query look hl.snippets parameter join conversation may even get ipad nook like us facebook follow us twitter. makes next i.e still answered question 'next 'previous really means already know next page another query get next record""",3023,"65,72","33,91","53,82","13,23","3,11","1,42","0,66","0,76","0,00","0,00","0,00","1,69","0,03","9,06","2,18","1,79","5,29","1,49","3,27","0,86","0,43","0,36","3,57","1,12","0,03","5,69","0,17","0,00","0,46","6,52","5,16","1,32","0,03","0,30","0,20","20,15","5,19","4,33","2,45","3,80","1,75","1,32","0,56","1,39","1,59","1,09","0,23","0,26","0,46","0,10","0,36","0,00","0,00","11,21","2,94","2,32","4,86","2,61","2,84","1,29","0,03","1,12","0,03","0,03","0,17","0,40","0,73","8,70","3,47","0,00","0,00","0,00","0,00","0,00","1,06","0,07","2,81","0,00","1,29"
"""f9624337619765efae4f99a7c5ab852f2712306cb84c17cf189f4aa7afb8bad9""","""hi went apparently normal build install using instructions http //replaced.url get finally trying http //replaced.url get plenty searching looking information couch.ini anything useful mochiweb looked couchdb wiki troubleshooting forth near tell complaint made something somewhere host request header undefined sure know. nice thought trying suggestions inspired jan brad commenting ipv0 hosts /etc/hosts monkeying hostname along way near tell solved problem tests test suite passed sake reference actual build/install complaints debian make ebin directory lib/common_test may manually created something along lines anyhow problem actual location erl_rx_driver.so start playing earnest couch 0.0a000000 well whatever latest version etch let people know coming along""",111,"55,50","35,14","66,67","15,32","4,50","0,00","0,00","0,00","0,00","0,00","0,00","4,50","0,00","11,71","1,80","3,60","6,31","0,90","2,70","4,50","0,90","0,00","2,70","0,90","0,00","8,11","0,00","0,00","0,90","10,81","6,31","4,50","0,00","0,00","0,00","23,42","6,31","4,50","2,70","6,31","0,90","0,00","2,70","1,80","1,80","1,80","0,00","0,00","0,90","0,00","0,90","0,00","0,00","11,71","4,50","2,70","3,60","5,41","4,50","0,90","2,70","0,00","0,90","0,00","0,00","0,90","0,00","16,22","4,50","0,00","0,00","0,00","0,00","0,00","0,00","1,80","0,00","0,00","9,91"
"""3d7a0f2243e5c6521e64c397be8cb6dc90793503a65a8e13b1c9ba6abcfa57a9""","""trick learned marketing look say product reverse adjectives ask would competition say things product answer drop terms endorsement product definition vacuous empty stick facts whit cleaner clear fluff might include metrics project health releases committers pmc additions etc info board would really find useful fluff-free shall weigh sure. aapche oodt use soley test ensure imap handling works""",58,"29,00","32,76","65,52","12,07","1,72","0,00","0,00","0,00","0,00","0,00","0,00","1,72","0,00","12,07","6,90","0,00","5,17","6,90","1,72","0,00","0,00","0,00","1,72","0,00","0,00","5,17","0,00","0,00","0,00","8,62","5,17","3,45","0,00","1,72","1,72","31,03","6,90","6,90","3,45","1,72","6,90","0,00","3,45","1,72","5,17","1,72","3,45","0,00","3,45","0,00","1,72","0,00","1,72","3,45","0,00","1,72","0,00","13,79","8,62","0,00","1,72","1,72","0,00","0,00","0,00","0,00","0,00","6,90","1,72","0,00","0,00","0,00","0,00","0,00","1,72","3,45","0,00","0,00","0,00"
"""4b889711d92465ada1f34e529edb774180412496e385322b9b33f40ec907edbd""","""hi potential likely extension couchdb contract working need win00 work done would rather even time similar couchdbx rather specialized weeks effort based experience similar thing osx preliminary enquiry see anyone suitably capable available interested order get client approval would need completed march rfn would fine currently deploy couchdb platform-specific ruby gem binary libraries required erlang icu spidermonkey etc binaries nginx packaged individual gems requirements managed using gem dependencies package ruby knows extend command line environment include binary resources path/env vars/libpath etc given command line environment starting couch case couch ruby allows configure multiple instances run/manage ruby binaries package location-independent done munging rakefile builds gem means package application isolated machine environment relative given baseline done osx cool although flaws always terminating process concerned getting running system deployed windows need done windows nginx could either must configurable managed via ruby wrapper must native code compilation required gem install e.g cygwin required would conflict existing cygwin installation use private gem repository reason needs work windows xp vista corresponding server versions simple osx wrapper application provides server coupled merb-based app present purely html interface similar osx finder itunes part contract manages updating gems compaction using xmpp talk presence server etc none contract need gui portion installer done windows xp vista although must require additional downloads e.g .net must use shared ruby install gui manages/starts/stops/log admin app points embedded safari mozilla automatically selected port nothing else fails start app must raise explanatory dialog obviously functions handled admin app must fully this-is-just-a-windows-app e.g box help link pretty trivial clear tradeoffs embedding gecko windows app rather ie concerned tracking ie version different client installs maybe issue part really nothing couch would person parts single contract copyright would vest linkuistics company bsd license credit author linkuistics including admin although gui wrapper may required hence finished stage gauging interest/possibility open suggestions/ quotes/offers/enquiries done non-local subbing would need way verifying competence presuming list reaches couchdb-aware talent""",338,"338,00","42,60","52,96","14,20","0,89","0,00","0,00","0,00","0,00","0,00","0,00","0,89","0,30","12,13","6,51","2,66","5,03","4,14","2,07","0,00","0,89","0,89","4,14","0,00","0,00","2,96","0,00","0,00","0,59","3,85","2,37","0,89","0,00","0,00","0,59","20,41","2,96","2,96","6,80","1,78","3,25","2,37","1,18","1,78","0,89","0,30","0,00","0,30","0,30","0,00","0,30","0,00","0,00","7,99","0,59","2,07","4,14","6,51","4,44","0,30","3,85","0,59","0,00","0,00","0,30","0,00","0,00","7,69","1,18","0,00","0,00","0,00","0,00","0,00","2,96","0,59","0,00","0,00","2,96"
"""5d25b0725f67b2ca1a4ec8d521523ff5cf9d08ae453584bfbd6acd705fb1a197""","""hi ahmed egypt spent last two months developing custom web-interface solr using html called solr search actually basic idea inspired ajax solr solr search provides different approach terms usability available options users know might interesting information already better search interfaces send email hope sharing solr search community know exact steps share kindly please guide appropriate please find attached quick overview document solr search. hi hi furkan ahmet thanks reply last email solr search proposal sent last sunday 0-mar-0 announce solr search simple html interface searching documents indexed apache solr tm actually developed last two months spare time small html interface solr far complete mature project features options might found useful users solr glad share code hosted http //replaced.url page quick overview link solr search found well link downloading thanks best regards ahmed replaced email.addr.es""",140,"70,00","27,14","47,14","7,14","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","9,29","1,43","3,57","4,29","1,43","3,57","0,00","0,00","0,00","2,14","2,86","0,00","10,00","0,00","0,00","0,00","11,43","11,43","0,00","0,00","0,00","0,00","11,43","7,14","0,71","0,71","2,14","1,43","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","0,00","13,57","4,29","1,43","7,86","0,71","1,43","0,00","0,00","0,71","0,00","0,00","0,00","0,71","0,00","7,86","2,86","0,00","0,00","0,00","0,00","0,00","2,14","1,43","0,00","0,00","1,43"
"""574ca75cae0d6cee843ca843d83fba69638211430d175ff62ef30fd1c06dfc89""","""time ago something similar testing another question upload files properly use refer upload office files know debian could please url server. 've visited right url works right uploading files files uploaded users drop whiteboard. hello alvaro works flawlessly server starts image/office doc upload nothing common. debian. hello hermann using different start/stop http //replaced.url necessary tags. ok. files uploaded users deleted save space server mobile sorry typos. hello maxim obvious red0 maybe right tested upload jpg file avatar conference room upload .ppt office file extensions""",87,"10,88","24,14","50,57","5,75","1,15","0,00","0,00","0,00","0,00","0,00","0,00","1,15","0,00","6,90","1,15","0,00","5,75","0,00","1,15","0,00","0,00","1,15","1,15","0,00","0,00","5,75","0,00","0,00","0,00","5,75","4,60","1,15","0,00","0,00","0,00","13,79","3,45","2,30","1,15","3,45","2,30","2,30","0,00","1,15","2,30","2,30","0,00","0,00","0,00","0,00","0,00","0,00","0,00","16,09","2,30","5,75","6,90","9,20","2,30","0,00","1,15","0,00","0,00","0,00","1,15","0,00","0,00","18,39","10,34","0,00","0,00","0,00","0,00","0,00","0,00","2,30","1,15","0,00","4,60"