"""861fd45532e720b19456ed7a5ae138ed94d62927fa268077f96c42b8f0c4d5b2""","""offer couchwiki project named gorsvet.urlyss tiny url-shortener service written flask using dare may become dares. hi geert-jan yup http//replaced.url modifications patch could work thanks info. run similar issue mix webapps non-solr guys use log0j rotating daily appender since supported default jdk logger mismatch log files produced would looked mention wrapping log0j logmanager big deal bit annoyance search reusable implementation find anything looked good since use resin resin support describing approach taking short-term embedded jetty usage shall back situation ryan arguing use apache commons logging have enough fun nutch commenting discussion. might look nutch handles issue nutch stopwords wants keep around generates combo terms like the- index query parser thing query phrase common terms wind searching across much smaller slice comes course expense larger index lot unique terms due combo terms big win example site http//replaced.url index source files without optimization searches could several seconds got 000ms lots breathing room. hi checking seems indexed fields specified sorting purposes query string least seeing date field correct miss info wiki someplace case would handy standardrequesthandler return error invalid request sorting non-indexed field requested thanks. hi regexreplaceprocessorfactory line means use match groups replacement string reasoning behind missing something groups used making hard write simple solution training exercise students need clean incorrectly formatted dates thanks. hi creating combo field searching straight-forward way boost contribution fields used create combined field would read past threads seem anything built solr simple hack copy field multiple times e.g schema field-to-boost effective boost 0x since highlighting display multi-value field seems work ok long course level boosting 0x 0x etc acceptable something continue work future issues approach run yet thanks. got version index appears open luke reports problem grins crafted matching schema tried use index solr solr-trunk either case get exception startup startup logging says trying something destined fail would expected schema/index miss-matches show later right index opened would seen various posts error due corrupt indexes buggy version java obscure lucene none seem apply situation thanks. thing bit previously using apis area solr call corecontainer.getcore increments open count balance call close call naming could better think common expectation calls get something change state maybe. hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor isdatadir gets used construction changing coredescriptor isdatadir effect since would go changing datadir core multi- thanks. hi way explicitly add record using solr add fail record already exists value unique field specified schema isuniquekey field course pre-flight query see record unique exists multi-client environment solr multiple users reliable thanks. exact version resin still confirm uncommented lines. requirement add index folder solr data directory generating lucene index mapreduce program later would like merge index solr index without bringing solr actually tried index merger tool tool works solr possibility merge segments solve problem. would good see cms configs send java. hello refactored hdfs directory implementation solr use project surprised see performed i'm using hdfsdirectory local machine using cache significant speed small enough file making lucene index docs fit block inside cache running multinode cluster aws performance pulling back docs cache much better without according log statements cache hit every time difference local several blocks setting cache used default setting specified ideas speed searches change block something put wrapper around cache number documents directory number documents directory thanks. told use request param module escape param default. move strfield since using synonym expansion anything. similar issue sometime back unable make multi-word ngram match work workaround require field replaced space characters underscore indexing searching replace spaces underscore field passing search suspect elegant solution using proper tokenizer filters could figure simple hack works use-case. solr bin/solr see it's still quite 're looking uploading arbitrary xml scripts browser security issue it's possible never api call best. similar issue started showing yesterday suspecting related azure update sure still. hi queried solr retrieve information database integrate web page dont know implement please help actually page search field search button need get results solr page pls help. still could replicate issue branch i.e queries going server numshards=0 distributed among servers creating cpu spikes servers cloud think behavior expected fixed release. i'm currently performing optimise operation ~000gb index million documents process running hours surprising machine ec0 r0.xlarge four cores 00gb ram 00gb allocated jvm load average steady memory usage less whole time iostat reports util gives. hi i'm new solr i've reads works cadon't find clue specific situation case i've database tables need add index related entry table clients could entry table contacts visual example think case simple search must present information customer question setup schema.xml way account two database tables best regards. hi ngrams http//replaced.url. spent fair bit time yesterday making 0.x compatible patch completed work yet it's bit work dependency build system may able get back weeks yet surroundqparserplugin really need make work need get compilation bit fixed things changed 0.x trunk contrib/modules rahul i'd like see done feel free stab i'll tinker time. hi all.for security reasons hide result dataimport command specifically section initargs order hide connection parameters database removed tag datasource moved requesthandler defined dataimport operation allows hide user password database url app everytime invoke /dataimport command=yyy output xml displaying information 00dih-tenant0-jdbc.xmlorg.postgresql.driverjdbc postgresql //localhost/db0udb0pdb0tenant0statusidlethis response format experimental likely change future wonder way remove completely initarrg section output xml way mask information i'm working restricted environment don't information shown xml output thanks help adolfo. perhaps help make sense advantages i've read article http//replaced.url don't understand well i'll try figure missing part thanks helping understand docvalues feature allow sorting faceting without blow heap necessary faster traditional method memory efficient huge indexes main limitation http//replaced.url thanks reply thought docvalues feature may summarized docvalues feature speed sorting docvalues useful sorting faceting example don't need change nothing xml it's thing need set docvalues=true field definition don't use default implementation loaded need add tag solr.schemacodecfactory docvaluesformat=true fieldtype i'm fully understand kind usage example docvalues. yes works many times result expected guide use regex solr. hi would like know designing index approach better large number documents million values document value field less number documents values document value right approach-0 around million documents group queries really perform slow freedom reduce number docs group multiple documents put identifier value field always search multivalue field. feature flag index operation provide create sematics would cool setting create-semantics flag index operation fail document simular whatever use uniquekey already exist flag set index semantics today except call optype possible values create index index default alternatives solr cloud provide well need current project might make outside solr/lucene hope able convince productowner make solr-feature contributing back especiallly solr community agrees would nice commonly usable feature believe commonly usable feature especially using solr nosql data store search index yes know might consider datarecord inserted solr indexed hard-commit happened many threads working indexing datarecords solr deleting/acknowledging source datarecords next hard-commit happend index believe another issue plans dealing thanks everybody. hello normal many instances solrindexsearchers open time heap analysis shows case autocommit every minutes opensearcher=true would close old searcher create new create new old still getting dereferenced older searchers get cleaned thanks help. jan issue hitting docker /proc/version getting underlying kernel would expect docker container errors update-rc.d service docker using trimmed. hello looking example solr distribution given expected behaviour somebody adds documents without committing without extra commits ever see even committed disk something going reopen searcher eventually soft commit enabled would reopen searcher schedule even hard commit example regards linkedin http//replaced.url time quality nature keeps events happening lately don't seem working anonymous via gtd book. probably need configure tfidfsimilarity schema rebuild index otherwise norm seems unuseful. removed tokens position wiki explains gabrielle would expect two tokens query two tokens would it'set. thing use csshx http//replaced.url mac dealing various boxes cluster issue commands terminal duplicated windows useful global stop/starts updates. i've mentioned i'm new solr i'm java guy rather large schema fields plus large number i've trying improve performance finally got around implementing fastvectorhighlighting gave us immediate improvement qtime nearly improved overall response time bring back extraordinarly large amount data xml results records back payload 0mb even 00mb lot report text used searching solr test allowed us leave report text return decreased payload significantly nearly large i'd expect performance boost however seeing greatly increased response times builds solr even though qtime put perspective original solr core believe test boxes running running older version wildcard field list returns payload approximately 00mb average seconds new version machines older version network traffic/latency/hardware/etc it's returning say reloaded core briefly saw 0.0mb payload back milliseconds within minutes back seconds noticed cpu pegged seconds running queries new build wildcard field list lower scale box running version success getting ms response time reduced payload probably minutes thing bumped seconds response time anyone experience random yet consistent performance degradation insight might causing issues fix i'd love performance boost fast vector highlighting decreased payload thanks advance. i've passed score parameter rejected requests pretty ready give documentation around function queries params working though i've using solr ten years figured lot systems impenetrable. expect happening solr caches effectively making two tests identical using memory hold vital parts code cases disk warming instance using local disk suspect measured first queries assuming auto-warming i'd see running tests curiosity running /dev/shm something 're considering production best. thanks confirming worked using lt instead would help note wiki found confusing dataquery examples used. don't issue optimize command either solrj client issues client.optimize command pressed optimize admin ui solr don't best. querying test solr field it's finding occurences seems substring word like supertestplan don't found unless use wildcards test write tokenizer someone know way around don't add wildcards messes queries multiple. search swap file distro tons info"""
"""1cc23fe0afdfd7abeb62bba9ce9deacde4fc680cf034895076d3c0eae42c8320""","""anyone please help following sure done something wrong downloaded zip http//replaced.url warning problems encountered building effective model org.apache.parquet parquet-scala_0.00 jar:0.0-snapshot warning problems encountered building effective model org.apache.parquet parquet-scrooge_0.00 jar:0.0-snapshot warning highly recommended fix problems threaten stability build warning reason future maven versions might longer support building malformed projects error see full stack trace errors re-run maven -e switch error information errors possible solutions please read following articles error correcting problems resume build command. ok tried http//replaced.url seems build fine. written additional parquet-tools helper access files hadoop environment along lines existing parquet-schema etc scripts provided part parquet-tools distribution designed live location scripts hopefully others find useful determine path isdirectory else. attaching patch put together decimal includes added fixed bytes try make generic based branch avro logical support avro classes missing would need copy parquet fix schema getlogicaltype accessor need use instead think pretty straight-forward questions let know. sorry delay getting back investigating angles using tableau drill create views drill access parquet files even though parquet files hold schema create views cast columns correct data bigint float timestamp etc updates suggested parquet-avro add support timestamp urgent originally thought since cast everything anyway. thanks sounds need go trial local copies code would good check delayed contact http//replaced.url discuss. hi think pushing solr space probably going non-starter solr isfocus index management serving side things different multitude issues faced looking good free would try ibm isuses lucene hood fact wonder could reverse index create solr schema. could use lengthfilterfactory restrict terms least character long believe patternreplacefilterfactory creates patternreplacefilter says. providing maven0 build would useful us currently jar in-house maven repo works clean would favor changing directory layout selfish reasons would favor changing build rely maven0 use maven internally krugle sometimes works well times royal pain butt option would nice handy personally would hate foist maven everybody else. looked briefly passing analyzer use morelikethis fields lot analyzer used given filters play performance really stunk use stored term vectors would nice yes thanks. hi list anybody know suggester component designed work shards asking documentation implies since suggester reuses much spellcheckcomponent infrastructure spellcheckcomponent documented supporting distributed setup make request get exception looking querycomponent.java:0 code see assuming docs variable null would happen response element solr response make direct request request handler core e.g http//hostname:0/solr/core0/select qt=suggest-core q=rad query works see element named response unlike regular query wondering configuration borked work fact suggester return response field means work shards thanks. hi got situation key result initial search request let issay list values faceted top faceted field values need get top hit target request restricted value currently total requests requests following initial query made parallel still questions magic query handle solr as-is best solution create request handler case developing custom thanks. hi particularly uwe robert yes gist question specify use simpletextxxx e.g simpletextstoredfieldsformat solr currently possible thanks. grabbed latest greatest trunk make main.css .query-box height tall enough least mac 0/ff config character descenders get bumped fixed issue solr looks like contains anchor text missing constraints see .constraints css added main.css ianawd probably best way fix issue see css used intent set character gray seems silly open jira issues types things add noise list approach preferred thanks. hi marcus look new project called katta first code check-in happening weekend would wait monday look. clarify issue using actual user search traffic seo content expose example people commonly search java hint url static content page language part generating static pages based search traffic though might decide content favor see based popularity yes need automate content generation regular e.g weekly basis big challenges ran dealing badly behaved bots would hammer site wound putting content separate system would impact users main system generating regular report user agent ip address could block robots.txt ip necessary figuring structure static content look like spam many links page much depth constrains many pages reasonably expose project scores based code activity usage used rank content focus exposing early low depth good stuff could based popularity search logs anyway lot topic feel solr specific apologies reducing signal-to-noise ratio. thanks fast response beginning worry nobody read posts see http//replaced.url attached test code issue plus simple fix json case regards. hi part interesting work creating custom query parser writing unit tests exercised extendeddismaxqparser first created extendeddismaxqparserplugin used create qparser via query something like complexphrase query expecting complex phrase query parser get used ishappening local param treated regular text makes think conceptual model local params processing wrong ishigher level code pre-processing step first hoping d get disjunctionmaxqueries queries complexphrasequery would mean processing happen inside extendeddismaxqparser code pointers poke around thanks. hi upgrading solr noticed filtercache hit ratio dropped significantly previously enabled recording entries debugging looking seems edismax faceting creating entries sharded setup distributed search search string bogus text using edismax two fields get entry shard isfilter caches looks like expected similar situation happening faceted search even though fields single-value/untokenized strings using enum facet method shall get many many entries filtercache facet values look like item_ net result even big filtercache 0k hit ratio still thanks insights. must missing something isschema exactly original example schema look index using latest luke would need rebuild luke current lucene code since format version tried using solr/admin/analysis see query returning get server error numerictokenstream support chartermattribute looks like happy analysis triefloatfield searches index using solr/admin results single value e.g ideas might going thanks. hi yonik thanks fast response ok curious made bit confusing api well-formed xml xml parser happy reserved xml characters need escaped using org.apache.commons.lang.stringescapeutils seems work pretty well great advice let give try thanks. hi quick note caveat using slightly older version solr without fragile sense relies current working directory resin home directory b path must ./webapps/ anyway worth solrsevlet.java isinit method change beginning see instance resin specifying explicit location configuration data note relative original source least version parameter solr.configdir lower-case build deploy solr resin called solr-notes example is/webapps/solr-notes/web-inf/web.xml a. uncomment first three system-property lines noted comment work around bug resin b. add init-param section copy contents solr config directory /webapps/solr-notes/conf part edit specify location data directory repeat steps second different. answering question use corecontainer.reload core force assume got embeddedsolrserver running time everything happen correctly covers need find programmatically change settings using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks"""
"""2558c44d40a4e4abe2b563f51229232dc151c6ff22a82b24fb53a6c28a9fcb31""","""hi think line dependencyinformationreport.java needs fixed replacing could someone update please let know need additional information kind regards. hi documentation wikipediatokenizer specifically wondering pieces source xml get mapped field names solr schema example seems going date field example schema got goes body way get example thanks. using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks. would parse html extract text first use index data nutch tika projects examples using html. hi ryan dug client code natural inclination would go latter fits better would write test use test request object. mitch right looking recommendation engine understand question properly yes mahout work though taste recommendation engine supports pretty new owen robin anil mahout action book early release via manning lots assuming list recommendations given user based past behavior recommendation engine could use adjust search results waiting jump best handle. ran minor problem clicked facet tried search would get error think problem fqs velocity macro vm_global_library.vm missing else insert url macro fqs p foreach p velocitycount else without url becomes /solr/browsefq=xxxx instead /solr/ completely new world velocity templating got low confidence right way fix. hi lurker interesting thread would suggest talking solr committers experience merging lucene got many similarities discussing though solr mature happened seen externally ultimately win though without lot teething issues many seemed personality conflicts groups versus technical/admin/operational issues additional comment inline ps counter example fine separate project solr releases eventually came faster lucene release cycle improved slow-down everyone expecting certainly reduced continuous painful debates belonged solr vs. lucene. believe code support handling form data sent put request logic make sure either unspecified application/x-www-form-urlencoded read body byte array use request convert string request unspecified assume us-ascii typically specified e.g use curl tool post data sent header convert key/value pairs using urldecoder.deccode string utf-0 since key/value pairs url-encoded believe standard assume us-ascii utf-0 would work well us-ascii viewed sub-set utf-0. two cases think cases two characters variant forms unicode tried unify still exist gb tons wanted support phonetic pinyin zhuyin search might collapse syllables commonly confused course would storing phonetic forms words. hi especially yonik http//replaced.url page mentions duplicate field collapsing later allow duplicate see mention deduplication happens search time normally requires field stored indexed efficiency might need fieldcache wondering status support thoughts potential thanks. hi james general immediate updating index continuous stream new content fast search results work opposition searcher isvarious caches getting continuously flushed avoid stale content easily kill performance issue interesting topics discussed lucene bof meeting apachecon alone wanting ways clear hard problem relax need immediate updates index accept level lag time receiving new content showing index would suggest splitting two processes backend system deals updates. code appears try maybe code date see /solr/conf used path code see created cwd/data data default specified work tried number different permutations maybe missed magic combination grepped source see pattern solr.solr.home used anywhere able control location config file using set location /data directory still gets created saw though seem use either see solr.datadir used code anywhere trying control command line seem work could uncomment edit tag file worked makes worried missing something wrong sources tell ways control location data directory used index a. change cwd sure inside resin b. edit tag file lets change directory ways control location config directory lets change directory thanks. got situation data directory needs live elsewhere besides inside solr home b moves different location updating indexes setting symlink /data great option best approach making work solrj low- level solution seems create solrcore instance specify data directory use update corecontainer wrong would like avoid mucking around low-level solrcore approaches thanks. hi robert -xx heapdumppath= something look versus gedankenexperiment. moving solr-dev solr-user quick impression given scope described page feels like boil ocean problem spent afternoon looking could use solr code getting much love somehow leveraging solr would seem like win three attributes solr interesting context complex query processing caching though critical things live noticed described issues automatic doc- server mapping calc stable hash quick exploration use hadoop rpc talk guts solr assumes query processing happens search server level versus master currently nutch way request summaries document via subsequent post-merge call master immediate problem ran notion solr running inside container currently penetrates deep bowels code even core level calls made extract query parameters url step going try clean manner would define side/solr core api layer would relatively easy least first cut hooking solr core nutch master via hadoop prc. hi michael build meant gui whereby user use special characters say quoting collection clauses programmatically build query without using query parser code wound write seemed like simple escaping quickly got complex convoluted e.g allow term get processed specially query parser ok case sounds like stuck escaping. hi ken given comments seemed describe using nrt opposite use case set solr use solr.mmapdirectoryfactory bother test whether nrt would better use case mostly sound like advantage focused things relating solr would love hear results someone testing batch indexing use case tested various xxxdirectoryfactory implementations please let know results testing. hi would thought queries would allow thing supported currently standard query would recommend re-posting lucene user list. general notice http//replaced.url uses solr search project data store index user-generated notes code finally went live last week openly post thanks solr community useful tool great. yes came exactly conclusion could try use ram jvm load index ramdirectory mmapdirectory wound slower configuration smaller jvm relied os-level cache make index accesses really fast trick heard /dev/null force index data. hi got fields contain embedded xml two questions relating appears though shall need xml-escape field data otherwise solr complains find start tag embedded tags finds tag field expected constraint xml-escaping data best way handle kind related question would easiest way ignore xml tag data indexing types xml-containing fields seems like could define new field e.g set associated tokenizer something new create though would un-escape data ick parsing skip tags thanks. hi uses solr generate terms mailing list text analysis extract good features things like classification similarity clustering last part cover using solr implement real-time similarity engine maybe recommendation engine well undoubtedly things unclear even incorrect please comment regards. ah yes would explain oddity saw order hits matching document boost since querying field omit norms really short. hi uses character separate elements e.g state|city solr 0.x worked fine query gets distributed across shards get solrexception severe org.apache.solr.common.solrexception use fieldcache field neither indexed doc values state problem appears fl= state|city parameter getting split functionqparser tries use state field actually exists ignored field since q=state|city ca| find entries california known issue way disable parsing field names field list thanks. actually tika htmlparser wraps tagsoup good option"""
"""637258698cb8be15b9d856019f6325312df39b6028e1a87e846b26d2dd4dfbc8""","""hi list requirement producers wait indefinitely kafka broker comes case maintenance network interruptions disk caching front producer producer waits drains producer eventually reconnects functionality working fine kafka producer settings used acks=all would like acknowledge gwen shapiras slideshare http//replaced.url setting derived. hi ian public terabyte dataset project would good match need course means actually finish crawl finalize avro format use data free collections data around though none know target top-ranked pages. would great facing issue rolling code keep solr index sync mysql db access via hibernate thanks. hi sorry missed reply thanks trying find similar report wondering file jira issue might get attention. sounds like hitting max url length 0k common default http web server using run solr web servers know let bump limit via configuration settings. think ran would like run two copies solr inside resin since two sets searchable data distinct schemas/usage patterns specify location configuration directory least using either solr.configdir solr.solr.home subscribed dev list shall post question timing fix versus something quick needs fwiw quick fix pass directory location init thanks. response header and/or meta tags page see code used nutch could missing could wrong either/both issue determining right arbitrary web page easy way analysis advance know sure always x going simplify things soon possible means right processing page collation settings db change db interprets data change data. ok thanks confirming absolute easiest approach would support new init value codecfactory schemacodecfactory would use select different base codec use versus always using lucenecodec would switch everything different codec could extend schemacodecfactory support additional per-field settings stored fields format etc beyond currently available quick dirty hack specified different codecfactory factory hard-codes simpletextcodec works files simpletextxxx format segments.gen segments_xx files pluggable. hi using simpletextcodec past noticed something odd running solr enable posting format via something like resulting index expected text output noticed files standard binary format e.g .fdt field data based simpletextcodec.java assuming would get simpletextstoredfieldsformat stored data holds true files e.g http//replaced.url adding simple text format docvalues walk code figure hoping need change configuration setting thanks. right looking snapinstaller seemed though deletion index directory would deleting files indexreader thanks. url work please include full stack trace runtimeexception version tika using thanks. worked around way. could background gc depending got jvm configured use though stay long. another trick described andrzej another field days write token e.g number times matches age document starting reasonable base date add +days:0 query lucene winds boosting results recent. hi assuming use first characters specific field grouping results thing possible out-of-the-box would next best option e.g custom function query thanks. hi olivier setting mime explicitly via stream.type parameter. auto-generated unique value would like. included bit information configure top-level sharding request handler use suggest component get npe get response results completeness pieces puzzle suggest-one suggest-one suggest-two thanks. hi geert-jan thanks ref good stuff think close understand correctly could get using top two versus top simplicity results looked like actual faceted field value/hit counts would top hit facet field followed facet field used field collapsing would improve probability asked top hits would find entries top faceted thanks. seemed work thanks suggestion though using case anybody else reads shall need run tests check performance improvements hadoop workflows output always fresh unless interesting helicopter stunts yes default index always rebuilt scratch thus long primary key used reduce-phase key easy ensure uniqueness index thanks. finally got around looking short field values returned java.lang.short xmlwriter.writeval textresponsewriter.writeval missing check val instanceof short thus bit code used thing happens binary field since val case byte get b b anybody else run seems odd known issue wondering something odd schema especially true since binaryfield write methods xml json via textresponsewriter handle base00-encoding data wondering normally binaryfield.write methods would get used whether actual problem lies elsewhere. different schemas would combine results two schemas define third core different conf separate conf/solrschema.xml set request handler dispatches two real cores. noted lot different approaches could search email list archive discussion using solr subject solr support integration providing compass-like mechanism keeping solr index sync db via hibernateeventlistener. works excellent trying build distribution trunk use prototyping noticed things fresh check-out build trunk/solr sub-dir due dependencies lucene classes done top-level ant compile cd /solr ant builds noticed run-example target trunk/solr/build.xml description show ant -p. tried ant create-package trunk/solr got error near see contrib/velocity anywhere lucene trunk tree recommended way build solr distribution trunk meantime shall use example/start.jar solr.solr.home thanks. others noted currently updating field means deleting inserting entire document depending use field might able create another core/container field plus key field use join support note http//replaced.url improvement looks like 0.x code line though see fix version. hi robert confuses suggester says based spellchecker supposedly work shards issue got configuration shards already trying leverage auto-complete quick dirty work-around would add custom response handler wraps suggester returns results fields needs merge. problems knowing possible set case seems like minor tweak call solranalyzer though understand mark iscomment setanalyzer method says required using like docnum method call tell analyzer either default standardanalyzer whatever gets set explicitly still get used case term vector. actually tagsoup isreason existence clean html wild tika ishtml parser wraps uses generate stream sax events consumes turns normalized. note shall need ant compile top lucene directory first trying solr-specific builds inside /solr sub-dir least ran trying build solr dist recently. tested impact certainly would cause index files read thus cached depending much ram available came big commercial user lucene think running something like fc0. sounds like roysolr running embedded jetty launching solr using start.jar app container newrelic installed. would use copyfield copy contents new field uses string use faceting. sir christmas card list shall fire tomorrow morning let know goes"""
"""61631242ce3a1b35bb227cfd729b9570d47c8ee74a208766faa6fcc599faaeab""","""processing html definitely use something like nekohtml tagsoup note tika uses tagsoup makes easy special processing specific elements give content handler gets fed stream cleaned-up html elements. hi floyd typically would creating custom analyzer converts words pinyin zhuyin index would actual hanzi characters plus via copyfield phonetic version search would use dismax search fields higher weighting hanzi field segmentation error prone requires embedding specialized code typically license high quality results commercial vendor first cut approach would use current synonym support map hanzi possible pronunciations numerous open source datasets contain information note might performance issues huge set synonyms weighting phrase matches sufficiently high using dismax think could get reasonable results. noticed prices showing even though got think issue line hit.vm number.currency function needs get passed something looks like number return could list values square brackets confuse number.currency get price think line needs since returns single value without brackets. never tried sending anything utf-0 solr comment issues shall run based experience date would strongly suggest converting utf-0 post solr. already fronts solr web service us pretty easy something similar use case map user list groups using ldap make required clause solr request field contains. given requirement break document separately controlled pieces would create fronts solr handles conversion could think ways using solr feel like unnatural acts general comment acls relatively easy way handle via group ids use restrict query document groupid list group ids authorized access user query converted query groupid xx groupid yy xx/yy groups user belongs. hi navina thanks confirming putall list required method supporting changelog functionality hoping others confirm range _not_ used samza system i.e used internally needed tasks true adding notes methods required used samza system changelog support vs. optional used task-specific code needed would helpful thanks. think change token frequency would cause jump quantized levels order change md0 hash obviously happen point quantizing frequencies make much less likely using tech page crawl seems working pretty well though done in-depth analysis seen different proximity hash functions ones referring thanks. hi learned krugle approach worked us block bots search page expose target content via statically linked pages separately generated backing store optimized target search terms extracted search logs. docs need updated believe code wrote back note escape solr uses support using bug two tokens white- note idea still issue regular escaping work around bug parser last character escape build expression looks like xxx signal escape character wrong general escaping characters query gets tricky parser shall save pain suffering since code dismaxrequesthandler added solr iirc tries smart handling escaping. hi erik missing change needed thanks much. hi jack thanks included details original question issue got index 000m records 00m unique value prefix characters long adding another indexed field would significant hoping way via grouping/collapsing query time possible thanks. note nutch try solve concerns searchers would slow rmi faster hadoop rpc less issue well handle potential summarizer problems case example remote searcher goes away gets bogged times got hit server needs summary case ran load testing though would serious getting completely wrong summary remote index updated search request happened summary requested munge count might enough pretty simple part remember issues servlet-esque objects getting passed deep shall another look afternoon poking weeks ago thanks. hi emmanuel spot room think would issues would useful able leverage solr s. believe solritas supports autocompletion box wondering anybody experience using lucidworks distro leveraging autocomplete plugin hooking solr facets curious tricks traps getting work thanks. describing web mining web crawling extract price data web pages put specific field solr using nutch would need write custom plug-ins know extract price page add custom field crawl results topic nutch mailing list since solr downstream consumer whatever nutch provides. pros/cons using cache-control no-cache response header avoid problem tracked entire thread proposal would use response codes along xml body would client deal container response would typically html versus actual thanks. hi looking using embedded solr lets extract ranked results state publish part task isoperation methods defined problematic though specifically range methods return iterators iterating lots results solr feasible newer paging support still abuse architecture wondering whether need support methods called internally tasks e.g task thus optional assuming state automatically restored changelog samza system calling putall list repeatedly dug details would example required method thanks. hi looking snapinstaller seems following copy new index directory master slave issolr data directory giving index.tmp rename temp index directory index commit send post /solr/update service new index gets use feel like must missing something seems like request middle processed step successful swap could fail due index changing underneath insights thanks. use prng mix ids form auto-incrementing field yes. started using regular dom parser coding hand switched xstream help handling solr pojo mappings seems work fine get past encoding. experience resin definitely explicit character encoding. hi erik mentioned appears line standardtokenizerimpl.jflex needs updated include extension b character range. typically run memory solr index update new index searcher getting warmed looking heap often shows ways reduce memory requirements e.g shall see really big chunk used sorted field see http//replaced.url http//replaced.url. another approach add additional acl field contents would list customers ids access document query implicit acl actual query idea would work case use control access source code krugle enterprise product though using ldap-provided groups line mike issuggestion. fwiw krugle started using moinmoin switched confluence general good change us especially scale scope wiki activity increased integration jira handy far different classes pages easiest way would create spaces assume possible w/the confluence config apache would set space different quickly hacked app used convert moinmoin pages markup syntax confluence switch confluence happens look around. hi shawn different use case ones covered response robert wanted call currently use embedded server building indexes part hadoop workflow results get copied production analytics server daily basis writing multiple embedded servers reduce task gives us maximum performance proven reliable method daily rebuild pre-aggregations need analytics use case regards. intermediate approach use find potentially similar files using apply accurate slower comparison determine true similarity data common get files due small text changes frequency term moves quantized bands changes uber hash get combining terms bands still get matches hashes individual number matching fingerprint values. took look search results seems like word red shows description tag every sure extracting color information smart color-specific keywords found associated text. issue ran daily rolling log files maybe missed find functionality jdk logging package however log0j advocating change noting worked around leveraging resin issupport wrapping logger set daily. try uncommenting lines see fixes problem. code see seems using fsdirectory another layer wrapping going tom ever get useful results testing curious impact various xxxdirectoryfactory implementations batch indexing thanks. hi lucene index generated solr optimizing able view using limo download mac try use luke fails luke complains /index/_jdi0.f0 file directory files downloaded follows known issue luke solr-generated lucene indexes references found file directory involve index corruption far tell index fine thanks. hi got pattern document call xy turn two tokens xy approach could use patterntokenizer extract xy custom filter returns xy next call caches next result could extend patterntokenizer return multiple tokens match though figuring specify schema seems another approach would require custom code thanks"""
"""9a60aceacd1b17a0e6a9d5926fbc40ec9b2df5679a5c3c6c967c0d0fb8475e20""","""tried compiler much different lib wanted sure lib future. hi tried apply with common sense files many differences would incrementally would better easier apply small changes make mostly introduced help reorganize patches quickly agree improved need check clang-format versions available right new support something like //clang-format comment could check new opts available personally hate clang-format makes short yet believe consistent even perfect automatic tool better chaos regards. unnecessary mostly disrupts git blame mean new parts keep old ugly inconsistent really believe importance constant refactoring maintenance means iterations code naturally changed new coding standards mean naming convention example lines functions untestable fork split split split introduce low level unit tests changes way code looks yet pretty confident move good direction developer never afraid making changes existing code tests ensure nothing gon break tests provide tests needs changed software rot became unmaintainable way tests changed provide future project although like live regards. cmake generate vcxproj files cmakelists.txt sure requires something specific cmakelists part visual projects could removed repo generated example part release keeping repo really makes leats separate build systems maintain compiler cmake could think releasing thrift.exe using visual would remove dependency mingw. i0 byte creates inconsistency. require lua support try adding without-lua. got shall commit soon check impl compact protocol. hi going travis builds submitted couple pull req failed g++ internal compiler error stop pull reqsts merged left still shame marks requests failed anyone able reproduce issue seems happen mostly compilingtransporttest.cpp maybe machine runs memory compilation single file takes 0.0gb machine maybe split smaller compile units. submited patch maybe shall get accepted. would go aliases including short long i0 keep everything consistent best. dense protocol fails tests support messages compact protocol fixed awaiting results merge. seems files yet uploaded apache dist server status beeing released staged change web page wait files available best regards. somebody started need oneway calls half normal call pass-through server oneways really nice feature thrift would nice make work properly thrift already defines oneway additional message use. suspected something break submited pull req see travis say shall look hope explaining comments issue devs. favour keeping things simple possible add new flag current behaviour intuitive simple especially need ask someone add new assignees makes process complicated requires two persons get hold anyone able edit jira users leave assignee usually issue result loose information jira simplification suggested removal close transition hope commiters join discussion shall least try suggest update doc/commitets.md mention anything assignee. yes work vs easiest way use cmake generate vcxproj done automatically pretty sure changes properly reflected make build changes build system like adding new file done directly cmake files vcxproj regenerated still imho easier remembering apply changes build system least twice. may remember working c+0/cpp_v0 gen/lib http//replaced.url yet put hiatus time ago maybe next month shall find time finish. hi randy think could somehow merge cpp0 efforts old branch put little hiatus yet think parts could useful best regards. hey keep old c++ lib least c+0 lib would break iface compatibility avoid confusion point agreed use cpp_v0 new lib user would ask c+0 perfect solution still least solution current c++ lib renamed cpp_v0 gen cpp alias cpp_v0 branch probably little bit outdated not merged rebased contains full copy cpp_v0 lib std :shared_ptr std :thread instead boost counterparts boost usage reduced optional fields using boost :optional need check maybe compilers already support c+0 std :optional could switch c+0 probably released cpp_v0 become simplified threadmanager tested think last thing missing best regards. somebody check missed lang. shall debug shifts masks etc hopefully programming message everything ok. hoped could find anything thrift something apache wide seems projects searching apache standards saw like mention design etc coding put bracket importantly put new feature less text code devs used read code specifications ever saw specs thrift langs design part standards could extracted general shared top level docs rules like keep functions short apply almost langs issues lib public api comments lies. think easier store info jira reference jira issue commit additional info could jira especially simplifies providing info user list etc searching even change commit message personally like client part would prefer component still decide jira integrates workflow change information jira and/or commit message could leave original authors commits could make contributors happy authored commit apache repo. looks like :methodname compiler code look like :method_name definitely prefer first currently playing around compiler use lib assuming recent compiler code move toward look. assuming aliases would present parsing time prefer ix notation like explicit yet imagine lot users bits bytes int make idl look like java/c would try detect typedef i00 int constructs next step would detect keywords supported languages might good idea would require lot work especially adding new lang maybe reserved members names doable stig errors think problem appears generated code langs typedef constructs like c++ idl leads something like typedef int00_t int tries redefine. shall try merge/rebase master include recent changes c++ lib c++ v0 lib probably weekend hope shall find time c+0 quick answer point thrift would introduce std00 std00 etc long answer started branch discussion lead following thrift c++ library best modern c++ updated along new compilers new language library features etc library named cpp v0 mix current new versions c++ standard thrift able make library compatible current newest standard std00 std00 point library become library kept common subset c++ compilers features library enable thrift older outdated environments keeping multiple libs cpp00 cpp00 cpp00 etc would add big maintenance cost thrift best regards. sounds great although prefer scons-type tools everything flexible/portable autotools good assume cmake replace automake completely support two building systems like compiler right huge best. fixed hopefully generators shall peek/statsprocessor etc commits go branch. oh really hope langs standards like please http//replaced.url added tasks consistency especially going roger issuggestion lib/ lang/readme.md would nice consistent lib layout avoid mails dev like saw lang x guidelines keep standard ones regards. hi please check current master think problem reported solved http//replaced.url best. soo see two potential changes result discussion add dictionary reserved keywords validate variable names a. global dictionary containing merge keywords thrift supported langs make idl portable like level might little easier maintain yet might surprising users changing gen x lead thrift errors 0a 0b would expect error /warning like int used c++ java. updated generators langs unfortunately fill new seems oneway message never fully supported. today links working idea shall forward dev somebody higher permissions might needed. good idea ignore gcc failure. change way things encoded fixed decoding procedure protocol version change needed imho still old server old decoding procedures may support oneway methods sent new clients compact protocol"""
"""e1081946adf1d4efd0c90d6d8b1923924184bdabb0f7c60c34facbf404c24e98""","""thought yet pretty busy studying learning basics curly brackets keyboard hint curly brackets italian keyboards love idea open source pretty sure skill-wise community let us see project progresses next weeks regards. hi notice map function emits every letter author isinstance entry db yields following results key null value using group=true clause increased clarity yields key value key value key value key value key value key r value key x value could another factor contributing slowness view. great hear best luck couchdb ltd.. hi fact working something similar think gnucash far tested following concepts storing accounting entries array alongside original storing chart accounts database source entries together order obtain general ledger far good absolute beginner couch web done volume testing conceptual level couchdb looks like clean powerful solution problems imho regards. hi run json json validator http//replaced.url says invalid character rec.add doc return rec would try get json jsonlint first pasting expression view hope helps regards bruno. hi thanks kind reply done advised removed update_notification restarting query db reference view continues hope mapping view etc correct get back hmmm couchdb apparently points right db pbo_documents stops somewhere road wrong thanks advance congrats fro great software regards bruno. idiot restarted couchdb-lucene running everything works expected thanks patience regards bruno. hi thanks expressing interest office right provide procedure soon back later day regards bruno. thanks help pulled git yesterday couchdb-external-hook.py assume latest fact download provided two files inside /tools folder home folder look tomorrow try put together list software versions use pretty late europe thanks regards. hi everyone querying view continue getting invalid utf-0 json problem get away database contains two documents attribute view using attribute key like doc.member tinypic http//replaced.url contains view appears futon try rebuilt yesterday trunk 0.0b000000 strange thing aware tests failing changes oauth rev_stemming thought authorization issue read enacted everything found topic still admin party way mac snow leopard pointers help thanks lot bruno. hi everyone sort beginner trying hand couchdb-lucene followed instructions github various blog pages around kind stuck ok since playing around version couchdb welcome version 0.0b000000 installed following instructions github seems running curl http//replaced.url get seemingly satisfactory response couchdb-lucene welcome version build indexing view like document contain property suspect things fail hookup couchdb couchdb-lucene modified .ini file like must something wrong nothing happens even unable find couchdb-lucene logs supposed changed permissions /target/ snapshot/indexes folder still see changes must something really stupid need help btw love couchdb think tight integration lucene recipe great applications user-experience congratulations everyone involved regards bruno. thanks lot escaping works indeed single quoting however need study wiki understand going regards bruno doc.member. hi everyone intend work couchdb-lucene downloaded latest version yesterday everything ok initial setup able obtain correct responses examples read would like proceed complex indexing getting behaviour explain happens change index functions views and/or alter view names i.e properties object start getting code reason no_such_view messages getting messages changed/altered views querying unchanged views within _design document debug level logs bear explanation least noob eyes cleaning curl -x post http//replaced.url help tried deleting index files help either thing seems work starting afresh new _design document design picked unstable state yesterday would really appreciate help regards bruno. exactly background accounting seems fact journal entries stored along original document makes incredibly easy audit track books turn enabled schema-less nature couchdb fact makes easy record data original document _attachment instance together happy see someone else thinking along line regards. yes works misinterpreted paul iscomment simply double quotes single quotes shows beginner thanks taking time point error regards bruno doc.member. please make seperate fix update jumped gun bad surround try/catch block update. right sanjeev classes done way without using map sometimes need use object methods available response returned list method think need add condition said use map classes closing pull request add new. don't merged merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. remibergsma please merge thanks raja. runuser command invoked daemon function pass back default behavior may find java may find merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. part little concern arguments separate line function defns really prefer argument lists line definition since find readable guides proposed actually suggest include arguments line unless docstring would really prefer follow rather applying rule indiscriminately it's request don't make debate. artifacts corrupted central warning pom testng jar:0.0 invalid transitive dependencies available enable debug logging details don't see it's wrong pom explains noclassdeffound. arunmahadevan document behavior non-stateful bolts topology stateful bolts traditional topology i.e include stateful bolts 'ibasicbolt instances tuples automatically ack'ed output tuples automatically anchored tuple 'irichbolt instances anchoring ack'ing left implementation feel think revans0 alluding contract must maintained adding stateful bolts topology believe patch please correct i'm wrong seconds question 'istatefulbolt implementations expected terms ack'ing/anchoring since output collector handed 'istatefulbolt instance 'outputcollector opposed 'basicoutputcollector would assume ack'ing/anchoring responsibility implementation 'ack 'fail methods visible 'outputcollector 'basicoutputcollector case i.e stateful bolts expected handle ack'ing/anchoring would best give 'istatefulbolt it's output collector implementation expose ack'ing/anchoring api similar relationship 'ibasicbolt'/'basicoutputcollector. forward port osgi related changes 0.x master branch include changes index-reader requires mindexer-0 applied merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. note happens java looks like compiler version brings it's set inconsistencies. solution could usable mng-0 http//replaced.url. missing bolt case merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. pr contains fix unit test i'm mocking path fail machine test running contains garbage given directory kishankavala tested kvm datacenter easily deployed please testing make sure full build source merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. think completed right could possible update pr cloud merge first step. hi related solr acceptable list talking arranging meeting delhi/national capital region ncr people using solr interested search related technologies first meeting specific agenda largely intended getting know another please see discussion thread ilug-delhi http//replaced.url moderate interest expressed solr mailing list propose merge meeting planned meetup python people coming sun. 0th tentative time 0pm three possibilities venue -dilli haat howzzat gurgaon micro-brewery jnu interested solr and/or python plan attend meet please vote preference venue http//replaced.url wed. people interested foss areas welcome regards. yes realized stored= false problem stated clearly documentation used basis suggestion field must stored thanks time"""
"""25d53f5bed4ab97165996b0526a5a9bbc19600442faa72d962fff32cac01dd49""","""hi tried result build failure nice colored system seriously different imho misleading error message. sap eventspies used well useless would welcome review convoluted incomplete listener interfaces. point failing build lib 0.0-snapshot pom file result app failed build exists another lib 0.0-snapshot parent 0.0-snapshot abides app-to-lib version range note relationship parent lib dependency relationship instead parent relationship failing case build app correctly succeeds exists another lib 0.0-snapshot dependent parent 0.0-snapshot version range. oh sure parent allow version range hourah thanks let see option ok developers case think reason fill jira entry mean iterating set possible resolutions fail set fails acceptable resolution found fact happen failing parent relationship replaced failing well dependency relationship understand root cause failing build pom considered error stops whole process whereas failing resolve transitive dependency marks particular resolution path unacceptable continue search imho behavior depend relationship parent vs. dependency behavior cases vote continuing version range resolution thanks. higher resolution path build would fail well regards. looks aether thought manage kind issue direct dependency dependency selector given chance exclude dependency graph hint possible configure aether ignore lib 0.0-snapshot resolving version range thanks. hi says suggestions overcome issue explanation let projects three minimalistic sample projects http//replaced.url happens time versions bumped projects deployed remote repo parent 0.0-snapshot still remote repo parent 0.0-snapshot still remote repo parent 0.0-snapshot still remote repo parent 0.0-snapshot still remote repo maven tries build pom lib 0.0-snapshot fails parent 0.0-snapshot available anymore however imho build app fail another dependency resolution lib 0.0-snapshot parent 0.0-snapshot fact relationship parent lib dependency parent-child relationship build app succeeds even transitive dependency parent 0.0-snapshot resolved note app build successful day lib 0.0-snapshot turn evicted parent 0.0-snapshot still remote repo suggestions overcome issue builds failing every day build farm reason thanks ps content mail duplicated http//replaced.url. fyi use-case eventspies sap exactly described igor use event spy instrument hudson/jenkins instance monitor builds maven project thrown hudson/jenkins monitored customization maven distribution dropping files lib/ext setting maven easy upgrade regards. thoughts anything configuration items check could second clusterstatus timeout messages i'm getting reference clusterstatus stack trace out:000s thanks thoughts. ok fyi solr replaces file instead editing means webserver needs permissions directory delete create solr.xml file fixed longer gave ioexception errors. try bobo plugin solr custom request handler http//replaced.url time use boostqparserplugin boost queries something like boost b=log popularity foo use boost feature conjunction external custom request handler like bobo plugin boost work standard request handler. sake completeness query w/o which=type_s parent search_t item0 parent which=type_s parent search_t item0 detail first symbol magic http//replaced.url. trying automate startup/shutdown solrcloud shards noticed bit timing issue server bootstrap zk configs complete process i.e data conf yet servers fail start obvious solution start solr instance responsible bootstraping first way folks handling. anyone explain provide links. solr wiki described install solr tomcat managed chapter configuring solr home jndi mentioned success query top level page solr admin link works anybody help. thanks shawn i'll give upgrade try see helps. defaults text anyway remove entirely solrconfig never specify solr query portion still defaults text anyway. solr wiki says repeated question score stemmed version solr keywordrepeatfilterfactory added assist see example wiki reproduced stemmed original term get indexed don't see original term gets weight stemmed term would don't require filter gives terms attribute missing. thank reply try corrections realised actually logs error tomcat log says seems saying executed successfully dont see data solr though. hi i'm new solr try tutorial deploy provided demo application got basic understaning work however tried deploying demo tomcat unsuccessful anyway i'm trying create two example searchjsp resultjsp searchjsp send search parameter solr server resultjsp receives result solr server display question best way deploy application/ suggestion structure help/suggestion would appericated. look like special chars filtered index time replaced space would keep correct offset terms paste definition fieldtype shema.xml data looks like query first word following queries must match matching ignore characters like match substrings far following field definition schema.xml definition matching works planned highlighting special characters seem move tags wrong positions example searching jsdhfjk misses last letters words special characters patternreplacefilterfactory solr many bells whistles must get correctly working highlighting kind regards. needs batchsize fetchsize mentioned much internal details fetchsize important thanks. hi shared core even tried reloading using collection api still cloud view shown orange however said core working perfectly fine exception logs cloud view bothering. thanks good stuff don't expect make fix find i'd like make something allows file uploads via xmlupdatehandler well ideas thinking could send xml payload another would work thanks. thank much responses guys acl need make web service call find user access document hoping get search results call web service ids search results telling ids user access filter others returning back user acl role based definitely food thought need figure synchronization issues thanks. yes thanks. intention it's happy slotted great it's even finished working might offer something suggested wendy look decide think better hack works happy suggest moved it's lame it's always it's experimenting wanted somewhere put access don't see lame it's working prototype it's trying create replacement it's nothing wrong making prototype pointed wendy knew touched code quite it's interested working it's got access started right away let run absolutely nothing wrong problem committer either wants hack away let work. trying force 0-alpha-0 think declaration maven-project overriding thanks. it's really matter going support saying it's important much don't believe acceptable least put somewhere else people work thanks. i've cleaned everything tagged rc0 anything else need rolling vote. turning row containing supercolumns lots subcolumns row supercolumn lots regular columns yes would efficient unless really read subcolumns w/ query anyway case don't matter. check console output http//replaced.url view results. see changes harsha minor verify acls group resource servers test cluster. see fetching upstream changes http//replaced.url plastic compilejava up-to-date plastic compilegroovy up-to-date plastic processresources up-to-date plastic classes up-to-date plastic jar up-to-date tapestry-core generategrammarsource up-to-date tapestry-func compilejava up-to-date tapestry-func compilegroovy up-to-date tapestry-func processresources up-to-date tapestry-func classes up-to-date tapestry-func jar up-to-date tapestry-test compilejava up-to-date tapestry-test compilegroovy up-to-date tapestry-test processresources up-to-date tapestry-test classes up-to-date tapestry-test jar up-to-date tapestry0-annotations compilejava up-to-date tapestry0-annotations compilegroovy up-to-date tapestry0-annotations processresources up-to-date tapestry0-annotations classes up-to-date tapestry0-annotations jar up-to-date tapestry-ioc compilejava up-to-date tapestry-ioc compilegroovy up-to-date tapestry-ioc processresources up-to-date tapestry-ioc classes up-to-date tapestry-ioc jar up-to-date tapestry-json compilejava up-to-date tapestry-json compilegroovy up-to-date tapestry-json processresources up-to-date tapestry-json classes up-to-date tapestry-json jar up-to-date tapestry-core compilejava up-to-date tapestry-core compilegroovy up-to-date tapestry-core processresources up-to-date tapestry-core classes up-to-date tapestry-core jar up-to-date tapestry-hibernate-core compilejava up-to-date tapestry-hibernate-core compilegroovy up-to-date tapestry-hibernate-core processresources up-to-date tapestry-hibernate-core classes up-to-date tapestry-hibernate-core jar up-to-date compilejava up-to-date compilegroovy up-to-date processresources up-to-date classes up-to-date jar up-to-date aggregatejavadoc up-to-date plastic assemble up-to-date plastic compiletestjava up-to-date plastic compiletestgroovy up-to-date plastic processtestresources up-to-date plastic testclasses up-to-date plastic test up-to-date plastic check up-to-date plastic build up-to-date plastic sourcesjar up-to-date went wrong run stacktrace option get stack trace run info debug option get log output. see fetching upstream changes http//replaced.url hudson.plugins.git.gitexception could apply tag jenkins-cassandra-quick-0 please tell git config global user.email replaced email.addr.es set account it's default identity omit global set identity repository. see changes hshreedharan flume-0 asynchbase avro bring different versions netty mpercy flume-0 example export command readme properly close string brock flume-0 make rename meta file platform neutral. see changes yavor fix brooklyn-0 createuserpolicy always resets password yavor adjusting test don't reset password executed. see changes ismael minor reduce log level peer don't authenticated went wrong run stacktrace option get stack trace run info debug option get log output. ok seems rules you're free make thanks"""
"""0d57a6bd7080012afdf24dc0d5ff3095d0b7a90c793c4a35d2739de2dedcbf01""","""documentation http//replaced.url specify names/positions fields csv file ignore fieldnames seems like would solve requirement different layout could specify mapping import could handy provide map versus value map updatecsv supports could use header provide mapping header fieldnames schema fieldnames. hi would started using embedded solr back via patched version in-progress code base wondered paragraph said given current state solrj expected roadmap solr general would guidelines special circumstances warrant use solrj know back namely multiple indexes run multiple webapps handled multi-core generating lots http traffic handled dih maybe solr search system since integrated system hands customers restart container option anything got wedged might still issue commonly compelling reasons use solrj thanks. hi arno need add boilerpipecontenthandler tika iscontent pretty sure means would need modify solr e.g trunk tikaentityprocessor.gethtmlhandler method would try something like return new boilerpipecontenthandler new contenthandlerdecorator though quick look code curious use. hi exactly something similar nutch searches using ehcache http//replaced.url store rewritten query string serialized xml response way dependencies stable searcher/doc ids nutch reference get remote searchers depending entries cache hit docs xml representation storing xml might. hi savannah comments scattered in-line store xpath expressions text file strings load/compile needed definitely yes using tagsoup clean bad html definitely yes needing per-site rules typically xpath optional regex needed extract specific details common sites powered back-end often re-use general rules markup consistent kind thing use bixo http//replaced.url requires knowledge cascading hadoop order yes would separate job field index though often job titles slight variants would probably work much better automatically found common phrases used otherwise get senior bottlewasher sr. actual format extension b characters xml posted. normally would say like getting swap based settings 0gb jvm space used 00gb box confirm nothing else using lots memory right top command showing swap usage right encounter slow search times top command say system load cpu vs. i/o percentages. hi sandhya would post question replaced email.addr.es mailing list include details document confidence level often low enough tika assume good match thus report language. hi erik let us say grins different field besides used autocompletion would places would need hit change field besides terms.fl value layout.vm example asking trying use latest support index uses product_name auto-complete field getting auto-completes happening see solr logs requests made /solr/terms auto-complete look like would expect work seem generating results odd try curling thing use would consider minimum set parameters get expected xml response ideas wrong thanks. curious mean utf-0 complaint mean thanks. interested need sorted output faceted browsing alternative output formats something along lines merge xml responses w/o schema proposal would fine much better would use hadoop prc versus http call sub-searchers assuming better performance might fewer connectivity issues leveraging work done embedded jetty example anybody data points relative performance master schema main search server could get distributed remote searchers would part thanks. hi trying morelikethis support getting odd results realized unless fields used similarity lucene re-analyze field using standardanalyzer case quite different using solr schema first note anybody using morelikethis make sure specify termvectors=true solr schema fields passed query mlt.fl parameters second note wiki page example schema might include reference termvectors field attribute example sample schema says made think initially attributes http//replaced.url make mention termvectors termpositions would edit page currently section talks attributes common ones thanks. shamed taking position vote earlier real experience slf0j indirectly via use jetty know would _something_ logging hood use log0j everywhere finishing earlier work creating jul logger bridged log0j another option would rather avoid. hi got field defined solr schema always contains two fixed values documents get added boost supplied varies max query field expecting ordering results would match boost values longer specifying omitnorms= true field still get seemingly random results use full search interface solr results two documents different boosts looks like expecting value different different document boost values missing thanks. currently links seem wind pointing api-0_0_0-alpha versions expected e.g search streamingupdatesolrserver first hit streamingupdatesolrserver solr api follow link get page http//replaced.url. difference free speech free beer see http//replaced.url. think concern could addressed set defaults fall back try following sources order get value found try contextroot namespace-specific property found pom.artifactid think plugin always able override project-level value first common system property like contextroot would allow multiple plugins configured property finally currently default. preparing patch submission maven-eclipse plugin fix bug enhance functionality setting context-root wtp0.0 project question pom.xml user specify right wtpcomponentwriter grabs webappsrc value maven plugin seems like logical place put contextpath/contextroot parameter time probably appropriate put property used plugin otherwise property could go eclipse plugin configuration feels bit odd reason. wiki page show use -h option curl set see. yeah terms expand phrases use wildcard fuzzy queries special characters slash part term try. uniquekey wiki recently updated indicate new solr solr field must populated via changes given contained updated wiki page. agree it's bad situation don't handled well lucene guys may good reasons don't execute decent plan migrate existing behavior. particular reason use see give bunch examples book. annotations jar breaks tapestry running jdk numbervalidator allow special case rendering zero used assetservice copy private assets onto file system within web folder static urls assets may generated may use file except compliance license may obtain copy license stored normally visible client web specifying two configuration values tapestry export private assets directory visible client web browser url value map directory specified value due changes tapestry longer needed issues security performance fixed tapestry allowing private assets accessed zero configuration. must archiva-0.x branch removed cant kept. thanks sharing experiences vaclav it's valuable yes bad things happen region server don't phoenix jar improved hbase hbase-0 hbase-0 fixed phoenix phoenix-0 chance would mind filing jira much detail thanks. ah don't notice timeouts shown final report failures seems build using jdk test run oom thanks. you're using avatica context phoenix might interested phoenix-0 adding load balancer phoenix query thanks. behalf apache sqoop pmc excited welcome venkat ranganathan new sqoop pmc member venkat ranganathan become committer last year since remain active project helping reviews user questions working various features recently added support hcatalog/hive added support lot new data types requested users see quite impressive list contributions well deserved venkat"""
"""11411a35018100822513329188627157767176b867623a5d794fb4ba2b98edae""","""problem original poster two years ago solr using shard searching performing sharded query would get empty missing results documents querying shard individually worked anything shards parameter yielded result documents able get results back updating schema include problem seeing solr formulating queries go get records shard including square brackets around ids asking e.g delved solr code saw query string formed querycomponent.createretrievedocs simply calling tostring unique key field value document wanted get value objects somehow arraylists something like strings annoying square brackets showed via tostring emphasizing schema field single-valued lists would hopefully stop appearing think least brackets went away isrelevant querycomponent code check depending may need tha simple tostring comment seems fit theory. outsider thing change seems completely sensible follows try treat dependency plugin versions words let 's merge focus work debuggint lib differences. would someone able fire services say basic zone host. thanks jean-marc n't use cloudera manager. unfortunately 's fair amount larger organizations use n't ignored reality imho. would check copyfield target field something updaterequestprocessors copies baring two field return put regards http//replaced.url. since going spin new apache bigtop rc0 see inline sounds like great. thinking little problem rest api solves honest agree completely n't think rest layer provides feature/function cql valuable except cases like described may common honest get excited creating separate project started thinking value-added services could delivered like capabilities riak etc think way components implemented believe might common needs reusable e.g maybe change spin bit focus services layer top cassandra instead rest interface services layer maybe change sign rest bigot. hi jason talking embedder stuff week looking batch jira issues filed time back n't really sure still problems could give component thanks. hi carlos something sequence changes plexus-spring yesterday started breaking archiva build validation errors change need make get things working n't changed svn redployed old version code latest snapshot repair build thanks. going leave couple reasons currently using maven-0-snapshot either needs sorted rolled back problem recently added pregoal clean clean thanks. hi stefan thanks reply authentication works however getting authorization error curl error unauthorized reason authorized access added reader list replaced email.addr.es someone 00example 0e.com futon able login email address successfully authorize command line curl idea thanks. analyzinginfixsuggester mini solr index it's working designed returning choices see don't think persuade oob took quick look solr-0 it's simple fix lines code change rest test code could consider applying patch code base using rather fully upgrading best. highlighting dependent data fed highlighter unless must re-analyzed see highlighting compressed files seems like odd use-case business reason need best. made progress writing analyzer basically added tokenfilters solr factory classes copy paste worddelimiterfilter course package. hi workaround use facet.query frange query parser. don't help unless move solr it's update processor book first character string field add integer value another field placed add-char-code.js file conf directory solr collection get value named field no-op empty test. hi use wild cards autocompletion lucene far better tools making good autocompletion since wild card term query passed configured query time analyzer comments use porter stemmer use german specific stem don't index time tokenizer defined possible behaviour undefined far know. used marklogic around months majorly used custom ontologies serious issues start asking search results default go regards. yes deprecated less typing curl commands solr don't default application/xml mystery. hi got problem unique key defined schema.xml difine query_id indexing says missing mandatory key query_id present root entity data-config.xml indexing product database unique key schema set unique key says missing mandatory key present root entity data-config.xml indiexing user query another table database user_id unique fix thanks index tables basically unrelated common fields thanks replaced email.addr.es wrote. hi recent version search real-time search feature require commit. hi don't give close work get cjkanalyzer cjktokenizer work lucene able get work solr look bright site least ant don't throw errors code don't going work since really don't handle chinese japanese characters look things lucene went archives found people used something similar maybe try way filterfactories let know goes kind regards. thanks looking query set solrj calling add make curl call. chance try checked nothing solr logs potentially need debug. check new documentation added today http//replaced.url collections link config sets following rules collection starts config set collection link collection starts shares config set link otherwise link created manually specifying explicitly creating collection http api boostrap_conf=true uploads conf set core solr.xml links conf sets collections creating conf set names match collection names collection linked config set manually update link new config command line util tool commands links collections config sets even make links starting solr first time start solr collections started find use link info created tool names match change link solr running would cores collection pick new config collections api http//replaced.url lucidworks.com. generally apache mahout http//replaced.url recommended clustering. hi integrating solr search website keywords would return 0000s records way solr get results pieces. hello many thanks replay. thought don't seem case. reason thought much complicated looking sources see going easy task thanks help view message context http//replaced.url sent solr user mailing list archive nabble.com. hi fields schema dynamic fields storing stored= false case need atomic update two fields belong stored list fields need change dynamic fields stored stored= true would don't considerably increase index affect performance negative way currently partial/ atomic updates two fields make stored= true without make stored= false fields stored= true accommodate atomic updates could pls give suggestions thanks. understand every example suggested don't get don't possible post exampe schema+commands give error goal process incoming text add derivitave stored fields may look 0-dev updaterequestprocessor need change token value indexed value stored value perhaps custom fieldtype override. yes definitely sounds like bug grouping looks like forgets weight sorts"""
"""11c430bae59cd170a1bd2f25110f9c28cc855382f89c010b23760d1af6263d8b""","""thank much gets moving. changed zkhost use root level works went using. hi using solr cloud zookeeper several shard setup try use solr cloud bring shard setup seems load fine without errors however go web interface click cloud exception thrown happens shard shard setup anyone seen thank. thank reply work around available could related number external zookeeper servers. outstanding task http//replaced.url done provided wouldn't objections serious considerations make losing compatibility elsewhere. think easiest/simplest/best thing would grab already sorted data files hbase re-write accumulo's rfiles empty visibility labels. fyi jaxws spec specifically avoids mention xmlrootelement beans java first case probably due issues seeing spec really it's top level element names thus need use information instead less ignore xmlrootelement annotation xmlrootelement xmltype attribute you're kind ok non-anonymous point however attribute blow things pretty badly definitely outside stuff jax-ws tck would. like foam vein spume fave. fyi wouldn't see attached file chance commit svn index.html thanks. chunking turned connection authentication currently happen httpurlconnection implementation hides needed information looking jakara commons looks quite promising works like web page says okay it's user wants it's gets surely please keep us line issues familiar jax-ws i'm learning. noticed component depends cxf-rt-bindings-http system regards. discussions make sure you're meeting requirements i've added box scenario understand things would work various requirements andrea couple questions things need done list hoping could answer i'm sure means elaborate something like combining httpdestination httpserverpolicy object configuration bean component put together list scenarios need big case seems you're using jax-ws static apis need customize transports/etc rough idea works head feel like kind nebulous yet. yup it's relative path erlang it's point view fine see couchdb reading files successfully debug mode ran file access logging tool procmon seems find couch/lib/couch-0.0/priv/lib/couch_icu_driver.dll prior tanking i'm seeing errors configure make makefile generate http//replaced.url would appreciate guidance looks like last hurdle thanks. i'm sure following exception expected cause build fail expected makes sense add debug message something like following badargumentexception. hi still couple issues left jira. likely case using wrong snapshot doxia vincent deployed new snapshots doxia lately probably get new ones. long wait someone create complete solution ever feel accumulo user better. pretty clear commit-then-review lazy consensus really issue regards commits said still think ignoring warnings best course action compiled warnings command line wouldn't see resource leak warning java voted use java wouldn't issue make move check warnings present building command line wouldn't sufficient please let know added following configuration block. thanks corey. would like merge today objections. ugh github decided earlier comment goes whole change line wanted selected vendor ibm set otherwise set -dcrypto.secure.rng.provider=ibmjce looks backwards. note wondering suggestions deal laundry list provided dependencies accumulo core writing packages bit ugly using accumulo start maven utilities automatically dissect provided dependencies make included. log0j configuration ultimately owned user enforce custom appender wouldn't think rely anything ~/.dt either. understand vlad correctly saying operator setup checks output methods threads different operators running potentially different nodes and/or processes connection. great thats thought suspect might seeing otherwise app anything needs done wrt offset management kafka reader app ensure works correctly developer need decide kafka reader app moves kafka topic offset forward acknowledge messages read point successfully written cassandra. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. ok. see using ui application gets suffixed int values make unique case way retrieve current running job within application use case needs access application details application track certain things retrieve dt.attr.application_name set within application regards. project http//replaced.url commit http//replaced.url tree http//replaced.url diff http//replaced.url. changes towards idiomatic groovy tests related access non-private fields transformed classes new feature. bind field container property matching field it's property actually exists used subclasses create default binding property container used subclasses create default binding property container return binding property null container corresponding provide default binding container actually contains property sets error condition value parameter bound default value property container whose matches component it's may return null container matching property yet implemented return null container matching property. tapestry it's built-in asset types context always incorporate version number part path alternate implementations encouraged well addition tapestry ensures context assets far-future expires header ensure aggresive caching client assets far-future expires header ensure aggressive caching client. hrefkey quite easily implemented diff would new window best regards. hi testing release much appreciated harsha said user-focused documentation still in-progress addition ssl kip zookeeper authentication terms various security features ssl authorization zookeeper authentication interest. hi together makes sense creating snaphot creates links files snapshot directory fast takes space links hard links symbolic ones running cluster compact sstables writing new deleting old ones example sstable0 snapshot links four compaction active sstable0 newly written consumes space snapshot-linked ones still still consuming space snapshot cleared get. filters ask specific questions specific statements made far on-disk format solr user absolutely zero reason concerned format lucene uses store index disk certainly welcome dive level wish something worth discussing list solr user index simply list terms positions determined character filters tokenizer token filters analyzer format information stored lucene don't impact behavior solr app way clear need thoroughly familiar doc section don't help try questions ask basic understanding stated doc page might help visualize doc says using analysis page solr admin ui give intermediate final results analysis process specific token/term text position step even don't help unable grasp stated basic doc page"""
"""0c88527e76ab30d41d0b192c43dfa142bb2212fafffd51b02f40c3ce9ded574a""","""use injection techniques add logging sounds eh interesting would elaborate. sure msdn library says fileshare.read allows subsequent opening file reading change fileshare.readwrite allows subsequent opening file reading writing might better luck hi quick question using rollinglogfileappender webservice logging works whenever try open log file reading another process get sharing violation log file open file would enable minimallock way use exclusivelock another process read log file kind regards. rebuilt log0net library strong key message log0net missing strong project. browsed log0j isdocumentation little came across file appender listens port certain signal comes rolls file i.e closes current file renames starts new file windows alternative sending hup signal make work would sort external log rotation tool complexity log rotation tool could arbitrary could simple windows think solution. would suggest try find locking file tool handle freeware tell split sec see another instance log0net someone/something trying read file latter case lucky specify reader open file read-only shared mode blocking gone case always good idea see problem really changes. well wrote log0net-0. reference strong file assembly info assemblyinfo.cs use attribute assembly replace empty string file. ok urban understand think find looking inside framework though question since use short form format specifier r mean use old version using took find list format specifiers way solve problem would use timestamp r process log file separately would quite easy calculate differences using excel example really small note maintainers nicko text manual page mentioned says conversion specifier starts percent sign followed conversion character correct. interop component vb components use interop component error tracing logging interops handle config files well hence using xml file config file load set rollingfile appender properties understand like app.config file put log0net configuration application put log0net configuration using normal log0net configuration syntax separate file e.g logging.config configure application simple line main program xmlconfigurator.configure new make sure use configureandwatch absolutely difference log0net reading file code reading specially-formatted file besides course upside reinvent whole configuration code. since currently way conditional settings config file would say implement condition code clean way would use custom property e.g friendlyappdomain set startup either current appdomain predefined constant depending appdomain looks use property configure file would still make application fairly flexible comes configuring file future still change filename anyway without recompiling personal opinion yeah ask anyway cleanest solution today would different config process imagine someone else know log0net make changes app probably go source diving order figure mechanics behind file naming scheme two config files less elegant completely straightforward him/her understand future might able two appenders different configuration inherit common properties virtual appender something like discussed. tested chainsaw java web start great thing liked production environment web application would much appropriate anyone experience thing favourite personally running apache/tomcat considered. yes reason sometimes takes generate argument log.debug code looks something like would course benefit using instead since time-consuming generation message occur log string literals suppose point using. hi guys feel little stupid matter much browse sdk zilch application detect internal error occurred see case bad configuration file detected testing logmanager.getrepository .configured detect errors say fileappender reason unable open file vague memory seeing noappendersconfigurederror find let us turn question something general would think examples good practises implement personally would like series tests startup app make sure people running applications skilled necessarily developers get decent information something happens. really simple solution loggers create note logger longer static counter keep track instances could course replaced something sophisticated naming logger. sounds good thresholds/filters distributed loggers log events thresholds/filters applied logging server injected remoteloggingserverplugin case need simple way distribute logger-specific parts config file distributed servers btw thanks excellent job support mailing list simply beyond comparison wrote. thinking along new requests features rolling file appender comes really done logrotate unix tool specialized rotating log files today googled around little astonishment realized seems like made windows port logrotate really strange apparently logs files opposed event log database windows way servers log files like iis generally include rolling mechanism reducing need still store log files matter many variables think set limit rolling file appender leave fancy parts external solutions even means us build log rotating opinion rolling file appender already grown complex several configuration parameters whose side effects cooperation parameters hard foresee without reading source anyone knows windows port logrotate please shout. sorry line became complicated necessary isget say. timestamp since start application creation logging event. well ilog interface isxxxxenabled methods solve situations need address current log level. dear question general kind use log0net system architecture involves remoting consider example enterprise system consisting webserver w companys dmz application server inside business logic components connects database d. sometimes business logic components need call components server hosts kind third-party software calls w using .net remoting calls using yes realize reveal person previously forced work dcom world question still still .net world several reasons wanting distribute companys software several servers consider ways go configuration file every server let every component logging locally means logging info single call spread machines passed merge changes configuration duplicated many machines next level might configure remotingappender machines logging persisted transmitting single machine hosts remotingserver sure flexible though local config files say deliver everything remotingappender much say logging server remotingserver picked event ugly solution might let every called component create instances loggingevent similar return array main app takes array logs dream solution might involve special remoting-aware loggermanager takes retreiving caching config files logging server transparently passes events shall logged repository logging server wish list looks like single config file rule darkness bind least place changes made place logging done appenders work net adonetappender netsendappender remotesyslogappender etc may course file logs windows event log machine code logs preferably know remoting scenario exciting problem decision wether log done logging level filters etc set config file another machine really ask would like must many ways think pros cons. using use standard xml escapes xml guru think values respectively course start wonder need put layout first place whole structure appenders filters layout stuff powerful infrastructure filtering routing files etc. give guys list shot helping. hi richard would much like see best give two major log viewers today chainsaw http//replaced.url opinion focused log0j apache/java environment unfortunately freeware viewer definitely space fill"""
"""05c55dc3e1b707311232820199d85f66084dfaab49c54c6c4f82163ae468a27f""","""scale note xml schema anything syntax change operating pom instead pom postpone things forever let us find future-proof solution please endless discussions led nowhere far means need revert new import scope behaviour would mind see minor version increment model version far problematic others know else introducing new model building behaviour iam let isharm road better wrong course bad xml would make things like polyglot maven even harder since consumer pom something technical edited manually could keep xml forever xml parsers xslt processors available nearly every programming language xml makes sense solved way change semantics could used transform different syntaxes changes made bump model version syntax related fact diff pom-0.0.xml pom-0.0.xml difference would value model value would need deploy two poms summarize need find solution handling different syntaxes solution handle different semantics syntax going bump model versions must clear everyone increment means syntax different semantics minor version leaves patch version bug fixes like changing order elements combine.children attribute quite xml related better think model terms xml currently master would work sure model version increment without change syntax really issue however regards. maven team pleased announce release maven pmd plugin pmd plugin allows automatically run pmd code analysis tool project issource code generate site report results specify version project isplugin configuration corrupted copied target directory wish. apache maven team pleased announce release apache maven component provides abstract classes manage report generation run part site generation maven-reporting-api ismavenreport direct standalone goal invocation maven-plugin-api isspecify dependency project isdependency configuration sub-task. hi announced quarterly report january studying jira migration codehaus apache migration happen week- create exact jira projects asf codehaus copy whole content mark codehaus read-only example done years ago near empty mpom moved http//replaced.url username migration codehaus asf happen migrating existing content user accounts migrated isway jira works avoid creating accounts already exist apache matching done account ise-mail address mapping look something like e-mail/username pair codehaus content email associated user asf jira instance reuse else create new account asf jira instance exact suffix still needs defined sure username apache jira please look e-mail codehaus check matches e-mail apache question hesitate ask keep informed operation planned details. closed issue already fixed. maven team pleased announce release apache maven toolchains plugins allows share configuration across plugins example make sure plugins like compiler surefire webstart etc use jdk execution specify version project isplugin configuration mtoolchains-0 add check plugin works expected. maven team pleased announce release apache maven specify version project isdependency configuration mshared-0 dependencies inside pluginmanagement taken account reporting defined. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project isreports configured pom notice breaking changes know upgrading specify version project isplugin configuration download appropriate sources etc download page sub-task generation jar menus point index.html ignored unless file generated-site/markdown/ distributionmanagement site url another defined child project property maven.site.skip set true available generated-site component wish. maven team pleased announce release maven ant tasks maven ant tasks allow several features maven used ant deployment maven repository information available current version maven ant tasks downloaded project defined contrary documentation wish. sounds intent would work test confirmed working way impression get neither false positive false negative test test intentionally changing behaviour longer supports new syntax support new vote mark existing test range= add new test clone switched sent phone. codehaus apache done week-end new jira projects ready use asf jira every migrated project codehaus turned read-only moved asf notice added reminder still issues accounts username codehaus already used apache please report issue infra-0 precise info regarding codehaus username expected username apache thank codehaus support migration generally great service given us many years thank apache infra huge work import great result hides problems encountered. maven team pleased announce release maven ant tasks find binaries find release notes. migration start 00h every jira project codehaus migrated asf marked read-only remember check jira account email settings codehaus asf sure result asf everything tracked http//replaced.url. development apache maven promotes use dependencies via shows plugins ready finished expect contest create new maven create mascot maven owl work progress integrate maven studying jira migration codehaus apache better end-users consistency since got feedback users lost requiring compliance issue closed report last status topic users mailing list activity reduced little bit last strongly increased within time frame http//replaced.url. sent replaced email.addr.es like jira issues told mailing list conf fixed days ago infra-0 regards. apache maven team pleased announce release apache maven doxia version used future version maven-site- doxia content generation framework provides powerful techniques download appropriate sources etc download page wish. see embedded mode core run minutes objection merge embedded mode master regards. put build-pom vs consumer-pom place need publish build poms repository repository consume already- built artifacts dependencies consumer pom without newer maven version prerequisites let people consume key feature able generate consumer-pom good old pom semantics without build configuration useless consumers suppose new include scope could expanded dependencies/dependencymanagement consumer pom example build-poms need central parent poms ie poms pom packaging poms necessary build artifacts putting maven version prerequisite use issue notice consumer-poms parent poms useful imho parent poms build-poms published central pom consumer-poms published sustainable solution pom artifactid limitation generate build-pom consumer- pom since new semantics really equivalent case remember artifact consumption artifact build regards. maven team pleased announce release apache maven project specified project specify version project isplugin configuration specified via pluginmanagement. apache maven team pleased announce release apache doxia doxia content generation framework provides powerful techniques sites consisting decoration content generated doxia documents like rtf pdf format generate page garbage skin generated content package o.a.m.doxia.siterenderer. apache maven team pleased announce release maven site site plugin used generate site project generated site includes project isreports configured pom specify version project isplugin configuration output parameter msite-0 report inheritance work specified site wish. apache maven team pleased announce release apache maven doxia version used future version maven-site- doxia content generation framework provides powerful techniques download appropriate sources etc download page switch parser pegdown flexmark. maven team pleased announce release maven plugin plugin uses tool generate project specify version project isplugin configuration project modules never installed/deployed. apache maven team pleased announce release apache maven plugin utility plugin allow publishing maven website supported scm primary goal utility plugin allow apache tested git scm example push content github specify version project isplugin configuration. maven team pleased announce release maven project info reports specified project specify version project isplugin configuration. apache maven team pleased announce release apache maven plugin allows generate pdf version project isspecify version project isplugin configuration download appropriate sources etc download page document. apache maven team pleased announce release apache either html sites consisting decoration content generated doxia documents like rtf pdf download appropriate sources etc download page get info report. apache maven team pleased announce release apache maven calls information standard jdeps tool please refer specify version project isplugin configuration download appropriate sources etc download page first release plugin. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project isreports configured pom specify version project isplugin configuration msite-0 reportplugins should/could inherit information. herve change look ones. maven team pleased announce release maven archetype available local nexus create-from-project command archetype-0 reading archetype-metadata.xml file would helpful know xml badly formed. apache maven team pleased announce release apache maven doxia sitetools extension base doxia component generates either documents like rtf pdf download appropriate sources etc download page duplicate already available velocity tools copy resources call copyresources element string skin wish used internally. maven team pleased announce release maven project info plugin used generate reports information project specify version project isplugin configuration warnings non-integral time-zones plugins. apache maven team pleased announce release maven project maven project info reports plugin used generate reports information project specify version project isplugin configuration produce output changed. maven team pleased announce release maven archetypes see http//replaced.url. apache maven team pleased announce release apache maven site plugin used generate site project generated site includes project isreports configured pom specify version project isplugin configuration download appropriate sources etc download page comes settings.xml msite-0 migrating mvn site run ask wish msite-0 much information maven logs site deploy. apache maven team pleased announce release maven dependency plugin provides capability manipulate artifacts copy and/or unpack artifacts local remote repositories specified specify version project isplugin configuration copies poms twice wish. apache maven team pleased announce release specify version project parent like following changes since version. apache maven team pleased announce release apache maven doxia sitetools extension base doxia component generates either documents like rtf pdf download appropriate sources etc download page date date without precision created last modified siterenderer side rather velocity side modules found wish. apache maven team pleased announce release apache maven maven-scm-publish-plugin utility plugin allow publishing maven website supported scm primary goal utility plugin allow apache projects publish maven websites via asf svnpubsub system plugin tested git scm example push specify version project isplugin configuration seconds retry naturally linked natural site lifecycle multi-module. remarks sub-project taken experience working first see facts http//replaced.url complete list projects documented pmcs lot software described grouped pmcs came conclusion question semantic around project either talk tlps sub-projects trying visions http//replaced.url started tlp sub-projects vision tlp pretty much used us projects top level projects sub-projects bad impression puts -ones fact committees project top see commons logging committees really main project projects like extensions plugin see ant velocity imho talking committees projects best way avoid bad passion comes tlps sub-projects vision terms question merging tlps becomes merging committees ie communities putting projects management merged committee imho description verbose debate less passionated focused main question really community managed committee opinion kafka samza case hope explanations help discussion regards. maven team pleased announce release maven plugin http//replaced.url maven plugin plugin used create maven plugin descriptor isfound source tree include jar used generate report files mojos well generic help goal specify version project isplugin configuration like previous versions fun. maven team pleased announce release maven skins parent specify version project isconfiguration. unit test failing jenkins regarding jgit reproduce machine anybody clue special jenkins regards. sent phone. code maven-internal looked fine seems issue improving toolchains required signature change well able merge global user toolchains assumption maven-toolchains-plugin plugin using specific code adjusted make compatible signatures another thirdparty maven-plugin hitting issue think accept signatures world plugin side always extra handling done either refuse certain maven versions cope signatures since damage already done change gives info instead less would advice keep code as-is provide little code-example solve reflection yes still means need create situations. maven team pleased announce release maven reporting api version. maven team pleased announce release maven archetype archetype-0 archetype downloaded pom placed working metadata.xml descriptors generated needed create-from-project goal"""
"""30b647497e524daa7f575690abd21920047511555b6fd6a469e4bd2a314579f1""","""hi compare maven dependency mechanisms home-brewn solution company among others major thing different maven know concept artifact life cycle least know mechanism refer build life cycle life cycle status information would allow extend dependency management new dimension could declare whether certain dependencies actually allowed used. hi jason refined proposal part user proposals page regards. hi great number issues fixed it's high time new i've put together provisioning bundles dependencies clerezza includes bundles new ext.jena bundles wouldn't include sling based full launcher depends snapshot versions felix suffers yet ported source zip signatures keys git tag vote open least hours. commented issue. hi andrija wouldn't worry it's blocker sense cloud i've tried fix testing. please review clean test pass new test test_router_dhcp_opts passing. see changes. projectmoon ip issue worked separate ticket/pr isolated issue dependency pr get pr create separate ticket ip issue say. animesh daan clear proposing keep feature freeze date aug 00th. classes implementing equals hashcode methods please add unit test case utilizes guava it's equaltester verify implementation please see article http//replaced.url information usage. suggest using sphinx plugin http//replaced.url. facing build failure different reason 0-forward looks like due /users/koushik/code/cloudstack-apache/cloudstack/api/src/org/apache/cloudstack/api/command/admin/router/configureovselementcmd.java:0 error package com.cloud.async exist /users/koushik/code/cloudstack-apache/cloudstack/api/src/com/cloud/network/element/virtualrouterelementservice.java:0 error find symbol. thanks nvazquez unit test wrote nice improvement i'm fine. thanks kuang http//replaced.url go chiba e-mail replaced email.addr.es. acton template wouldn't work new template check checking release version thanks. restart network failing using external loadbalencer failure number format exception broadcastdomaintype.getvalue executed returns string untagged trying parse long number pointer exception happens vlan uri vlan //untagged cases number instead untagged vlan tag used succeed although trying convert number long really using converting number long back string creating ipaddressto removed unnecessary conversion case fixing issue hand manual restart network checked number format exception eip/elb setup merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi onuonga pleasure connect arrange live hangout session hangout air many people join remotely well share detail thanks. hi animesh looks like couple critical bug fixes newly checked sounds like i'll spinning new rc confirm pick fix new rc well thanks. thank i've raised cloudstack-0 jira non-vpc rvr issues company needs issues solved short term i'll begin working fix branch fix later useful upstream shall create pr need re-work things different branch i'll happy subsequently. got anything specific lb rule go loadbalacingrules table. problems currently around asf services issues worked feel free look http//replaced.url http//replaced.url. case seems putting design changes new gpu types docs problematic seems docs document vgputypes added link docs changes perhaps part problem administrator documents lacking key info explanation design docs become increasingly hard locate missing information versions released difficult find gpu information cloudstack version find design docs updated admin doc add bit context reference. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. ilya check/share cloudmonkey version probably uninstall reinstall latest cloudmonkey sometimes pip upgrades may work due compiled pyc files path 'filter added cloudmonkey several months maybe year ago work unless problem somewhere else i'm sure 'filter works output display types json output added many moons ago tested 'filter. good point believe determined yesterday could use either approach get results. hi kelven elaborate you're referring past i've changed code set breakpoints inside vmwareresource vmwarestorageprocessor able debug way say code running inside cs ms. would code run inside ssvm instead cs ms i'm unclear thanks clarifying. slipped attention pmc couple days please read consider applying gsoc mentor interested. hi mike thanks reporting issue i've working created pr http//replaced.url i've testing seems solving issues reported please repeat tests thanks. responsibility committer get ccla signed employer paid work done clause employment contract give ownership employer work term employment impossible know unless know employee it's situation worth asking question rights employer contributor trust contributor contributors icla it's employer signed ccla committer it's responsibility assuring contribution committed becomes simpler life committer gets easier every contributor takes minutes gets icla filed ccla required get boss sign put file wouldn't see advantage. yes api-freeze release although planned feasible hold back release longer favour fixed api future pre version get feature freeze think useful feature least earlier patches welcome. .couch functions return promises basically added returns functions cept userdb function edited fixed error uuid function calling multiple times return different types values sometimes string sometimes object removed .couch.urlprefix wouldn't working correctly anyway since wouldn't working correctly assume nobody using merge pull request git repository running alternatively review apply changes patch. exactly asked think accomplish purpose http//replaced.url best adam. thanks noah setting bikeshed really going commits like. './configure run every local install described possible every single individual user it's output files predictable locations either way see like build time control rebar sees build time modify move files could possibly ask except problem go away entirely course good catch mention lot stuff actually think perfectly surmountable key separating erlang build main build think like compiling app would wouldn't compiler try move files location handle installation /etc/config/file treat rebar whatever compilation step otp app even though result directory tree bit like next really use autotools plop right location filesystem. hi recent changes autotools config i'm able build trunk however i'm longer able 00bits mac x snow leopard seems include path used relevant i'make'/i'make dev output i000-apple-darwin00-gcc-0.0 file directory error unicode/ucol.h file directory error unicode/ucasemap.h file directory trunk revision machine .libs/couch_icu_driver.so -bundle anyone else issue well happening even fresh git checkout needs specify parameters configure etc. hi gabriel believed i've reproduced locally thanks report could file ticket would great i'll shortly file i'll get email ticket progresses saying. somehow wouldn't via jira query owe recognition ralph goers put significant work creating new regards. apache isis team pleased announce release apache isis primarily bug fix release includes upgrade wicket fixes major performance regression previous version wicket isis used includes bug fix relating wicket viewer it's reference panel choices small bug small number new features new markup value allows wicket viewer display new logout page wicket viewer support monitoring tools full release notes available apache isis website access release directly maven central repo alternatively download release build source"""
"""e5ef67ecbaaf1d8e8992ed3534836b7b1d76f86fb60f4e6a14fc65d14afa33c2""","""double slashes issue scm-0 opened mng-issue contains simplistic patch fix relevant test case fix might bit invasive affects url scm connection url alternative perhaps normal tests passes suppose original code removes double slashes reason avoiding http redirects perhaps anyone know oh documentation mentions profile integration tests -p run-its maven tells profile. think actually tracked defaulturlnormalizer module maven-model-builder indeed part maven-core defaulturlnormalizer following element scm/connection removes double slash hostname part url normalization occurs release plugin find plugin descriptor instead mavenproject.getscm .getconnection used retrieve scm url method returns scm connection url normalized important double slash removed somewhere lost maze maven-core sorry pinpoint issue better. hello understand strictly concern blocker release maven scm-0 still blocking upgrade 0.x including 0.0-rc0 anyone using mercurial hg scm absolute paths remote server maven release plugin chance get fix included. right weeks ago made weak attempt look source code scm module found nothing obvious something wrong url component actually parses pom.xml extracts string scm/connection element maven use scm url mercurial contains absolute relative path handed hg mercurial looks project wrong place release build checkout fails mercurial special non-standard url annotation two consecutive slashes hostname separate relative paths absolute paths example scm url works fails 0.x double slashes localhost tells hg file path absolute file system located /opt/foo double slash removed hg instead search directory opt/foo relative home directory user server perhaps /home/luser/opt/foo. fill kip purpose kips give full overview feature work implemented considerations involved etc like sentence wouldn't enough anyone know thinking moving configs something i'm guessing would zookeeper-based massive change really need describe way widely circulated actually think would good idea advantages good old fashioned text files terms config management change control trying support may may better. current implementation producerperformance creates static payload useful testing compression test production/custom payloads decided add support providing payload file producer perf test made following changes added support provide payload file list payloads actually send moved payload generation inside send loop cases payload file provided following changes producer-performance evoked must provide payload-file record guaranteed using custom events earlier required config must provide exactly payload-file providing result error support additional parameter payload-delimiter added defaults merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. realized known problem since feb jason showed ticket submitted patch http//replaced.url make sure return data record failed deserialization throw exception next poll. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. see changes went wrong run stacktrace option get stack trace run info debug option get log output get help http//replaced.url findbugs searching files match pattern /build/reports/findbugs/ .xml using gitblamer create author commit information warnings sending mail unregistered user replaced email.addr.es sending mail unregistered user replaced email.addr.es. said general point think consider supporting topic patterns wire protocol requires thinking cross-language support seems surmountable could make certain operations lot efficient fact basic regex subscription causes consumer request metadata topics great. i'd like start vote kip-0 add command line tool offsets details thanks best regards. nexus scheduled maintenance fall i'm trying cut release nexus appears dead thanks. would consider something like sfl0j something part api think it's fine expose something like i'm sure holds true cdi would wouldn't better completely hide thanks. think it's really ok default trunk mode maybe unless checking tag main project it's feature remember clearly would i've enjoyed first time ever tried change something maven clearly checking repositories like crazy building dependencies finally managed find thing interested time felt like built half known universe components reached goal similar-sounding names hey wouldn't check course workspace-plugin would keep pom changes top-level project separate commit wouldn't mess actual changeset could support patch command. need rephrase change documentation release process update fixed -latest place svn copy release svnpubsub give us expected regards. dependencyset specify following things. following issue deleted jira. hi custom enforcer rule occasionally failing looking code defaultartifact appears related error fixed getversionrange could field getter protects maven happens running jdk com.nds.cab.build.enforcer.engineeringreleaserule.execute engineeringrel wonder really defaultartifact.setversion wouldn't null versionrange recreate something else play something wrong enforcer rule following results bogus pom phases line causes projects exhibit behaviour multi-module projects several modules pass hits wouldn't like message confidential intended addressee received message error please immediately notify replaced email.addr.es delete system well copies content e-mails well traffic data may monitored nds employment security purposes protect environment please print e-mail unless necessary. looks interesting eric soon definitive place add link maven site. found bug modello plugin breaking /any/ build using modello multi-modules i'm fairly sure kind issue found elsewhere quite simply it's use plexus components singletons mojos contain per-request mutable state since you're short time away beta-0 wanted know think done treat documentation problem maybe update guidelines regarding singleton usage maybe keep list known b add kind isthreadsafe attribute metadata could used assert run concurrently without something else thoughts b done. android maven plugin team pleased announce release version plugin com.simpligility.maven.plugins new features/bug fixes specific release fix allow release-plugin ides correctly consume aar deps http//replaced.url would like thank contributors release valuable help invite help us well specifically release would like thank following contributors awesome work would like thank help received maven community members everybody else help received issue tracker beyond documentation issue tracker found plugin websites http//replaced.url http//replaced.url enjoy congratulations everyone involved http//replaced.url. hello benson looking http//replaced.url maybe could point maven users developers general better installing jdk regards mirko. really called beta release criteria could vote it's beta. i'll leave guys suggestion make submission package contains references repos userids clas file userids new clas along clas send email pointer repo perms plexus people add userid cla file directory new clas may get i'm sure active plexus mailing list send message herve collect together see discrepancies deal ask legal covered clas acceptable submit it's ok great start trying expunge thanks. please add new version ant-contrib ibiblio. never heuristic replace people taking apis testing without combination part project external party trying collect metrics use probably going great value retaining api compatibility along high degree coverage probably useful metric really it's conscious effort api broken even inadvertently mechanical turk going help us much implementation metadatarepository api it's costly deal dependencies currently pom swizzling deal ranges number possible paths grows quickly sat solver fast could predigest dependency information pre-calculated subgraphs probably useful change range path you're recalculating anyway boil something far simpler sat solver guaranteed find working solution ranges set set fixed weighted say pom fix version depman change calculation like fixed fix let sat solver find solution magic already osgi resolver tycho thing p0 uses sat it's osgi made clear it's osgi proper use right osgi resolver simple state machine it's p0 osgi resolver even though retrieves ultimately fed osgi thanks. sure anybody apache say yes synchronization ibibilio codehaus repos i've got. maybe something get/ set tags wouldn't make changes anyway wouldn't think important worth hassle. split verifier improve cli performance core plugins -target committer free call vote minimum etc. crawl designed kind issue mind long thanks. questions specific closed issues might warrant really implemented intended close wouldn't fix knowledge feature delete issue unless completely mistakenly filed think close incomplete/wo wouldn't fix otherwise it's impossible get back information lost think would useful thing add patch still applies part bootstrap check released version reopen wouldn't really sure fixed patch date eclipse code site good reopened patches optional non- default functionality patches specific problem keep open updated right thanks"""
"""4ee8a6b0f4c6cbd020ffa14217436cc3226ba516a8fb32a15047fe61784dc626""","""thank good start understand seems complicated though got way regards. hello quite dumb question new nutch/solr migrating web indexer commercial product nutch/solr yet understood internal need spending time problem installed nutch/solr everything works fine solr standalone mode even installed solr install_solr_service.sh everything fine go pass solr cloud mode found elegant way modify /etc/init.d/solr /etc/default/solr.in.sh order launch solr cloud mode succeeded start dirty point hints links cleanly remark reason trying migrate solrcloud able basic auth around documents index standalone enough us regards. hi though launch solr -c option last tried. oozie-0 fix warning file wouldn't listed dependency files details oozie-0 using uber mode regex pattern used extractheapsizemb method allow heap sizes specified bytes oozie-0 reduce number threads test execution oozie-0 oozie job submit wouldn't report error message user issue job conf oozie-0 status update recovery problems coord action children sync oozie-0 get ooziesharelibcli perform final rename destpath creating sharelib oozie-0 dataset url contains spaces handled rightly oozie-0 log.scan.duration used error audit logs oozie-0 map-reduce launcher need distributed files archives except jar input/outputformat oozie-0 introducing new counter instrumentation log distinguish reasons launcher failure oozie-0 ssh action succeed exists command fail oozie-0 rerun failed/killed/timedout coordinator actions rather specifying action numbers oozie-0 queuedump command display queue information server oozie-0 loginfo uses action instead oozie-0 instrumentation configuration rest api web ui include oozie servers oozie-0 cache list available timezones admin may edit subscription. oozie-0 new api fetch workflows corresponding coordinator action reruns oozie-0 user able set bundle/coord end-time start time oozie-0 support oozie-0 ability view log information corresponding particular coordinator action oozie-0 new configuration specify server-server authentication oozie-0 pom.xml use profiles building hadooplibs oozie-0 workflowgenerator package tar.gz file though seems creating intention may edit subscription. wasn't implemented today feature generators wouldn't therefore createfeaturegenerators method creates new feature generator every time needed tries read xml descriptor model creates say uses default feature generation happen createfeaturegenerator method returns null true case place exactly add getter method fix problem tokennamefindermodel call tokennamefinderfactory wouldn't instance trying understand proposed fix works usually model created using constructors inputstream file url use different constructor create model try reproduce bug see first train model command feature generator config use command line tool evaluation maybe post command. using tip head issues bytebuffer reads http//replaced.url error seems different you're seeing though might different field 'uncompressed_page_size found serialized data. testing must revise vote encountered http//replaced.url. i'm favour merging parquet-format parquet-mr moment would merge mr cpp development speeds release cycles differ thus would inconvenience repo. reef.net currently depends newtonsoft.json .net core compliant shall add jira update newtonsoft.json .net core compliant. mentioned bug report reason npe happens storm vulnerable fraud message processes outside cluster reproduce npe need send message taskid host :port anywhere taskid tasks running host :port case storm return taskmessage payload set null deserializer storm check whether task message processes within cluster deserialization may fail think storm skip fruad task message rather shutdowning worker deserialization failed. preparing patch publishing in-backlog receive-queue out-backlog send-queue see average value metrics ui time window period executors section aggregation component level etc users may interested instant value metrics wouldn't know fit ui suggestions welcome. part looks good things clean. think you're something addressed pr wouldn't think merge adding document patch master could revisit address remaining comments file issues done follow-up patches wouldn't think addressed could leave comments discuss make decisions could revisit sort remaining things side. able make integration spring problem autowire every bolt spout means even parallelize spout bolt get started instance way bolts spouts mean parallelize bolts spouts individually share conf somewhere possible thanks. actually looking git blame appears view never two columns think intent however two columns exist view i'm adding sandbox testing replaced email.addr.es wrote. i've managed set rsync daemon appears easy get deb packages get rpm packages i'll update readme file later http//replaced.url. thanks compiling list see reporter issue plan look see current state reviewing old jira tickets bunch closed following ones gone close ticket would like check especially ones category called seems fixed checked waiting feedback proceed close ticket say much ticket detailed seems fixed checked check may able help checking may able close brief discussion/conversation able add tags projects working right understand feature already lacking documentation feature suggestion discussion person said old configuration removed said however discussion stopped think close connection validation config connection pool keep connections alive even idle least recreate problem keys pretty big see need fully display keys simple shift used select keys copy maybe copy transfer area button would better invalid information regarding environment see bug feature request understand use seemed worked thanks advance future time help. security issue high impact treat blocker. like remind everyone review coding conventions coding conventions going place recently please look like propose extend column limit columns recently reading following code followed even current coding conventions would lines ends lines doubled whole file like thinking extra makes cts act 00st century using wide screen lcd monitors let it's format code fit column amber text screens please it's worse i've found people actively breaking existing source code columns causing bunch unnecessary merge activities eclipse actually set types formatting rules send epf eclipse preferences file don't deal trouble please mindful cts thanks caring health. congrats everyone welcome apache cloudstack community good folks may officially mentors feel free reachout community anytime mentors admin guide well advise side develop work open beginning example start forking github branches share repos/branch i'll working community dev email i've yet already start getting source building play around ask things don't understand case issues first weeks gsoc bonding period understand research explore projects alongwith cloudstack bond mentor community people interact us irc ml attend weekly irc meetings local meetups possible good luck. interested know got original link posted saying don't listed somewhere i'll need get updated kind regards replaced email.addr.es find range cloudstack related services email attachments may confidential intended solely use individual addressed views opinions expressed solely author necessarily represent shape blue ltd related companies intended recipient email must neither action based upon contents copy show anyone please contact sender believe received email error shape blue ltd company incorporated england wales services india llp company incorporated india operated license shape blue ltd. shape blue brasil consultoria ltda company incorporated brasil operated license shape blue ltd. pty ltd company registered republic south africa traded license shape blue ltd. registered trademark. hi wondering reach hotfix branch merge thanks. thanks i'm sure mean escape cloudstack. trying think need add repo cs system build last head master branch bubble saw cosmic specific changes challenges road ahead. don't recognize gsoc fri may pm me replaced email.addr.es. ok thanks stephen makes difference code resource need change managed storage. daanhoogland don't figure rats build failing please help us"""
"""eb4a21a7dedf4a3b309af5d9dc0a1012b0a3a4a736db77d1d539917f19d0c317""","""currently initial patch seems work would like know whether overall code ok. added documentation properties configdef suggests sure importance assigned properties normal properties find info totally sure validations correct tried figure code still might miss something finally mailing list right place ask questions submit patch jira ticket get review even sure quality thanks help patch used specifying set expected configurations defaults override topic created altered defaults.compact compact else delete configuration s\ .format s\ .format check given properties contain log config names values parsed check given properties contain log config names values parsed parse values unfortunately validate smaller number replicas since information newline file. using something like nio-based frameworks value far concerned lot work evaluating nio-based general event-based i/o frameworks good old one-thread-per-conncetino blew everything else water performance wise. i'd like try latest t0.0 release it's built repository point able access thanks. i'd passivate null field string null interpret appropriately onactivate causes npe i'm passivating empty string instead produces url pagelinks render wrongly appear urls based current point web context rather root web better approach using passivate null and/or empty fields. lot presupposes method names going change new committment backwards compatibility. right attention given change api public internal package. considered using jackson objectmapper wouldn't seem really good reason implement plugin jackson reads/writes bson objects i'm sure would really nice tapestry json interface based concrete implementations wouldn't convert different wishing time work. damn forgot mention important stuff read tapestry. thanks moving wiki update post objections. merge you're discussions. pr going merged. pr changes success message box zeppelin it's dialog box pr tested try save blank values credential page breaking changes older versions needs documentation merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. question present conf taken default value configurations defined http//replaced.url thanks. hi moon many people use zeppelin data analysis work user often confuse paragraph stay pending running long time seems zeppelin working turns users running paragraphs time think it's useful provide dashboard show running pending paragraphs state easily see many paragraphs run. prefer plan merging two different r interpreters sounds like maintenance documentation headache users community feel specific additional steps technical development perspective need happen order merge know it's holding back merge technically history aside work community solve looking forward helping. restart failed job http//replaced.url. i'm committer easiest post rules file cwiki. hi rajesh last test hyper-v system vm probably august 00th downloaded hyper-v system vm template currently use worked fine mind could file defect look provide mechanism download build output. would love sink teeth assist code doc. bug node without schema bootstrap. recompile analysers use 0.x regards. hi lately using jsonoutput handle json serialization find jsonoutput.tojson overloaded pretty nifty however trying pretty print object calls look like it's pretty awkward opinion i'd like propose change following change would overloaded methods .tojson call boolean prettyprint legacy without boolean parameter backwards compatibility like. work unreliable links resume aborted partial files. know although give parsed results could use query parameters i.e q parameter debugging use parens fully qualify term e.g way_analyzed rue way_analyzed it's best. thanks helpful indeed debugquery get index omitnorms relevant field index without omitnorms relevant field get non-unit value thanks giving way reassure omitnorms really it's thing dive debugquery figure wouldn't seem much effect anticipated relevance. hello setup cloudstack work fine launch vm etc problem ssh vm host running vm ping exernal network download package setup lamp example i'm basic network mode config network config host /etc/init.d/networking/interface. hi looking issue migration vm volume storage motion fails root volume vm local storage issue localstoragepoolallocator :select gets called hostid included plan function ends returning local storage pools hosts cluster happening list local storage pools host volume i'data otherwise list storage pools cluster anyone aware check data disk added first place break scenario remove check regards. url http//replaced.url. resmo problem shipping '/etc/sysctl.d/cloudstack.conf packaging issue handled distributions people installing source don't seem easy problem. anybody decide log vmops.log anymore lot trace log hyperv module coming console log0j config don't changed someone made sure used instead default anyone. concern doc news changes files removed cf thread. far known it's possible replicate either pull push direction. couch_db allow custom system database names. hello would like call vote apache couchdb release first round encourage whole community download test release artifacts critical issues resolved release made everyone free vote release get stuck voting following release artifacts artifacts built tag subversion. would like define intrusive would clearly show whether exercise worthwhile big issue even something exists admit pushes discussion scope regarding release found http//replaced.url says implemented http//replaced.url seems update _rev key couch processes numbers json map function emits number key sends json couch stdio couch put index hypothetically couch either parse float instead b greater/less-than support speed regression think recall collation functions straightforward pattern matching could trigger alternate missing situations couch must parse number process re-encode i'm sure 're getting whole point make perfect json encoder store one.point.oh line printed stdio view server would contain one.point.oh javascript/json0/whatever free parse however likes. welcome great board. understanding original motivation release manager don't worry outstanding commit i've merged i'd his/her fault said make release distribute something got missed sort mechanism see commits maintenance release seems like i'd begging miss something said dropping bookkeeping development something like posting list commits wiki day two release people chance comment things would well think bottom line avoid saying oh hehe missed commit roll another release kthxbai release manager"""
"""23f365c7cc67a9c7e0569e37f4dc4a7569f5320786759c5b92b12e887d7c5fa4""","""following question particular considering extremely nasty release policy maven project maven-jar-plugin seen release eight months although plugin extremely important nasty bugs maven-changes-plugin seen release ui know long although plugin required migrating maven. hi mjar-0 mjar-0 fixed suggested brett releasing maven-archiver-plugin maven-jar-plugin thanks. already patch mjar-0 ready work someone volunteers work pull patches. pops question expect move regards. running attached test case might well added plexus-archiver whenever fixed demonstrates plexus archivers issue tracker file issue attach test regards. due release axiom thanks. hi two months ago vote held releasing maven-changes-plugin see issues raised particular concerning maven-changelog-plugin opposed maven-changes-plugin conclusion defer release reproduce called blocker anyways least imo question whether would possible release thing finally helps would glad required work. synchronized ibiblio normal write unfortunately belongs apsite group member imo apcvs committer access. believe misreading specification continuation defined otherchar newline otherchar defined anything nul cr lf words even continuation line lf allowed. thank. thanks issue created. quite amicable exchange hadoop developers i'd like re-submit following scheduled go march beginning gets released thanks. it's great resourceful person board. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. developed field generalized point think useful many cases aledsage grkvlt particularly welcome feedback y'all i've done similar things know merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. add documentation control catalog brooklyn karaf notes easily put different page don't live merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. see changes. sam think cassandra-0 deal breaker terms regression reading comments jira sam mentioned affects trunk i'm going poke cassaadnra-0 get done soon. propose following artifacts release 0.0-beta0 http//replaced.url artifacts well debian package available vote open hours longer needed. hi protobuf message it'sessionproto receive field recursive define data model cassandra store thanks. binaries lib folder objection piling maven central repository via ivy available maven central assume legal ok'ed they're redistribution originating apache sent android phone random spelling mistakes random nonsense words nonsense direct result using swype. hi believe lot interest developing ramp transactions cassandra concrete activity yet. problem got following error com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem0 standardmbeanintrospector.java:0 com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem0 standardmbeanintrospector.java:0 com.sun.jmx.mbeanserver.mbeanintrospector.invokem mbeanintrospector.java:0 com.sun.jmx.mbeanserver.perinterface.invoke perinterface.java:0 com.sun.jmx.mbeanserver.mbeansupport.invoke mbeansupport.java:0 com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke defaultmbeanserverinterceptor.java:0 com.sun.jmx.mbeanserver.jmxmbeanserver.invoke jmxmbeanserver.java:0 org.apache.cassandra.io.util.columniterator.next columnsortedmap.java:0 org.apache.cassandra.io.util.columniterator.next columnsortedmap.java:0. yea bgl0 green without impending risks additionally yellow projects lwr00 mtv00 path lead green coming weeks thats folks. see concern yes community continue discuss hopefully others join views. testing run. it's good question i'm sure preconditions work marvin cases know tests run generically say run copyvolumetoprimary sure test exists hypothetical moment gets run slew infrastructure configurations using local storage well shared iscsi ceph back ends dropping test storage suite give guarantee hitting different storage back-ends it's understand works today i'll defer prasanna sudha anyone else runs tests aggressively fill gaps make corrections. remibergsma swill test always failing environments don't good feels like accepting crippled system carried i'll spend time. hi thank reply sorry issue similar already gave looks similar. don't think analogy works difficult get acs running may granted know struggle made people move think analogous buying car dealer give keys prove change oil. hi prior break discussion move new doc format rst investigating process infra needed thought repo guide/book would good otherwise readthedocs build infra create massive pdf/epub think pages need decide guide meaning get admin guide instance hit http//replaced.url propose use. would disabling autodownload systemvm template simplify deployment cloudstack system although download url configurable value maybe property file thru api somehow via pre-install command really like download.cloudstack.org mirror i'm looking getting dns entry builds.cloudstack.org new jenkins got objections using cloudstack.org domain community services ii'm pretty sure would ok use download.cloudstack.org would setting current webserver website could. hi sure i'll prepare patch master used use git-format-patch recently i've using rbtools post-review difference patches generated post-review git-format-patch post-review produces diffs don't like probably remove recommendation use http//replaced.url thanks. don't worry like discussion don't mind losing argument best solution arises let it's see. thanks test. issues caused components really mocked instead loaded directly main source benefits spring integrates mokito nicely need probably base test configuration returns mocked daos common components unit test purely need finalize way write java unit test everyone follow reduce frequency/prevent happening i'll find time update unit-test guideline document wiki cleanup things area re-enable unit tests build. acs template delete button missing ui design would major bug folks operate public clouds. url http//replaced.url daanhoogland jenkins job kicked build packages i'll keep posted make progress. set trunk mode virtual switch hyperv provide us thank much. xs works expected system vms ssvm cpvm monitoring thread check running tries bring ps available deployment fails tried expected. sure swill pushed ran tests environment"""
"""3875138fac55dea58c60afbfb915d2f3c3cdd2304c980682a90a6b1811544513""","""try use temp view couchdb response temporary views supported couchdb status_code see temporary views supported couchdb0.0 documents changelog page broken http//replaced.url. lame mailclient changing titel every time reply hate tobit mailclient sorry good news found firewall blocking port server opend port screensharing loading //my_external_ip/webmeeting sadly cant start screensharing doesnt switch stop screensharing started connect //internal_ip/webmeeting screensharing working perfektly serveral ports found websocket thing wich load behind mod_proxy wss //internal_ip/webmeeting/wicket/websocket wicket-ajax-baseurl= wicket-app-name=openmeetingsapplication read proxy_wstunnel_module need first tests trying redirect webmeeting/wicket/ wss //0/webmeeting/wicket/ ends working openmeeting website regards. ok thanks. thanks try. thanks error ast_odbc_sanity_check connection attempting reconnect ast_odbc_sanity_check connection attempting reconnect. rtmps set application ensure close port change random value configs make sync red0.properties. try create need download driver work. i'll doublecheck. eric intention unsubscribe mailing list. thank help past appreciate writing storm application process alerts applications real-time need sort stream severity field called severity contain following strings critical high medium low debug need send streams critical severity specific bolt call bolt criticalseveritybolt looks like use topology create custom stream grouping using customstreamgrouping sure best way anyone examples either solution replaced email.addr.es http//replaced.url. thank response thinking using codahale it's metrics trident application wanted sure storm ui rest api useless case. hi looking way following solr somebody search show results category facet display results category along showing total number results category always using facet search kind overview search results user click category see results pertaining category way think making many queries categories show results category inefficient way thanks regards. hi change default location /tmp/lib using creating jar files cassandra uses java property java.io.tmpdir temporary folder default it's changed command line arguments cassandra -djava.io.tmpdir=/path/to/tmpdir launch cluster minutes. i'm geting nervous user data enabled network offering work upgrade i'm testing serious broken rohit patch. hi meantime i've stumbled across another seems originate either networksdaoimp.java upon update allocatevnet still looking it's message ke:0- proxy00.take:0-datacenterdaoimpl.allocatevnet:0-nativemethodaccessorimpl.invoke0:0-nativemethodaccessorimpl.invoke:0. multiple repos theses project use sphinx-doc might install spinx test output submitting pull request tables bit thanks. look devcloud0 host xencenter though says ssvm replaced email.addr.es wrote. agreed though honestly would agree decisions regardless whether user breaking features unplanned manner blocker don't fixed change broke reverted imo. comment added jira ticket thanks hi reproduced created iscsi volume san gb created disk offering storage tagged use primary storage lead creation gb volume executed attached vm first time volume vm took two hypervisor snapshots vm reverted first looked sr contain cloudstack volume hypervisor snapshots saw two snapshots active vdi see two hypervisor snapshots active vdi thanks replaced email.addr.es wrote. sheng slowbuild actually failing week last successfull build 0th i've looking findbugs reports look ok. publishing part fails either jenkins jenkins plugin i've reproduced similar job internal jenkins works fine i'll keep looking update. since directly etc indirectly thrown allegations since merged prs list prs answers merged case-by-case basis please keep replies technical specific pr please point revert needed http//replaced.url enough lgtms related change fix tested ui screenshot remi personally looked diff therefore merged http//replaced.url enough lgtms simple npe fix one-liner personally thought cheat given travis/jenkins passed merged http//replaced.url enough lgtms diff removed unused variable leading change constructor definition explicit integration tests necessary i'd simply dead-code removal simulator smoke tests passed travis/jenkins passed merged http//replaced.url enough lgtms change related marvin test adds new test methods need run regression integration test integration test result marvin test shared comment pr merged basis http//replaced.url enough lgtms regression tests results shared attachments daan case someone missed merged http//replaced.url enough lgtms regression tests results shared attachments daan case someone missed merged http//replaced.url enough lgtms regression tests results remi merged http//replaced.url enough lgtms text changes api doc-string merged given travis/jenkins passed http//replaced.url enough lgtms npe fixes explicit integration tests required given travis/jenkins passed http//replaced.url enough lgtms simple java oop fix travis/jenkins passed merged i'm aware codebase http//replaced.url enough lgtms changes would require manual tests wrt usage server etc well confirmed comments seen regression test result new/modified marvin test wrt feature merged regression test suite include among tests http//replaced.url enough lgtms findbugs related fix travis/jenkins passed findbugs mvn job result shared confirm fix works merged regards shape blue ltd company incorporated england wales services india llp company incorporated india operated license shape blue ltd. shape blue brasil consultoria ltda company incorporated brasil operated license shape blue ltd. pty ltd company registered republic south africa traded license shape blue ltd. registered trademark email attachments may confidential intended solely use individual addressed views opinions expressed solely author necessarily represent shape blue ltd related companies intended recipient email must neither action based upon contents copy show anyone please contact sender believe received email error find range cloudstack related services. rhtyd agree moment already intermediate state projects moved maven standard know utils head ovm0 plugin created reason move project going create test resources don't put resources production code. thanks answer makes bit sense. kelcey technically gsoc project progress. create jira issue make noise. thanks rohit found vmops.log assume management server log devcloud0 command run management server mvn -pl run background add make another command using run management server. hi thanks please squash commits rebase current master make sure commit i'll able test. i'm sure else commands i'migratecommand actually execute addition it'start/stop/copycommand include i'migratecommand' consists commands thanks replaced email.addr.es wrote. agree general lines case test succeeded far started failing lot since days something definitely changed looked implemetation test see root cause yet look important bit integration tests maybe misleading see functional enduser tests api users ui users integration tests sense don't expunge thread see vms expunged certain time fact use case mention alternative alternative another usecase tested solution sounds fine. hello likitha thanks test cases fail exception raised moving vms across subdomains different domains problem test case uses api client root admin privileges instead domain admin therefore raise exception regards. hi good day thank reply yes guest vms reach vr able ping yes vms really configured use vr dns server according /etc/resolv.conf together /etc/resolv.conf guest vms yes remove comment first nameserver entry vrit's able resolve domains properly may know enable dnsmasq debug documentation steps believe vr looking forward reply thank replaced email.addr.es wrote. thanks i'll try address p.s. might trim best"""
"""7a6ddb98f89bc8f6a761a842d9297a7972e1ff5e70a7321b7610bc6f65472b87""","""hi heard wrails project sorry senro project senro http//www.senro.org like wicket rails check http//replaced.url glad copyright note still comes perfect timing complete beautiful week open pd sorry bother stupid things needed catharsis action. tag fix issues clientdataencodertest merged back branch component fixed broken tapestry.png 0c000af tag fix tests broken recent change 0b0badd use application root package application commit looks like could related issue experiencing. use case fwiw using couple block contributions used components used ajax stopped working introducing main really handy component automatically rendering really clue fix shall love working thanks denis bringing discussion. problem maven latest versions think using maven use maven build tapestry0. projects taken extreme allowing index pages context enforcing rule code workaround trick pagerenderdispatcher skip index pages adding new dispatcher pipeline forces empty_context index pages module http//replaced.url btw talked like months ago http//replaced.url. maven team pleased announce release apache parent pom pom contains settings likely useful apache project building releasing code maven specify version project isparent configuration wish parent asf-parent. maven team pleased announce release maven dependency dependency plugin provides capability manipulate artifacts copy and/or unpack artifacts local remote repositories specified specify version project isplugin configuration. someone interested build fails run parallel mode -t0 could reproduce failure local machine someone interested discover goes wrong parallel investigate wanted false positive asf jenkins regards. development apache maven promotes use dependencies via use default super pom version requirement expect near future update minimum requirement maven jre permit make cleanup jdk0 compatibility status latest informations available wiki plugins fixed quarter maven-eclipse-plugin wagon surefire components yet preparing enable rat check every build next maven-parent pom ensure full compliance currently change cause real guide line definitive descriptor plugins changed accordingly means every new release contains appropriate download link every release reported quarter contains already issue closed report last status topic apachecon europe talk developers said attending expect meet discuss maven future plans users mailing list activity reduced little bit last six month comparison whereas developer list activity less constant within time period. apache maven team pleased announce release apache maven plugin generates report regarding code used specify version project isplugin configuration causes parallel build failures would nice see check together output violations wish. maven team pleased announce release apache maven site maven site plugin plugin generates site current specify version project isplugin configuration download appropriate sources etc download page msite-0 use property items child module site msite-0 inclusion resources basedir velocity used. apache maven team pleased announce release apache maven maven plugin tools contains necessary tools generate rebarbative content like descriptor help documentation specify version project isplugin configuration broken using tags running twice skiperrornodescriptorsfound. apache maven team pleased announce release apache maven maven plugin tools contains necessary tools able produce maven plugins scripting languages generate rebarbative content like descriptor help documentation maven plugin plugin used create maven plugin descriptor isfound source tree include jar used generate report files mojos well updating plugin registry artifact metadata generating generic help goal specify version project isplugin configuration wish metadata create new version descriptor. apache maven team pleased announce release maven ant find binaries find release notes. brings eunit tests 'config top level pr apache/couchdb-couch pr follows commit patterns part unfortunately actual tests suite disabled odd bug 'couch_log updates made 'config don't able track yet 000f000 added list deps start stop every tests resulted unrelated test suites failing mysteriously way currently start deps 'test_util start_applications hack clean properly i'm huge fan approach 000f000 least properly cleans merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi i'll usual meeting couchdb-meeting meeting room access meeting via web specific topic talk reply bring irc. hey everyone sorry long delay got help coworker two us fixed issue systemd service put attached files directory couchdb directory get building run snapcraft snap get ~00mb couchdb_0.0_amd00.snap whatever arch 're installed snap install couchdb_0.0_amd00.snap force-dangerous get running couchdb instance http//replaced.url see attached screenshot force-dangerous needed it's local untrusted file it's published snap store don't needed user install simple snap install couchdb it's configured put local.ini couchdb.log snap_data /var/snap/couchdb// actual database files snap_common /var/snap/couchdb/common/ first forward-copied every time install new version second unversioned don't duplicating large database files upgrades i'd like get upstream produces working snap improved needed based feedback users replaced email.addr.es. well googled basically couchdb_plugin.erl tells erlang behavior looks like define functions arity couchdb_plugin would export probably better erlangers list might chime looks like okay bang-for-buck much bang much buck. yes committed thanks adam. usually people subscribed reason important mails need let whether legitimate users forgot subscribe apache members notifying us things occasionally someone company like cloudant replies thread may made internal list stuff like. getting loc conflict rebuild time use base"""
"""d3bbe7127318e1711f6fb3991caba157662051ce4fb2400f6db294afca2bb977""","""looks overengineered types zips jar either. think looking maven embedder. horribly afraid right documentation says b defined b isparent isdependency management section since dependency management selected referenced ispom b compile scope thanks. aware change intentional conforms poms procedure described docs nothing nothing less. hi release preps constantly failing build maven mvn -preporting site help simply means actually tried read jar file skin received classes directory target traced proceed debugging debug symbols match source code anyone clue system tries resolve instead local repository highly assume sonatype eclipse aether issue insights welcome. already. hi folks anyone already noticed plugin http//replaced.url might automate test pr jason wants thoughts. convention way omit last zero done maven plugins/components. thank offer best time go channel. ok thank process sites later day. canceling vote due problems pom files herve found shall respin vote today. hi folks would like patch several issues jira unfortunately right change tickets e.g target version component forth anyone able promote thanks. cause problem dependency transitive dpmgt section read/compared mdep would least expect warning anyone else. noticed work therefore asking last 0.x release branch intended able cut release permissions promoting release thanks. hi still couple issues left jira note release depends release skins parent. updated good possible. think special idea mind could read snippet retrieve bytes specific encoding compare guessing encoding tricky would require icu0j course user supplies wrong encoding lost. done http//replaced.url completely disassembled tarball build maven theirselves non-canoncial build likely going support modifications known us rather untar offical tarball rebuilt package though grab tarball internet somehow. hi folks recently proposed mskins-0 downgrade 0.x two reasons updated site.vm custom.css bootstrap yet knows real changes model change must happen patch version received feedback neither positive negative would like perform downgrade site rendering work expected additionally made several improvements css rendering stuff locally parent project would like merge back plugin bring site plugins let us postpone bootstrap 0.x fluido skin 0.x. able restage tomorrow working patch doxia-0 would like commit. model would require maven cleanup env like m0_home .m0 m0.conf forth otherwise break current limitations without. hi barrie docs definitively correspond poms moment chapter staging latest documentation says release prepared simply wrong release prepared performed documentation staged due missing next problem made push pom back forth simply work scm publish bound parent site-deploy phase site stage-deploy goal scm-publish publish-scm works know whether correct verification kinda awkward says wait sync linked site say anything sync given timeframe sync occurs path /www/maven.apache.org/pom exist /www/maven.apache.org/content/pom moreover directory skins updated skins-0 directory exists pom-archives skipped steps stuck lost help would greatly appreciated. hi devs promote staged repos skins parent fluido skin gone happened auto-dropper active rao source releases already promoted herve thanks. hi robert back october result retirement vote positive plugin still without notice issues still open jira repo github simply. currently running patched version maven offensive commit reverted surefire master freebsd 0-stable shall see wether cause hour. thanks fix added purpose resembles output site tool generates modules menu looks awkward outputs uniform. file jira ticket. hi brett thanks promotion seems like edit tickets old created ages ago able drop completely thanks. file issue. quite annoying took quite time figure correct settings git cmd egit even know whether got right subversion made way easier stackoverflow good help place anyone better cross-os setting. yes resposibility always good simply make build fail instead log collision happens. assumption yes. located lifecycle phase maven-plugin. though minor doc issues related msite-0 downgrade advise velocity noted caution guarantee velocity request tools properly work additionally filtering section updated context variable makes easy display dotted maven properties. hi herve seems like entire process requires another sign fortunately already jira account xircles account wrote codehaus support asked transferring account intend recreate jira account created many tickets current thanks. hi tibor currently running full test bed various maven versions surefire master pass log files along target directory shall join channel. pages missing apt files remains empty maven actually fixed. hi still couple issues left jira. hi folks performed another cleanup jira last couple days old issues right open issues manageble point view noticed several plugins touched years status following plugins anyone working planning release version thoughts objections would like retire clean jira plugin plugin. already known issue year http//replaced.url. guys agreed reuse tags apply procedure tomcat team staged build fit abandoned tag remains relased added change log http//replaced.url see version opinion release notes shall mention skipped skipped. first thing came mind. nobody retiring fluido skin subject. indeed mngeclipse jetty gone years codehaus support assitance thank much"""
"""05e13208ea5297172044cb524152d78290e0894d7aa7ed77a8b683ef1833f8b9""","""way query table based binary column bytes proofpoint inc. sql query looks part like sometimes returns port values ip column ip values port column delete group first/last sections seems work fine run query cores runs fine fail part number cores effecting bug makes pretty sure israce condition along fact printing values changes bug have looked client code not seen anything obvious occurred suppose could server code anyone else seen anything like proofpoint inc. done. fine would like note first announce publically users mailing list think deserves major bump plugin version wdyt. hi changes since last release. would favor move java make strong use nio0 file operations lot pain go away. could announce least five days advance would give least timeframe merge prs handle issues. think must tandem packaging zip finally. yes works stupid fault simply noticed use secured url used read-only url sorry noise. yes maven mvn -prun-its install build.log files say updated msite mpir projects touched subversion. would drop altogether go. hi paul good idea done already pre-0.0 versions going think strictly follow no-fix-version-for-incomplete/invalid/wontfix able assign real versions great. hi staged site http//replaced.url. rather add plain text release notes html junk unreadible. not course solve problem pre-existing files give clean way things right new files. given companies/folks react something discontinued would move java baseline christman first release java e.g. policy make apps run java months even compatible not concern. help becuase repos located github apache. non-binding. cloned tested upgrade msite pain go away parse/ include. possible ran uts twice check dep tree mediated. thanks announcement actively providing several years finally able contribute officially directly thank. confirm tried wagon 0-snapshot maven master. glad found documentation hard would like repeat information serveral places need update maybe interpolation issue please open jira issue thanks. hi vote passed following result promote artifacts central repo pmcs please promote source release zip file add release. hi folks in-house project jcl-over-slf0j nothing special far module dependends net.sf.michael-o.dirctxsrc dircontextsource jar:0 compile slf0j-simple test scope mvn dependency tree properly gives master gives questions another fix master relied erratic behavior core previously depmgmthave influence transitive deps direct really say right wrong two slf0j bindings slf0j simple logback obviously wrong thoughts. please reply promotion http//replaced.url would like manage release like codehaus jira thanks. insane works thank much need fix fail w/ maven 0.x due aether advise must raise mpir msite mksins-0 work anymore may others. yes http//replaced.url obstained merge patch version minds course. hi folks seems like full access maven directory see anyone able check error correct permissions username michaelo thanks. thanks summary helpful keep everyone loop pmc don't decided particular dev agreed last week nice blocker fix asap don't make include fix shortly document situation welcome efforts tag team issue don't hold. hi passing since=n parameter reconnect changes feed feed=eventsource think changing format changes feed suit eventsource spec don't change. sorry follow jan probably post new email summarising result thread consensus move forward along minor details plans. configure work without flags. group_level rejected single key specified. servers testing working 0.0b000000 weird 0.0b000000. thanks testing used approaches problems please see thoughts commit messages. hi still problem ran things try code http//replaced.url trying login still get reason password someone try runs machine missing something stupid. don't think case clear cut paul points certainly borderline would like think devs think. thanks feedback couchdb github development happens http//replaced.url wholeheartedly agree lot work went allowing us make stable releases faster said world framework week 're database means things different pace couchdb responsible many magnitudes terabytes data core many crucial business civic infrastructure healthcare applications users trust couchdb hold data reliably it's responsibility seriously things happening deliberate necessarily slower pace realise makes couchdb less attractive casual contributor make better areas see relevant think collectively appreciate slower pace everyone enough time keep everything make sure everybody chance think new code thus reduce amount errors shipping best. someone create us. minimum get auto-apply patch service going i'll try soon. hi thanks answer javax.xml.ws.soap.soapfaultexception object reference set instance object instance object. hi i'm confused correlationid generated configuration piece follows think rules generating correlationid follows it's left side condition it's right side rule generation correlationid usercid means client gives correlationid conduitselectorprefix means client configures wsdl useconduitidselector means client configures wsdl conduitid means string generated jms-tranport it's jmsconduit index autoincreasing number indicates uniqueness right use rules generation correlationid best regards. thanks freeman looks fine yes it's bit tricky make sure works across jax-rs providers especially custom mbrs expecting inputstream null. don't worry typos happens time confirm show annotations retained runtime don't deal it's job ejb factories/proxies/etc deal thing need provider ask container-specific ejb factory/etc create release instance see may work principle please accept cxf manage instantiating ejb resource classes delegate ejb factories may keeping pools thus don't need reference jaxrsserverfactorybean fact don't need object reference ejbprovider resourceprovider impls reference ejb resource needs passed constructor definitely need cache create objects reason thought debugging would help. don't outstanding messages case could go ahead terminate sequence messages add terminate message db upon restart similar messages would sent terminate sent thoughts. yeah think good idea put cxf dosgi separate project jira thanks. hi place need know cause soap fault org.apache.cxf.databinding.source.xmlstreamdatareader saop fault throw don't know exactly error cxf server put production client developed people outside company difficult client trace real error cxf don't send cause exception back client lots place cxf throws fault/soapfault message changed fault. i'm going start omnibus thread. could please raise jira thanks. usually spits java version found. thanks fusesource part red. stems discussion back guides live built site intention would move simply need copy code branch check site branch got tired reminding people keep synch stopped. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi intellij basically pain generated code stuff way typically deal use intellij go root directory go intellij reimport whole maven project able see everything may go project settings make sure sources generated avro maven plugin included sources module uses sdk core hope helps let us know continue run trouble"""
"""b88e5ab8b78d81d5d2641e4ea1e017560904a5a481c38692833f255be2bee640""","""deb package rpm. cassandra testing using old server core celeron processor 0gib ram another 0gib cores two consumer sata hard disks works i.e memory error etc writes reads second maybe database extremely small even days megabytes configuration absolute stock configuration changed anything except noticable node small server remember somewhere hand noticable interesting disk higher log hard disk contained system data disk grain salt intention test setting cluster two distant datacenters performance test. thank mail restarted affected server noticed mail likely related leap second introduced today. yes column stored value every key may matter switch compression afaik advantages default worried storage space mysql table intend move cassandra columns long column names average characters column values mostly byte integers hand many colums empty specifically null afaik mysql able simply store column value default table data without indexes mysql gb millions rows cassandra gb without compression gb compression includes single index row compression switched specific case storage requirements roughly cassandra mysql sorted key sorted column. opposite true requests fast slow case percentile fast slow except order samples opposite direction usual. appearance old rows caused old timestamps set columns turn caused threadlocals cleaned since fixed timestamp rows returned corresponds latest saved state every case. requirement nodes unique token still global cluster/ring node needs unique logically seperate rings networktopologystrategy puts rest code. thinking frequent example theory using tokens dc0 dc0 significantly affect key distribution specifically two keys move next much however seems unexplained requirement least could find explanation nodes must unique token even put different circle networktopologystrategy otherwise data evenly distributed. use propertyfilesnitch networktopologystrategy create multi-datacenter setup two circles start reading page http//replaced.url moreover tokens must unique even across datacenters although pure curiosity wonder rationale behind way someone enlighten first line output nodetool obviously contains token nothing else seems like formatting glitch maybe role. played test cluster stopping cassandra node updating row another noticed delay delivering hinted handoffs know rationale node originally received update noticed server waited started pushing hints log endpoint. noticed strange phenomenon cassandra would like know something completely impossible see log extract new versions row written reads returns obsolete data read version even already written single cassandra node cluster client local network rows written read seconds would think test environment see obsolete data actually thousands log entries hours test say row read match latest data written checked detail history another node seems eventually receive up-to-date row took minutes specific case fyi started evaluate cassandra without significant. copy init.d datastax package. hi think you're good put vote bits needing discussion updated migration section kip please look receive non-null message step correct many thanks mayuresh don't objections receive non-null message step correct specially like part mentioned get rid older null value support log compaction later don't change semantics message format without long people reading documentation acting warning something fundamental need bump magic byte don't support versions forever said support direct upgrades years means message format version could theory removed years it's introduced heads would like mention even without bumping magic byte loose zero copy client x+0 convert internally null value tombstone bit set tombstone bit set null value automatically internally time move version x+0 clients would upgraded obviously support request consumer x loose zero case magic byte reasons explained ok since going meet side note update doc magic byte say bumped whenever message format changed interpretation message format usage reserved bits well currently magic-byte corresponds message.format.version broker set initially producer client sends message magic-byte since broker magic-byte convert means tombstone bit set value set null consumer understanding magic-byte still work consumer magic-byte able understand since still question supporting non-tombstone null value producer client magic-byte sure almost clients upgraded message.format.version broker changed conversion step happen point get consumer request older consumer might convert loose zero copy cases rare becket review plan add details missed/wronged something put kip thanks guys discussing getting consensus clear others proposed think understand make sure could ask either directly update kip detail migration able support non-tombstone null value kip noted discussing kip logic based null value don't clean separates concerns discussed already though split kip-00a kip-00b look deliver kip-00a compacted topic address discuss kip-00a completed options later renu mayuresh discussion following brief agreed bumping magic value may result losing zero given bumping magic value almost free benefit avoiding potential performance issue probably worth issue still need think whether support currently supported kafka allow non-tombstone null value message exist kip-0 problem message supported consumers prior kip-0 null value always interpreted tombstone option keep current way i.e support message would good know concrete use case message probably support something think migration plan migration plan look something like currently lets say broker version x move version broker x+0 supports tombstone client x+0 value set null automatically set well good producer client x broker tombstone bit broker x+0 supporting consumer client x+0 aware able handle consumer client x convert message broker point specify warning clearly docs behavior changed log compaction next release broker x+0 say tombstone used log compaction broker side clients x+0 still null tombstone set automatically client worried go magic byte route unless something sounds like kafka stuck supporting value tombstone bit log compaction life long it's good point might considered earlier plan think stage broker converts message stage next release say log compaction based organization moving release sure goal kafka clearly specifies log compaction means null value tombstone don't bump magic byte broker side always look tombstone bit value imagine broker sees message set broker know message produced message converted value see null order determine tombstone logic put consumer well consumer know message converted understand correctly sufficient messages appended upgraded include kip-0 information contained email strictly confidential use addressee unless otherwise indicated intended recipient please read copy use disclose message attachment please notify sender email telephone delete copies opinions conclusion etc relate official business company shall understood neither given information contained email strictly confidential use addressee unless otherwise indicated intended recipient please read copy use disclose others message attachment please notify sender replying email telephone delete email copies opinions conclusion etc relate official business company shall understood neither given information contained email strictly confidential use addressee unless otherwise indicated intended recipient please read copy use disclose others message attachment please notify sender replying email telephone delete email copies opinions conclusion etc relate official business company shall understood neither given endorsed ig trading ig markets limited company registered england wales company number ig index limited company registered england wales company number registered address cannon bridge house dowgate hill london ec0r 0ya ig markets limited register number ig index limited register number authorised regulated financial conduct authority. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi contributors apache spark looking rat catching licensing errors codebase git log clear kafka apache rat build later removed clear alternative taken information would helpful sense learn experience. see changes. hi according github page kafka requires gradle higher. however try build kafka using gradle don't work com.github.johnrengelman.shadow plugin version used kafka requires gradle seems already jira http//replaced.url wondering supporting gradle higher intentional bug readme file github bug gradle build intention continue support gradle could someone help clarify thanks regards. ah thanks much insights position profile new library real data next week i'll let know goes. agree makes sense cover zk-based topic creation topic creation policy limit zk access brokers going forward point need way disable zk-based topic creation topic creation goes topic creation policy specified kip-0 make sense example solution add broker-side config defaults true user overridden config false controller delete znode /brokers/topics/ topic created controller probably need trick differentiate znode created controller znode created outdated tools example new controller code add new field iscontroller znode /brokers/topics/ topic creates new znode znode don't field child znode controller sure created outdated tools remove znode zookeeper users using outdated tools create topic find topic created"""
"""ddca80046db3358a64436ad45a3fcd95c3e42bd773e997bc5acc2d6892cb3795""","""open issue related matters seem received much attention regards. simply list dependencies report. hi folks actually proceed third-party plugins comply naming pattern given plugin created document beeen first published simply add disclaimer legacy reasons plugins central created date. question obviously. infact added introduce maven.conf m0.conf mng-0 make conf modification /etc /usr/local/etc snap. yes shame simply forgot agenda already ok. think credit goes github eases participation non-committers tremendously though need improve pr process mirrored repos. considered running versions display-plugin-updates. open issue infra inquire default pr procedure look like mirrored repos github think need common approach entire asf. would like fluido reference parent released yet ok actually moreover found another bug fluido would like report fix. think variations depenencies must go either classifiers build qualiafiers start allow arbitrary elements people start ask custom elements x. therefore custom elements allow plugin s. working sample project attached already please check. thank much bringing personally disgusted stupid names proposed like boom shotgun anything else related rifle arming. proverb not make think rm mono-module multi-module project he/she needs know perform steps fine relase docs scattered several pages references would like avoid another exception. might added http//replaced.url. thanks turns something wrong repo r.a.o able browse yesterday try browse nexus ui get http repos broken file issue infra. opened issues least fixed. thanks guys look http//replaced.url next couple days process required. thanks change poms accordingly. forgot always use relative path docs otherwise could misinterpreted. aware retirement plan procedure go anyawy post list first figure plugins really dead sorry part. opposing previous proposals really see urgent need animal link look meaning maven clearly see refers well-educated human think needs evaluated though know represent graphic. note maven-0 repo probably typo. think fully correct leave goals defaults distributionmanagement/site element perform site-deploy default check m-relase-p docs say. ultimately mean plugins run maven someone still uses older version java build projects. big question would like clarify first commons compress gone java minor release people requesting. head idea mind several months make clean cut read period new issues asf jira someone found issue already codehaus jira migrated course. hi folks would like another bug squash like last year currently unresolved bugs updated three years hardly believe updated ever maybe resolved new versions automatically please look whether see still anything else object go ahead close fix let know week. avail still error leave profile reporting. see changes fetching changes remote git repository fetching upstream changes http//replaced.url honour jvm settings build new jvm forked please consider using daemon http//replaced.url downloadwrapper up-to-date honour jvm settings build new jvm forked please consider using daemon http//replaced.url went wrong run stacktrace option get stack trace run info debug option get log output. hi got yes case fall back corner case handling failed replicas recreated another available log directory later failed log directory fixed existing data broker startup detect duplicate replicas fail starting let admin fix issue manually optimal sure common case though overall wondering need storing created flag replica seems useful uncommon case common case log directory fails broker running tradeoff don't optimize uncommon case need write failed replicas instead successfully created replica zk former lot less latter thanks. attempt fix kafka-0 appears problem broker closed connection passively client react appropriately networkreceive.readfrom rather throw eofexception means stream reached unexpectedly instead return negative bytes read signifying acceptable stream selector channel passively closed don't try read data don't try write close key believe fix problem merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. done. it's good idea past major new features tend create wiki page outline design wiki pages organized better looking thanks. wiki user. please don't let analyzing pr away actual working time spare time it's intended away working hours may well non-issue states added equals method stamped.java implements comparable always pointed intellij it's code analysis warned classes implementing comparable always implement compareto equals end-user point add objects java.util.sortedset compareto equals implementations consistent would violate contract java.util.set defined terms equals findbugs complained punctuationscheduler subclass stamped nothing equals implemented equals stamped.java written consistent already-existing compareto works hashcode autogenerated timestamp field used compareto equals findbugs complained punctuationschedule.java subclass stamped.java needing methods stamped.java equals hashcode auto-generated merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi thanks sharing experience enabling tls clusters helpful comment far know openssl reduce tls overhead still lose zero-copy optimisation attempts making possible retain zero-copy tls kernel probably early us consider kafka http//replaced.url. super useful thanks jason. i've fixed function slightly predicate needless ported verifiableproperties it's really try arround already utility function utils.parsecsvmap str doubt worth moving simple try-catch separate function hope resolves. hi see error logs running zookeeper server don't action error shutdown server state changes org.apache.zookeeper.server.zookeeperserver:0 zookeeper server don't action error shutdown server state changes org.apache.zookeeper.server.zookeeperserver:0 make sense fix providing shutdownhandler best wishes. hey manikumar clientid don't unique clients clientid share quota quota management merged trunk enabled given company kafka clients company explicitly specify clientid. hmm correlation application level information kafka request maintained o.a.k.c.networkclient associated tcp sessions even tcp session disconnects reconnects correlation reset still monotonically increasing maybe make clear suggesting anything relying tcp transport layer everything handled application layer clients perspective timeout defined tcp timeout defined upper bound time wait receiving response client receive response timeout reached retry suggestion long fetchrequest needs retried matter reason use full fetchrequest depend networkclient implementations i.e regardless whether retry existing tcp connection new question trying answer essentially let leader followers agree messages log comparing following two solutions use something like tcp ack epoch request/response level piggy back leader knowledge partition level follower personally think better direct leader maintains state leos followers day leader wants make sure states correct directly confirms states followers instead inferring epoch note subtle maybe important difference use case epoch tcp seq difference tcp ack confirms packets lower seq received case high epoch request mean data previous response successful kip statement leader receives fetch request epoch knows data sent back fetch request epoch successfully processed follower could tricky expensive make right cases sure considered thinking comparison following two potential issues came thinking case consumer consumer.seek consumer.pause called consumer essentially updated interested set topics positions needs full fetchrequest update position leader thus create new session users call seek /pause often broker could run fetch corrupted messages fetch response corrupt message follower back try fetch back period follower fetching partition corrupt message back partition added back current design seems follower need keep creating new two cases might still useful let session unique client instance like producer idempotent produce allow client update leader side thanks. hi thank kip become really useful thing scanned discussion far wanted start thread make decision keeping cache connection session sort uuid indexed sorry settled already missed case could anyone point discussion rather see protocol client hinting broker going use feature instead client realizing broker offered feature regardless protocol version indicate feature would usable seems work better connection/session attached metadata map could allow would make client-side code easier would don't thank kip clarified already please drop hint could read best jan. yeah seems reasonable well even places add info seem like really useful ones enum generating lowercase came kip-0 first expose info technically enough require change change still simpler/easier multiple calls might reference kip reference call specifically enum already lowercase always compatibility consumers api probably make assumptions capitalization. hi new open source contribution get started apache kafka anyone help find bugs work mentor ask help thanks advance regards"""
"""4ce4268ef0b844ab722cf9bdcc579e412df2a4bf8d392d4743e1b92839384627""","""read storm-0 http//replaced.url question work hdfs state trident topology ensure record ends written hdfs states none. trident topology processes tuples opaque kafka spout three bolts write different forms processed output form different record parallelism rotation policy applied three outputs results two streams writing small files hdfs m thinking setting different parallelism outputs reason different parallelism settings across output bolts. trying understand relationship emitted transferred values spout trident topology multiple instances topology executing expected ratio emitted transferred would least similar instances topology differ quite bit example 0x another unit measured tuple tree storm topologies trident batch. thanks helps answer question batch fails could records batch get written disk hdfs state another. unfortunately wild card search terms get processed analyzers suggestion fairly common make sure lower case wild card search terms issuing query. use solrj dynamically created core capability know advance cores require almost always complete index build previous instance index needs available complete index build two cores index switch required indexing run issummary re early prototype implementation right production quality code tell voluminous methods identify core exists create method instantiates two solrserver objects solr indexcore requires create two cores indexname already exist live core used searching second core used indexing indexing two switched just-indexed core become live core way core works live core always named indexname indexing core always named indexname _index datadir core alternate indexname indexname create core already exists returns true new core created false otherwise solrj provides direct method check core exists getstatus index clearing first complete rebuild various logic submit batches could adjust core index complete reindexes need index always searchable hope helps. good answer may depend wanting restrict 000k documents seeking reduce time spent solr determining doc count wanting prevent people moving far result set case display digits return count solr performing adequately could always artificially restrict result set solr actually return 0m documents returns number specified query well cache next results anticipation subsequent query total count returned exceeds 000k report 000k number results similarly restrict far user page results sounds like sort results descending post date fact get recent ones coming back first. think client-id useful grouping quotas even secure clusters clienta user0 treated different client-id clienta user0 grouping clients user enables users allocate quota effectively clients guarantee critical event processing clients throttled auditing clients kip down-sized support user-based quotas hoping could extend later time enable hierarchical quotas understand confusing switch semantics quotas based modes set brokers let try promote original kip-0 time flag revert existing client-id behavior maintain compatibility perhaps necessary sound quotas may configured users sub-quotas may configured client-ids user quotas may configured client-ids users users don't quota override allocated configurable client-ids without sub-quota override share remainder user quota user quota limit default quotas may defined user sub-quota configuration client-id matches current quotas client-ids user quotas set authenticated users multi-user cluster currently possible define quotas client-ids multiple users longer supported. see changes kafka.common.kafkaexception socket server failed bind localhost:0 address already use java.net.bindexception address already use java.net.bindexception address already use went wrong run stacktrace option get stack trace run info debug option get log output. hey gwen yes approach much better described solves objections don't think mm rather original producer don't think adds overhead beyond additional header bytes i'm mistaken since mm already unpacks repacks messages unlike broker tries avoid repacking hence don't efficiently add header think objection around likely solvable kind aliasing scheme guys think made work think two pretty credible use cases tracing family uses plus number things least moderately improved i'm convinced. hi everyone feedback suggestions welcome thanks. hi pac0j vision direct clients ldap authentication example currently supported knox pac0j gateway two components authenticator validates credentials profilecreator create user profile default relies data returned authenticator means kind authentication assume two identity sources login get attributes currently supported knox pac0j gateway component client represents authentication mechanism done via identity source additional mechanism authorizationgenerator attach client compute authorizations login user profile retrieval fact could even use switch identifier user profile attribute thanks best regards jerome. basically test it's module run mvn -dmaven.surefire.debug -dtest= method-name test test wait debugger connect details. discussed comment. thanks himanshu done changes imposing restriction required weights side-effect couple test cases blowing restriction fixing xml it's post patch. nice another plugin moving forward. following comment added issue sure i'm looking forward thanks quick response. vote passed http//replaced.url replaced email.addr.es wrote. don't move makes easier build maven-plugins it's already installed. sure it's good idea i've found quite bugs related boxing projects example auto-unboxing field sometimes null may cause unexpected npe it's always obvious causing npe it's implicit code using integer using int vice versa remove explicit boxing usually many compiler warnings wade using suppresswarning auto-boxing justified work keeping explicit boxing/unboxing. thanks info refactoring imply api changes expect major changes alpha-0 sorry miss eclipse ever europe let know could don't meet eclipse summit europe eclipse project means license work never. could easily become problem example docker maven carlos maintaining bring in-project released maven project bundle jre maven project publishing could include jar known jsr-0 docker i'm sure annotations retained bytecode counts don't even started poor licensing classes jar least source headers x i'd need double check think x might bad. discussed couple lists already consensus projects i've involved issuing pull request github clear enough intent contribute long pull request properly forwarded right dev list think mailer setup ok several projects grabbing fixes via pull requests. hi i'd suggest use jira it's attached testcase part commit testcase small reflects problem well making part commit ensures 're fixing right issue. subscribed wiki page wiki category maven wiki change notification following page changed. new issue created jira. course. issues seems like days work quoted http//replaced.url quoted http//replaced.url. part still puzzles i've published new docs strange menu see don't match ii'm time work plugins page linked 0-alpha-0 archetype plugin docs many people still using make easy find sets documentation code published archetype project docs thanks raphael. actually proof concept don't finalised moment though possible probably avenue right check sources try thanks interest. cool thanks. 're going try cut release works 0.x would don't worry updating scm think users git likely m0 scm issues yes cut quick release 0.x users would try leave many existing deps alone. don't work snapshots iirc. following issue closed"""
"""804a7d6bac6f39e9ab931b81ed7b0092838d087a9ab4a1ba46520d1a84533188""","""hi sam android apps created internal hobby projects connect couchdb via replication preload database remote instance documents add local instance replicated back remote instance use basic add authentication hardcoded considering use accountmanager make couchdb listen interfaces mount sdcard edit local.ini would normal pc would rely apps though since expect people installing app. curl http//replaced.url couchdb welcome version congratulations violence last refuge incompetent. possible solution might simple create virtualhost apache0 points particular documentroot file /etc/apache0/sites-available/couchdb.example.com allowoverride none sudo /etc/init.d/apache0 equivalent system /var/www/couchdb.example.com/htdocs/ following content. hi message would like accomplish following use flash access data couchdb requires couchdb serve file called crossdomain.xml flash application load first verify allowed flash requires file located root http server e.g http//replaced.url accomplished using virtualhost url rewrite options couchdb offers described earlier nicolas orr blitz.io neat app describes follows tried works beautifully edit domain.com zone add record blitz.domain.com create blitz.txt file put content blitz told put upload blitz.txt attachment design document get content blitz.txt attachment ok test couchdb hit root couchdb reports version however put rewrite couchdb complains many security issue fair enough 0x thus design doc looks like instead blitz course need figure hostname couchdb database would like access e.g flash.example.org make sure hostname actually resolves machine access i.e runs couchdb edit virtual host part couchdb reflect choice e.g restart couchdb afterwards think even necessary add entry futon use couchapp way like create design document called _design/flash contains crossdomain.xml file attachment rewrite statements much like mentioned curl http//replaced.url provide crossdomain.xml added attachment second rewrite points root database question allowing get documents database http//yourserver:0/flash/doc0 accessible http//replaced.url setup allow flash application connect couchdb without problems hope helps. hi 0cts although seem see nails everywhere couchdb hammer better storing xml xml database like sedna allows versioning documents xpath searches documents sent mobile. hi looking amount replies wrt topic seems much interest full text searching really hard tell would expect feature implemented couchdb way would supersede nice couchdb-lucene combo said _really simple_ probably bad solution performance wise search implementation look couchdb lists decide _view sent _list function within _list function implement full text search inspecting document data setup least allows replication functionality might enough. mng-0 doc define plugins pom mng-0 pattern parameters default values pom need standard usage mng-0 make additions pom backward/forward compatible mng-0 allow maven monitored using jmx mng-0 make build process info read-only mojos provide mechanism explicit out-params mojos declare mng-0 use uniform format tags. welcome board. i've using rc0 since uploaded i've checked zip file working expected earlier. buildcontext looks indeed like good candidate know cross dependencies almost unpredictable would require parse evaluate sources opted recompiling sources yes algorithm make possible problem simply delete files parts might created plugins e.g resource processing. use maven-plugin-testing-harness 0.x maven 0.x maven-plugin-testing-harness 0.x maven 0.x think 0.x 0.x testing harness provide client api maybe able run tests different versions maven certain. changes done css provide us working xdoc plugin m0.0 interested test soon possible. thanks reminder jira back need call vote plugin jason try building snapshot plugin trunk see solves problem i'll go ahead ferry release. except maven rc0 released exactly happened don't process releasing maven bundle specified plugins uses latest bumping version snapshots agree vincent worth first need releasing maven bundled already released plugins. hi run maven deploy phase maven upload nexus final file artifactid version respect set build could indicate maven deploy plugin upload nexus example prefix_ artifactid version anyone know thanks regards. following comment added issue jar test classes code test plugin. hi i'm saturday sunday kind regards. think snapshots different aspect outside version ranges version ranges meant specify range versions snapshots fit perfectly version range problem statement it'snapshots fit perfectly version range missing point technically correct snapshot version precedes version number however point specifying version tag dependency problem allowing snapshots fill version ranges unless specified version range inconsistent behavior non-version ranges work don't use version ranges get precise control snapshots/non-snapshots i.e specify never get snapshot specify 0-snapshot always get snapshot get specified currently use version ranges loose control expect behavior specify expect snapshots specify snapshot hand specify 0,0.0-snapshot would expect 0-snapshot fulfill therequirement quote prevent accidental use snapshots released versionsare available given range. don't available determine used theversion range never include snapshot unless directed theversion range determine resolved version nothing else. quote mechanism place explicitly state dependencies beresolvedto snapshot versions dependencies certainly useful yes would useful. quote 're dealing way nothing withversionranges able say don't snapshots acertainartifact specifying normal version without snapshot going beresolved snapshot unless it's another declaration mention versionranges could happen.the current resolution mechanism uses first match it's pretty random order searched repositories important good random results build dependent quote someone specifies version range don't versionexcept mustbe range knows versions work snapshots fallinto rangeare compatible don't used dependencies _are_ declared snapshot bring ina snapshot versionalso declared somewhere else snapshot-excluding range thebuild fail say snapshot version work fall thetechnical definition version range yes goal snapshot thatdid change major version would api break anythingbut guarentee snapshots may alpha versions theapi changed experimentally see something work may notwork reverted snapshots potentially state changeand assume anything them.as case specifying snapshot dependency samedependency version range don't think build fail inthis case stating explicitly use snapshot versionand willing associated risks practical matter theonly way possible version range specified ofthe child dependencies least level lower youspecified snapshot dependency case much different thanwhat would maven dependency version afixed-non-version-range version quote anyway could 'assume people specify version ranges theydo don't wantsnapshots apply change i'm worried side effects since using snapshotsin ranges possible b excluding snapshots ranges iscumbersome apply b possible even cumbersome. currently b possible way today disallowing snapshotswhen use version ranges state latest released version i.e. forced use build maven today really use public releases maven quote ow thought something 'processing instructions similar inthe version ranges like snapshots=off use xml attributes ofcourse yes good idea allows finer control meaning therange i.e would still allow snapshots explicitly stated theversion range docs.one thought issue think would useful way toover-ride version range behavior command line switch isone use case would useful version ranges always pull insnapshots let backup moment explain build server deployssnapshots upon every code commit i.e clean deploy goal don't wantall builds get snapshots normal processes hencethis version range issue would somewhat helpful able runsomething like -allowsnapshots=true clean verify dependent projectsjust ascertain snapshot break anything mode doesbring next snapshot automated server ran alldependent projects could find sooner proposed change anartifact going break something note would rate feature asminor compared resolution limit version ranges notinclude snapshots default.-davekenney westerhof-0 wrote. done ibiblio hours. yes remember works without. yes don't already decide java even. he used codehaus dav test worked quite well codehaus account dav drive work catch irc help. info creating reports directory '/home/projects/maven/repository-staging/to-ibiblio/reports/repoclean/0-may-0000_00.00.00' info source repository '/home/projects/maven/repository-staging/to-ibiblio/maven' info target repository '/home/projects/maven/repository-staging/to-ibiblio/maven0' warning warning encountered rewriting artifacts source repository target repository. provide see instance. believe copy maven jar dependencies local repository presume don't wait several minutes attain goal first time use maven jars used many plugins don't think especially useful people might find use. hi need currently taken archetype plugin assumes sub-modules refered parent. sent iphone http//replaced.url suspect cross compiling ruby lead something developers could wrong think pure impl using api conventions would needed fwiw think could said java life short personally don't mind sucking jruby"""
"""178f713589e948dbd7b2f430525f28805f89a3fd393d8f0f76882a0dc432f5d5""","""works fine thanks herve. questions stackoverflow.com relating maven-indexer created tag re-tagged appropriately. hi herve username carlspring documentation site needs serious work lack thereof developer unfamiliar indexer must sources try figure deprecated currently tricky business unless willing invest lot time pray using right thing site documentation non-existant created separate module examples currently based tamas examples github great starting point need extended examples much illustrate bits pieces scattered across different sites gather things place make useful possible would like open issues jira chase guys filed issues order polish requirements close obsolete ones schedule relevant ones know need extra permissions gotten latest indexer work spring need using project would like add examples needs done order able fully drop plexus replace wagon code proper started working went vacation pressing work soon get chance look well kind regards. hi barrie nice hear back guys indeed jira account last time tried able assign issues could somebody please look working version maven-indexer dropping plexus polishing examples re-working certain bits api believe tamas still vacation surely explain thanks. hi would downsides like bind process need download remote repository isindex least usually small recall correctly maven centralisindex something like mb .gz talking top head recent work maven-indexer downloading index central order run tests took would people start saing oh maven slow build tool resolve million dependencies even slower switch some-next-cool-build-tool-goes-here referring maven slow lot people feeling takes ages build larger projects due resolution dependencies need update local index regular basis day least quite convinced would accepted well everyone could option dependencies could resolved mixed basis index use/update option specified ignore option specified saying current model best trying illustrate downsides switch kind regards. could always clone repositories fixes locally submit pull requests seems problem clearly questions lists right place ask. hi writing concerning bug misleading maven locks around quite preventing us able use distributed build system properly use hudson although possibility run build private repository reasonable solution large number builds due amount space need node hitting lot lately hudson running nodes using different operating systems maven problem described follows build snapshot module install locally work days commit code meantime changes module however maven-metadata-local.xml contains line tells maven update snapshots even -u using older snapshots locally solution remove artifact islocal metadata file edit manually comes regular dependencies written small plugin works around issues locked maven-metadata-local.xml files info however comes parents situation much trickier projects get loaded interpolated long would like ask proposed patch accepted/reviewed confirmed bug exists maven proposed patch david rousselie applicable maven copy five lines changesto another 0.x release could included could bug patch reviewed writing workaround plugins good idea using modified maven company therefore would like request issue fixed next version maven quite critical thanks advance looking forward replies regards. hi would like become maven committer lately contributing fixes maven-indexer current area interest working together tamas cservenak deeply familiar writing maven plugins large part core plugins see pulls currently working things maven-indexer well contribute knowledge free time project written quite plugins hosted github familiar contribution process pulls work look project already read stephen isblog becoming committer senior build release engineer agree approach believe fixes live high standard would privilege able join team kind regards time yearly committer school announcement become maven committer http//replaced.url committer.html let us know. hi furthermore feeling adding option artifact bound lead headaches hand could specify groupid includes/excludes repositories things could much better terms resolution times frankly think remote repository isjob first place people otherwise two different places handled making things complicated track later artifact routing rules exist. thanks accepting merging olivier. oh right seems broken recent merge believe 0d0ed0af0eff000ab000ae0c00d00f000e00bf0a safe revision rollback try things look get back. hi created account long time ago probably early late sure need re-register something. okay registered xcircles username seemed work. hi thinking whatever animal owl would really cool wearing sort super-hero outfit letter maven brought super-powers order proposal. hi kenneth version maven trying build building fine maven kind regards. hi looked commit 000ce000000000eb000c00c000000dd0d0baa000 seems try delete primary storage every time stopping vm maybe root cause kvm blocker bug cloudstack-0 kvm guest vms crashed automation test agent log find lot umount primary storage error never kind operation know add referring code. case seen check http//replaced.url really something go away gossip. looked jmx object however compaction manager support multiple threads moves 0-filesize time compact set files useful showing current progress rather lifetime history. generic problem semi-structured documents meta-data consistent naming making names word might created_on pdf created etc really frustrating investigated figure field map created tika solrcel map find way go map dynamic glob pattern stored field look pops satisfactory best. call anything solrj call url solrj lots convenience stuff set particular parameters parse response etc it's communicating solr via url look something like solrquery instance nice command setfacetprefix it's entire method really it's really setting solrparams key/value pair equivalent remember it's setpath method use set destination request suggest maybe /suggest it's something like best. worddelimiterfilterfactory probably stripping parens try running terms http//replaced.url i'll see effects various tokenizers filters sure check verbose checkbox it's good place start understanding intention various options particular worddelimiterfilterfactory split intra-word delimiters alpha-numeric characters best. bq make commits reliable unreliable sounds like 're saying 're expect 're talking predicting documents searchable mikhail spot it's real time get fetches recent copy document whether it's searchable limited application 're interested searching newly-added. agree upayavira seems architecturally example crux matter differ field figuring going expensive burdening searches kind logic create custom update processor use component build updates ingest docs build signature field issue delete query update best. version solr/lucene instances index corruption see lucene/changes.txt file might account something stab dark though troubling best. stay master/slave full replication switch probably price i'll pay indexes different safe side solr replicate whole thing really much problem shawn says though much would simpler best. uploaded files right place release harish could please confirm things look good release apache download page time mirrors catch though updated sourceindex files jakarta-site0 well probably time update well checked http//replaced.url well harish update website i'm sure involved try later tonight get. personally i'd strongly recommend point mandating whenever going version solr another example schema config files ship new version used customizations re-merged/re-worked supported work safer work new configs don't know developers solr long since ditched solr certainly don't manually test back compatibility back-compatibility unit tests i'm aware. ok good authority way go. ensuring replacements properly encoded xml. i'll get better response solr don't taking time retrieve large stored field writing response client-side parsing data sure. ok thanks confirming impala table backed parquet it's surprising hear get higher throughput loading parquet table compared phoenix even using parquet file structure used impala quite lightweight compared hfiles example column non-key row phoenix contains full row key column qualifier need sorted writing hfiles trade-off course query single rows extremely quickly hbase whereas don't performant impala using suggestions gave likely able improve performance loading think creating parquet files impala always give substantially higher throughput. thanks keeping date andrew couple considerations okhttp.version property used okhttp driver could run openstack-marconi live tests rackspace-cloudqueues ones marconi uses okhttp driver default able send patch requests make sure version bump breaking anything tried could try upgrading latest thanks. noticed added closing scanners left auto-close gc exactly sure mean point comment. null value comes byonlocationresolver.extractconfig http//replaced.url port missing config bag null gets coerced null transitions call getoptionalval understanding optional value provide default missing exact meaning null whether it's missing property map mapped null make difference opposite interpretation would put nulls maps means byonlocationresolver similar client code must implement null checks"""
"""ba513f1638042a57f5f9c8cd0c995142ba498abd14ddab3ae6ae0e6b2dfc6fee""","""p.s volunteer help kip sent protonmail http//replaced.url encrypted email based switzerland. karsten said providing detail actually trying usually makes better helpful/accurate answers guessing search key value right create multi-value field custom indexed stored indexing add entries custom set analyzer strip index key e.g. would first try setting post explicitly utf-0 matter appears resin tomcat issues properly handling form data without matter issue configuring tomcat see http//replaced.url patch applied may 0th year work around appears bug tomcat assume version solr patch. hi sorry noise finally realized running using java code enwikicontentsource lucene benchmark explicitly set fields push results solr. use characterencodingutf0 think shall get back stream utf-0 bytes db know mysqlxmlfield isstart array bytes returned jdbc call create string array using utf-0 encoding use bytes directly writing xml best thing make sure xml send solr starts line make sure converted text xml fields wrap fields. hi low query rates using shards approach improve performance multi-core cpus solr cores setup distributing requests effectively use cpu cores parallel request spread shards across spindles maximizing i/o throughput issues approach binary fields work results back b versus actual data short fields get java.lang.short text prefixed every value deep queries result lots extra load e.g 0000th hit shall get shards hits collected/returned dispatcher though unique score returned case followed second request get actual top hits shards something wonky way distributed http requests queued processed load see ioexceptions always n-0 shards succeed shard request fails good reproducible case yet debug. general issue months ago generate reports things like scm commit activity given day larger customers users multiple timezones timezone use wrote blog post http//replaced.url short answer ultimately decided use utc times server report api ui least heinous various options. hi encoding using pushing documents solr specified xml post request separate issue mime-type use post using latest scripts solr characters look like xml pushing example encoded two surrogate characters instead code point extension b set xml parsers handle correctly source similar issues seen potential issue xml parser used updated handle extended unicode code points older parsers still failed handle example. hi tommaso slaves configured use vip talk master easy dynamically change master use via updates. related item might check jmaki http//replaced.url jsp-based amount jsp-generated content pretty easy add support dynamic. hi working project speak stefan friends going live separately something independent solr/nutch view search plumbing usable multiple environments makes sense view replicating core solr nutch functionality sucks sure outcome. error either schema.xml file messed might still need uncomment lines beginning file ones say uncomment trying use resin version even though using later version resin lots issues xml parsing. confused answer assuming based page referenced url provided approach textprofilesignature would generate different md0 hash single letter change change resulted change quantized frequency word uncommon word would even show signature. low qps multi-core servers believe reason multiple shards server provide better parallelism request thus reduce response time. idea thought given document/field set would contain text single language write special token language e.g analyzer add my-special-token-prefix-esperanto token field query time assuming know language make required term. depending complexity solrj could solution see section talks solrj provides apis create. would use data files. hi list working improving performance solr scheme cascading supports generating solr index output hadoop job use solrj write index locally via embeddedsolrserver mentions using overwrite=false csv request handler way improving performance see http//replaced.url removed support solrj deemed dangerous mere mortals question whether anyone knows much performance boost really provides hadoop-based workflows straightforward ensure unique key field really unique thus performance gain significant might look figuring way trigger lock re-enabling support solrj thanks. hi especially yonik grins trying solr resin following copied solr-nightly.war /webapps/ cloned/edited resin.conf file include mapping solr adding started resin created /webapps/solr-nightly directory expected tried use solr complained finding see stack trace email realized solrconf folder included default .war added inside /webapps/solr-nightly directory result tried moving uncommented lines file yonik indicated necessary get resin work properly solr change anything figured would ask correct way configure solr resin rather continue thrash would help resin jock far usage limited futzing resin config file add simple thanks. sort answering question seems like need get current core use instantiate new solrcore exact config documentation solrcore isconstructor says core already exists stopped replaced unclear whether graceful swap like hard shutdown old core thanks hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor isdatadir gets used construction changing coredescriptor isdatadir effect since would go changing datadir core multi- thanks. right baking fine-grained level security information bad idea example worked pretty well code search krugle project number groups granted access rights file project would inherit list groups user logs get authenticated via ldap set groups belong returned ldap server becomes fairly well-bounded list terms query acl-groups field file/project document set boost portion query. large index would recommend separate solr installation use update/commit changes use snappuller equivalent swap live search unlikely switch lucene tune new parameters control memory usage updating. believe mistake recent email thread list. based experience using jira manner would agree mike automatically assign fix go path wind steadily growing wave deferred issues constantly getting pushed next. patch handles case corruptrecordexception thrown iterator directly merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. behaviour-change enabling enter-keypress saving values sweet possible reject changes 'esc. ready review latest ignite release published next day merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. put time reading rfc yes actually went code latest changes lgtm think last terbolous comment sent remibergsma made change checks pub ip within range reserved rfc. pr fixes issue doubled new lines lines converting jupyter notes zeppelin note pr tested run 'java -cp zeppelin-jupyter/target/zeppelin-jupyter-0.0-snapshot.jar org.apache.zeppelin.jupyter.jupyterutil -i path/to/ .ipynb produce 'note.json' import 'note.json zeppelin don't doubled spaces breaking changes older versions needs documentation merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. generally big changes touch multiple spark cassandra angular etc hard review forever get merged need consensus maintainers parts would suggest pr separated least improved reviewed fast independently. could please check make check runs test suite. added couple comments think 're ready merge hostname changes comment yet cluster changes note knowledge setting hostname based experience past it's possible modern distributions things differently perhaps recommendations changed nevertheless way still work. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message"""
"""b851c6f2472149ec2d0802860acf21ea1087b80e19565dd8a0567ba75429e6c0""","""hi well aware maven default build.finalname unfortunately look eclipse packages source artifacts corner case example snippet list bundles plugins directory eclipse problem maven-eclipse-plugin isto-maven intended serve default tool create maven artifact eclipse correctly recognizing bundles source suffix source artifact corresponding java artifact instead creating new java artifacts contain compiled java classes java sources assumption ask eclipse guys completely revamp builds account maven isbuild.finalname defaults would possible fix maven-eclipse-plugin generate correct maven provides possibile fix would possible apply thanks. hi current implementation maven-eclipse-plugin isto-maven given following two bundles according maven conventions create two artifacts groupid artifactid version using classifier source second bundle instead creates two completely unrelated artifacts creates problems using -ddownloadsources=true trying ago attached patch could please shed light wheter reasoning correct thanks. hi others post worth specialized resolution wrote similar requirement may help similar requirements running wanted able return facets matched actual search rather facets entire result set example user searches author twain present list facets match twain exclude facets twain found tell users facet values present alpha-sorted list author names count associated documents search author search field identify matching documents get facets i.e normal solr processing point filter facet set include match original search added extra facet parameter facet.sirsidynix.filter.facets instruct solr special facet filtering modified simplefacets method gettermcounts right final return counts like added method filtercounts basically wrapping things run search facet value setting instances based schema inserting facet value running original query anything matches score ones keep filters counts entries match original query using lucene isfast in-memory single document index queries run string value count create run original query anything score means hit value matches original query retain score means hit i.e facet value associated document matched query facet value match query param field field facet values came original search would initial_author_srch_boost well string time shove single-values initial_xxx fields good enough query able correctly bit explanation schema order suffixed facet fields _facet hence first statement matching searchable facet fields names basically differ suffix strip _facet append _boost _fuzzy two field types searching possibly applying boosts fuzzy matching shall see exactly hopefully modify version match schema basically idea derive field original search issued facet field shall read see works rather re-iterating anything date faceting ranges anything facet prefix handling may may work need prefixes anything else facets handle least test say special case us way intended general solution fit prime time submission solr enhancement. maybe jumping gun bit known location maven-changes-plugin i've checked don't seem find. changes version include use relative paths directory properties issue mpant-0 thanks changes compile tests run junit present ant display automatically install plugin following single line manual installation download plugin fun. using forkmode pertest log0j output console. looked links provided seems good. following issue updated full history issue see. none new features available public use yet 're focusing making drop replacement first don't think anyone us http//replaced.url don't know thanks. believe maven-invoker intended run maven maven-verifier hand appears conglomerate functionalities experience maven-verifier core rather limited favor separation concerns wonder whether don't split instance maven-verifier could either directly depend plexus-utils instead duplicating classes sake core classes need different names could use shade plugin relocate stuff allow pre-/post-build hook scripts invoker plugin easily check asserter something holds path current directory e.g base directory project path local repo offers several assert methods really feel something nothing invoking maven could properly designed serve core well hook scripts used eclipse plugin. following comment added issue chance mpear-0 mpear-0 could looked rc0. hi guys following surefire-0 wonder get progress feature even optional surefire/failsafe plugins idea likely rely scanning metric rely like classes avoid loading bytecode show progress bar give user console ee tck often testng listener really lacking plain surefire hope gets enhanced. proof-of-concept pr introduces support test sources organized module descriptors merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. really don't know going provision toolchains without something settings means core changes i'll see i'm sure using properties fashion really maybe change simpler plugin configuration pom element introduced removes need plugin configuration set project level. hi folks pass core w/ external resolution outcome logs almost could find artifact org.apache.maven maven-settings jar:0 central file target/null alike -dmaven.it.central=http//replaced.url idea. hi i'd like release clover plugin v0.0 it's issue left jira don't quite agree content changes version include added maven.clover.multiproject property set true master project 're using multiproject setup setting true make clover goal maven-clover-plugin report pom execute subprojects i'll call new clover multiproject new clover multiproject goal run clover multiproject setup new clover merge goal creates new clover database merging several databases specified using maven.clover.merge.databases property maven.test.failure.ignore correctly reset clover fully disable clover clover clover compiler adapter fixed issue clover report calling clover leading fixed issue clover task definitions using explicit apparently breaks maven changes ensure clover init clover clover called example calling clover twice row execute clover -report goals call clover test anymore generates clover reports clover report always generate clover reports even coverage data reason clover generates metrics coverage like loc ncloc useful issue mpclover-0 removed maven.clover.jar property used override clover jar property working requires ant clover license need point maven.clover.license.path please cast votes it's thanks. following comment added issue ended way call goal using attaingoal using session required updated werkz release get it's pretty works sample webserver plugin. yes though users start asking questions m0 alpha release would answer user list different ones made primarily interested unified development core folks involved coding core worked m0 m0 brett act like bridges brett knows m0 still know things know m0 well whereas michal trygve emmanuel familiar innards maven0 john casey probably entering mix soon long history supplying patches helping gave big patch maven0 fact brett likely person knows maven0 even though user. think would legible fetch push urls separated something part url normally ie instead would stand visually otherwise seems like workable solution current pom model. hi explain error means appears every report think found bug timestamp checking verifying. hi think sounds reasonable something like instead. i'm downloading source cvs run maven eclipse debugging session debug maven. far know first time saw trick/hack get michael mccallum tell us picked pom.xml it's wild depend feature may get personally strong attachment range ordering imply less 0-classifier would seem big regression breaking change 0.x. little surprised find m-r-p branch handles git committing branch version current branch making branch reverting version current branch make branch change version. hi guys someone tell nowadays maintains parser http//replaced.url bugzilla cvs longer available. position artifacts central never ever change it's something wrong new version needs deployed. seconds mng-0 show repository downloading uploading from/to. i'm afraid useless staging repo url transitory besides pom contain revision number neither benefit mailing archives completeness vote request assume mean. think would nice reasons listed jira issue created yet"""
"""0f1757469d1c549990a866d08ed0b74dd6f447db8d5aa88eb06ed24ef95413af""","""please send mail. website looks great. hi problem documentation kind missleading must add new element config update existing need add. could post full query url see exactly query post output debug=query show us lucene query generated. something everybody get transitioning world solr/lucene schema describes possible fields absolutely requirement every document index fields unless define solr happily index documents fields missing feel free able define people parts documents choose perhaps common fields i'll form queries like ralph assuming field people sku parts continue path de-normalization it's another thing db wouldn't document index contain data need moment find asking join stop consider de-normalization. i'd give go solr wouldn't modify library. hi michael missed elevator component standard request handler add following snippet request handler http//replaced.url http//replaced.url. hi thing describe possible set uses spanfirstquery sure it's going post debug output. hi ahmet tried follow recipe adapting solr testing right first try gave message java least file license it's license proper format see logs complexphrase tried ran successfully hope ok. registered queryparser thing i'm running multiple cores could search successfully wouldn't split already separated field multivalued= true need able search call even catch go thanks helped lot. bq it's good reason small cluster small mean 000s nodes well good reason would system continue operate zk nodes lose communication rest cluster go completely clear though zk nodes definitely wouldn't need beefy machines compared solr data nodes since light-weight orchestration yea data node system might willing go node ensemble tolerate single zk node dying depends much cash willing spend availability level looking. hi trying index collection solrinputdocs solr server wondering call make add documents add collection call blocking function call would like know add call call would longer larger collection documents thanks. yeah sure send mail plain text spam filters pretty aggressive comes anything else glance problem i'm guessing user run solr wouldn't create permissions best. hi lance well actually copied whole configuration files instead added missing configuration fresh copy example directory implementation mean readers used seems performance actually still becoming better moment average dropped even lower 00ms comparison 00ms. probably may use sanitizer http//replaced.url. query indexer i'm impressed got 0s replication work reliably. still able search already-indexed data exists values limits reached suspect solr would commit continue indexing however chance use features dih mis-configuration caused import finish without indexing anything thus wiping data aside continually index search time almost every day using. requirement solr search finds string return entire text document emails rtf format process outside solr achieve say process outside process rtf document search result return original document able successfully solr core stand alone. replaced email.addr.es wrote artifact normally unique enough idea still useful please file issue jira. project uses maven 0.x support pom 0.x project requirement jdk version requirement parent deployed converted format read maven version sure understood idea let pom format project level tool envolve always deploy pom artifacts meta-datas format requires 0.x improvemement translatable way example globalexclusion could translated brute force exclusion. really ridiculous lukas got account today perceive asf don't see problem anyone entitled work wants don't think anyone came blame lukas choosing things interest help people appreciate nobody demanded payment special recognition think time consider alternative don't get http//replaced.url flame someone shreds demand someone else fix bugs worded strongly i'd choose current situation tended towards recently apache way i'm seeing whole lot respect henning i've publically mocked maven community thread various places already discussed 're really concerned 're going handling wrong way 're going participate i'll ask give others respect lot people contributing positively don't see discouraged hope find constructive inflammatory regards. don't think matters much long rc don't since i'll get confusing. hi agree something like mng-0 indeed help experience looking implementation find projectbasedirectory confusing would thought project module already things like project.basedir project.executionroot could don't use executionroot seems exactly think projectbasedirectory abstract i'd prefer concrete using basename config override global i.e environment variable based properties project specific values scripts right thanks. might interested developed extension sort jdiff maven plugin checks-out version defined property defaults current generates docs generated probably done adding links menu first attempt jelly code likely little sloppy anyone interested strictly related jdiff plugin submit many thanks. think wiki lacks commenting facilities rss etc makes poor mans blog platform publish first blog copy wiki link blog enable comments etc. ok i'll arrange vote around suggestion seeing far thing people seem like. oozie-0 support getting credentials cluster hcat credentials config empty oozie-0 dataset url contains spaces handled rightly oozie-0 log.scan.duration used error audit logs oozie-0 -dtestjarsimple option mentioned minioozie doc work oozie-0 map-reduce launcher need distributed files archives except jar input/outputformat oozie-0 oozie acl specify group does't work oozie-0 minioozie work outside oozie oozie-0 introducing new counter instrumentation log distinguish reasons launcher failure oozie-0 ssh action succeed exists command fail oozie-0 rerun failed/killed/timedout coordinator actions rather specifying action numbers oozie-0 queuedump command display queue information server oozie-0 loginfo uses action instead oozie-0 instrumentation configuration rest api web ui include oozie servers oozie-0 cache list available timezones admin oozie-0 docs explicit multiple sub-workflow definitions possible may edit subscription. oozie-0 extend fs action allow setrep file oozie-0 spelling errors log messages exception messages oozie-0 reduce number threads test execution oozie-0 oozie job submit don't report error message user issue job conf oozie-0 status update recovery problems coord action children sync oozie-0 get ooziesharelibcli perform final rename destpath creating sharelib oozie-0 dataset url contains spaces handled rightly oozie-0 log.scan.duration used error audit logs oozie-0 map-reduce launcher need distributed files archives except jar input/outputformat oozie-0 introducing new counter instrumentation log distinguish reasons launcher failure oozie-0 ssh action succeed exists command fail oozie-0 rerun failed/killed/timedout coordinator actions rather specifying action numbers oozie-0 queuedump command display queue information server oozie-0 loginfo uses action instead oozie-0 instrumentation configuration rest api web ui include oozie servers oozie-0 cache list available timezones admin oozie-0 docs explicit multiple sub-workflow definitions possible may edit subscription. oozie-0 use pom properties rather specific version numbers pom files hbaselibs hcataloglibs sharelib etc oozie-0 oozie servers don't talk oozie kerberos oozie-0 loginfo uses action instead oozie-0 new configuration specify server-server authentication oozie-0 workflowgenerator package tar.gz file though seems creating intention may edit subscription. help us periphery follow along closely isolate distribute mentioned issues list please use library .net via http//replaced.url i'd like follow along releases directly available drop solution less aware code structures work lower level however interested advances library. favor. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hello thank much interest planning implement features listed however due breadth approaches suggestions hints based experience course welcome. hdp0.0.0 compatible phoenix0.0 yes i'll go ahead put rs thanks everyone quick response. pranavan theivendiram university moratuwa winner gsoc-0 done project apache software foundation apache stratos interested working following issue sound knowledge databases someone assist thank"""
"""cbbc1cfe5bddc6a69e0c3ba62cd14e45b039d4ecd166998f460cd9ca82369338""","""newbie solr done everything solr tutorial section using latest versions jdk solr see solr admin page http//replaced.url hit search button receive http error tried run solr tomcat unsuccessful solutions document links appreciated thanks help. trying overlook expected i'manilow get response though wow geoff wouldn't know musician well like gogh it's birthday two days ago names famous artists would spirit use unusual color names simpsons theme maybe oughtta stay away homer d'oh could self-referential tapestry know you're wikipedia many projects use release release number think tapestry start road map im-ing erik brainstorming theme liked city names chose favorite cities single word names portland vancouver boston amsterdam etc it's possible roadmap methods/classes probably sensible naming etc. like unless i've completely base hardest part new liberal allowances template location i'm deliberately avoiding assuming resolved handling anonymous components actually fun i'm figuring way spindle represent anon comps way able following mixed spec comp/anon comp edit bindings anon comps spec editor changes pushed pull anonymous comp template insert spec naming template component truly anonymous push spec comp template make implict/anonymous need improve validation/error handling include definitions sources brings another point would hard change tapestry parser make easy extend following possible would cool 'fatal-ness error configurable would useful spindle could set switch something parse would go completion unless true fatal error like well-formed occurred mechanism collect errors create error markers thus example validation error markers validation problems first course error occur would ignore parsers result object would nice get line/column info errors generated tapestry code probably harder last list support last thinking adding page spec editor contains template source would hardest thing accomplish much harder finally wiki page template directive indicate page it's format directive. resources defined dojo it's css served. hi edwin changes recommend bin/solr.cmd make easier work nssm please file jira i'd like help make process easier thanks. it's call release manager binding technically vote passed. shell gets it's memory config accumulo-env file accumulo_other_opts reason value low lot data loaded tab completion stuff shell could try upping value file try running shell disable-tab-completion see helps. thanks eric wouldn't cleaning something hadoop-data directory zookeeper. hi i've read source code documentation following iterator commonly used batchscanner accumuloinputformat parallelize search shardids means key0 key0 shradids searched misunderstanding indexeddociterator search shradids thanks regards. right got change maxversions many times wouldn't persisted disk compaction rewrites data scans however always consistent current configuration implementation accumulo instantiate current iterators minc majc scope depending it's minor major compaction write output memory/files compacted filtered/transformed iterators back single file. thanks calling principle community code super important matter code kafka project could simply pull samza code write new stream processor without asking permission would don't get samza community thus think community aspect important part discussion 're talking merging projects it's really merging communities chat friend lucene/solr committer originally two separate projects merged said merge always easy probably net win projects communities overall community people tend specialise either lucene part solr part it's ok it's still cohesive community nevertheless benefits close collaboration due everyone project releases don't slow fact perhaps got faster due less cross-project coordination overhead allayed concerns big project becoming slow besides community code/architecture another consideration user base including mailing list good users i've thought last days reducing users confusion good someone adopting kafka need way processing data kafka moment kafka docs give consumer apis nothing choose separate stream processing framework burden users especially framework uses terminology inconsistent kafka make samza part kafka unify terminology would become coherent part documentation much less confusing users making easy users get started good simplifying api configuration part making yarn optional good would help part package people download part documentation simplifying api/config decoupling yarn done separate project becoming part package would require merging projects supporting users choice programming language good used work ruby ruby community plenty people irrational hatred jvm imagine language communities likely similar samza becomes fairly thin client library kafka using partition assignment etc provided kafka brokers becomes much feasible implement interface languages giving true multi-language support thought coming conclusion stream processor part kafka project would good users thus successful project however people experience stream processing systems samza community leads thinking merging projects communities might good idea union experience communities probably build better system better users jakob advocated maintaining support sources kafka totally see need framework think need pretty well satisfied storm already spouts kafka kestrel jms amqp redis beanstalkd perhaps don't see much value samza attempting catch especially copycat provide connectors many systems different means hand failed attempts implement systemconsumers kinesis postgres make think stream processor supports many different limited lowest-common-denominator model samza supports kafka think could support kafka better framework thing well sure understand point departing vision distributed processing library-ified samza would still allow distributed processing small amount glue could still deployed yarn cluster conclusion i'm starting agree approach jay advocating thread. hello using hdfswriter provided samza package samza-hdfs try write newest patches version bug regarding closing files fixed bucketer seem work given every new event sent hdfs outputstream system creating new file hdfs according taken appending events existing file bytes threshold reached known bug missed sth implementation kind regards. jay yes current function jobcoordinator partition management maybe call partitionmanager make. auto scaling module package org.apache.samza.autoscaling.deployer however whole auto scaling samza-core problem. rather relying external stream hello samza example perhaps look something local option would simulate/fake data may bit boring real data could look local box treating hardware resources/dmesg/proc files stream writing little samza job summarizes/processes data pretty nix focused though minus. rebase yesterday realized something git refused add files right it's missing couple files going address comments add files update rb sorry confusion. i've put patch get samza working yarn. hi assuming using configurations set systems.system-name.streams.stream-name.samza.reset.offset true systems.system-name.streams.stream-name.samza.offset.default oldest see hope helps thanks replaced email.addr.es. try reply-all request dc-allow address add rb subscriber regards. chime i'd interested monitoring blog post 're kafka implementation robust data pipeline initially samza look interesting monitoring use-cases replaced email.addr.es wrote. request incorporate sure best way bring kindly let know formal way propose apache forum apache kafka team format discussing new topics via kafka improvement proposals provides meeting members read topic proposals hangout/ meeting consider. heartsavior would like look it's similar pr put. release based clojure 0.x code migrating apis org.apache.storm big non-backwards compatible move major version bump 0.x seems like good move would like move user facing apis org.apache last things translating clojure code java moving org.apache concerned two code bases diverged significantly another terms functionality storm code soon heartbeat server nimbus different implementation resource aware scheduling distributed cache like api log searching security massive performance improvements shaded almost dependencies rest api programtically accessing everything ui sure missing things jstorm many changes including cgroup isolation restructured zookeeper layout isolation matter large effort port changes code base another clojure java proposed initially broken incremental changes may little longer always working codebase testable compatible current storm release least move user facing apis org.apache lets community continue build test master branch report problems find incredibly valuable personally don't think much easier especially intent always maintaining compatibility storm bobby thanks merge plan question phase mean community plan create fresh new java core based current clojure core firstly migrate features jstorm confused really huge job might require long developing time make stable jstorm already stable version release planned release nov 00th already run stably several month alibaba besides many valuable internal requirements alibaba fast evolution jstorm forseeable next months java core totally fresh new might bring many problems coming merge point view think much better easier migrate features clojure core basing jstorm java core please correct misunderstanding regards based number discussions regarding merging jstorm code i've tried distill ideas presented inserted result i've divided plan three phases though necessarily sequential obviously tasks place parallel none set stone presented discussion comments welcome"""
"""ee79066e5e393cd954df45326b189c7d93967ef5152edd52354d057256b8eb66""","""hi hjars required typically configured though tez.lib.uris config property thanks regards kuhu missing tez jars likely missing custom setup please follow instructions setup client hadoop environment http//replaced.url. running windows default timestamp using time.time 0e0 get timestamp twice code wouldn't use pycassa thrift api wrapper created python code implemented following function getting timestamps increases call. sake updating thread orr wouldn't yet task trackers cassandra nodes time likely due copying ~000g data hadoop cluster prior processing you're going try installing task trackers nodes. general today large amounts hints still pretty much makes node angry longer nearly nasty unless really low throughput you're probably going gain much practice raising hints window today later get file system based hints think approach work better today i'm concerned practice larger hint windows buy lot see following details http//replaced.url. running http//replaced.url fixed believe need explicitly grant select permission onto system.schema_triggers user workaround. second exception states file sstable missing possible didnt delete commit logs nfs mount stale. i'll need temporarily lower gc_grace_seconds column family run compaction restore gc_grace_seconds original value see http//replaced.url info. re-introduced corrupted node followed process thanks folks mailing list helping listed operations wiki still cleanup node point noticed seeing exception appear times minute existing node new think started around removetoken solve restart node cleanups/resets need thanks. never mind found http//replaced.url. hi tl dr superficial understanding cassandra currently evaluating project cassandra embedded another jvm application embedded instances form cluster application use failure detection cluster membership. thank reply you're sure yet use application distributed cassandra embeds point see ring real pain ass goal prevent users connecting cassandra change anything internal flexibility might point cassandra shoot-and-forget kind really sure yet best regards disclaimer information contained message attachments intended solely attention use named addressee may confidential intended recipient reminded information remains property sender must use disclose distribute copy print rely e-mail received message error please contact sender immediately irrevocably delete message copies. imo deleting always better better store column value associated. nodetool repairs bring much data lot sstables created disk space almost doubled level compactions run slow turned throtting completely wouldn't see much utilization ssd cpu example 0.0mb/s ssd insane anything speed thanks. three things first design doc talking strongly consistent reads wiki gives simple exemple read it's even followed warning actual contradiction second point design docs slightly outdated point least support quorum writes since http//replaced.url resp read provided wrote quorum resp third good recall counters considered stable yet includes documentations. right it's said proxy layer would need read result appropriate consistency level returning memcached client application client application would need declare consistency preference using configuration file. thanks anybody know distributed in-memory system supports structured data e.g tables. dynamic endpoint snitch works keyspace true well seeing running bug left dynamic snitch disabled unless added extra option. yes almost done make possible. range wouldn't contain nodetool ring shows token-ranges node primary range thinking primary someone confirm primary replica always changes change racks secondary replica move next replica different rack either last case next node primary replica different rack r0 contacted prove disprove stopped ran query consistency came back fine meaning indeed hold data ss tables show mean data actually gets moved around racks change probably queries primary replica replicated data read repair automatic data move rack changed least sure deprecated ignore_rack flag useful move data manually rsync sstableloader. use nodetool cfstats show keyspaces cassandra-cli see flush settings default think minutes million ops 0/00th hte heap cf created automagical global memory manager see http//replaced.url http//replaced.url. first update schema cf run nodetool upgradesstables node sometimes works node restart upgrade leaves previous format compressed uncompressed best regards pagarbiai email replaced email.addr.es follow us twitter adforminsider adform watch short video disclaimer information contained message attachments intended solely attention use named addressee may confidential intended recipient reminded information remains property sender must use disclose distribute copy print rely e-mail received message error please contact sender immediately irrevocably delete message copies. client library use. irunnablecallback used many times assue interfaces clojure invoked java example make use translate time.clj java merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. nits docs otherwise looks really good. users implement iclustermetricsconsumer configure cluster conf file effect refactor nimbus.clj merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. darionyaphet different wait strategy would least worth comparison time. don't really like plan.0.0.x 0.x sound great call 0.x version put master implies released even preview potentially regression yet really confusing going branch continue supported stability lets merge master think going longer stabilize time frame lets create feature branch tied release number personally don't think long stabilize order allow work continue master branch move closer release i've created following branches inactive unless important fixes discovered changes cherry-picked master unless specific 0.x changes cherry-picked master included release e.g storm-0 storm-0 merged master master changes i'm excited keep 0.x sync master think nimbus work needs many eyes much testing possible dedicated branch make easier create preview releases allow users kick tires ease burden parth it's spent lot time keeping work upmerged anyone questions concerns suggestions regarding branch/release management let know. may better revert mk-transfer-fn 0000c00000fc00c0000b00ab00ebc00c00fe00af grouping task handled don't need handled mk-transfer-tuples-handler. lujinhong could squash commits mind merging step let know way prefer. harshach merged think close pull resolve storm-0 dupe storm-0 please let know don't permissions jira grant. retry seems ok two concerns first retrying runtimeexceptions would prefer us restrict retry case conflict curator already lot retries experience curator returns error retry going help second concern quickly retrying suppose common case got unlucky downloaded assignments middle nimbus updating retrying tight loop feel correct interested others opinions don't consider something would block going. hi tried implement pluggable scheduler mentioned http//replaced.url get java.lang.classnotfoundexception error try run understand problem unable find source storm http//replaced.url tried adding demoscheduler mvn compile mvn assembly assembly fails backtype.storm.serialization someone please suggest done server service-handler. check console output http//replaced.url view results. mean available anytypes used keys last two maps regards. hi try use setformat method example know create new template access setformat methaod without process server throw error saying format doesnot exit given key please give answer regards. don't expect problems changes best regards"""
"""af1c54dfaff10f453ee8a71ab9a5e42d8a7daa76351d7ae06b906e249a2637b6""","""new api allows cloudstack copy deltas two snapshots addition cloudstack longer needed plugin scripts plugin scripts mainly try backport code prior even plain vanilla work api cloudstack source code refer xenserver combination xenserver0.00 indicate significance change. 'it's columns it's rows see example statement queryprocessor processstatement reads rows list. apparent behavior nodes simply answer let timeout it's alternative would unreasonable amount work asked extreme cases understanding result oom restarting cassandra clears issue something misunderstood error pasted said aborted scanning tombstones alternative bad performance really wouldn't create many tombstones restarting node impact whatsoever many tombstones sstables cassandra it's current behavior protecting bad design general cases delete whole bunch stuff added time occasionally bad log structured said could fact run major compaction tiered compaction gc_grace_seconds passed remove tombstones re-iterate tombstones symptom cause. event weekend germany used couchdb replication replication crashed able restart actually i'm sure stable replication couchdb case trouble seems get difficult get information working. thanks makes sense given evently it's limitations. code provided first lib think referring code snippet use firebug look output headers response look like would expect log request shows info sure something small missing stumped. confirm using couchapp generate design doc keeping view show funcs files included files couchapp area. http//replaced.url. hopefully i'll rebar point. i'll do something like call like get like. access query data apply update handler like allows update value xyz like way access data sent via post. close jan it's except i'm inserting data i'm running. app production handle backups way you're describing every night push zip archive s0 works great. yes running n=0/w=0/r=0 bigcouch couchdb merge see behaviour today it's couchdb 0.x disclaim say ektorp driver part apache couchdb knowledge http responses post-merge ektorp 0rd party drivers need enhanced understand report responses arise scenarios. know noah unless setup uses heartbeat option new couch might start old goes away crash eaddrinuse without error init it's restart user finds without wouldn't know i've happen centos. ii've answered read old thread. hi easy way overcoming error add hadoop mapped-site.xml files hadoop installation tomcat/lib directory restart oozie server please let us know resolve issue regards. partof gsoc i'm well things mixin work t0 grid mixin works fine components t0 grid tricky apply context menu cell row i.e provide way show different content based cell row value read threads list someone trying extend t0 component like grid something grid intended comment list make new component etc imo would awesome t0 allowed things like t0 grid integration mixin working involves changes gridcell abstractpropertyoutput change introduces another service similar don't break anything tests don't fail components like pull cell data grid rendering cycle used outside mixin grid similar make patch component imo developers don't thrilled commiting changes say would agree case please read make proposal extending t0 concept mixins bit t0 semantics components generally exposed parameter convenient powerful don't box way expose structure component outside world ok argue it's inside component private implementation changed outside component example please consider example t0 grid component gridrows gridcells simple fact grid it's structure change imo possible exposed outside grid components mixins like use long don't ground breaking suggestion radical change simple suggestion get positive comments devs would like work right away suggest mixin applied component able component it's render phase events render phase events embedded components component example let it's possible downside would users start making mixins use embedded components change get removed future maybe control something like exposerenderevents something similar anyone consider extension worth. think lenny reported bug css file still even refers non-existent assets logging warning keeping invalid reference unchanged actually lenny it's suggestion. would cool git place committer playing around lot looking patches private dedicated branch git approach would help lot currently using great intellij history feature track private changes feels like workaround. believe things possible via mix ~/.m0/settings.xml mvn command line options ask. maintainer. buildbot detected restored build builder tapestry-site-production building asf buildbot full details available. principle agree abstraction layer fine thing looking amount client side code thats necessary support web goodness i'm really curious abstraction cut should/could made would reinvent wheel operations supported box dojo extension mean even current code base generic response builder allows add libraries least case half year ago far know nobody ever used possibility think would favor slimmed dojo library well maybe easy option build customized version fit additional needs like custom packages etc. fun need life tried integrating jcaptcha jcaptcha.sourceforge.net tapernate-example application anyone it's interested code it's available via svn http//replaced.url p.s i'll probably make component library make available project. afaik base classes like parent go base package. buildbot detected new failure builder tapestry-site-production building asf buildbot full details available. yes consumes bytes return get less http request it's quite bit overhead handling request it's threshold makes sense push bits embedded single request even separate request iti'd cached see example implements displaying site massimo i'd great configurable setting lets automatically embed assets smaller certain i've thought using dataurls attached. hi really question users list said use outputraw component. project tapestry component report trunk following change author howard m. lewis ship made following changes click http//replaced.url find thanks. single override possibility sounds ok like symbol source logic definitely error conflict. think page really component faciliator components template note safety issue goes beyond performance gateway optimizations features rely annotations may change different times usually null making hard analyze statically could add annotation java code pin easier use property use property place don't think use often situation would make sense would iterating objects component example display object. need construct beanmodel injecting add new column say trashcan provide parameter alas forgot add version model.add pass propertyconduit i'll wait make couple changes. subscribed wiki page wiki category tapestry wiki change notification following page changed erikvullings. http//replaced.url optional modifier relates list cause null/undef return value thrift file c++ code server using boost :shared_ptr perl code client use v0.00 use warnings use utf0 say test case every optional corresponding undef return value delete optional thrift file get right return value optional modifiers related list cause null/undef return value thrift file c++ code server using boost :shared_ptr perl code client use v0.00 use warnings use utf0 say test case every optional corresponding undef return value delete optional thrift file get right return value. rversocket base classes fixed thrift-0 generated perl compiler exception handling code added perl make cross fixed thrift-0 allowing perl listen specific interface construction arguments add support perl client sslsocket verify server certificate authenticity time merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. really cool hear working rust support know others would appreciate available currently listed reserved word compiler/cpp/src/thrift/thriftl.ll due rust using way would add reserved list modify test longer support identifier would cause lot pain others pretty commonly used term would break exiting expected behavior anyone using identifier idl today. npm release removed node-unit http//replaced.url it's deps 00mb install gained extra files different 00mb release near fix. agree still work tag rush would like get better release cadence first"""
"""2d42c4ecc5cfc3ffed8d308fce4b7673b599cefabe78b400cb31223353b3875f""","""hi remove lock file solr//data/index. hi far know never good idea run lucene openjdk either oracle java higher openjdk. hello majisha codepoints content field however stripping method built invalid middle byte exception mind seen even solr 0.x upgrading parts infrastructure solr 0.x got struck confirm content field sent nutch causes. set proper permissions tomcat. either upgrade tika manually use pdfbox. well somewhat problem url isuniquekey contain exclamation marks idea allow escaped thus ignored compositeidrouter. hi use wild cards autocompletion lucene far better tools making good autocompletion since wild card term query passed configured query time analyzer comments use porter stemmer use german specific stem index time tokenizer defined possible behaviour undefined far know. pkill work. not elect leader. maven team pleased announce release parent poms maven default config wish parent asf-parent default config wish default config wish. recently discussion within pmc discovered complete misconception release votes lot people feel voting releases useful pmc wrong digging issue found years ago improved legal requirements releases ensuring vote source distribution convenience binary minimum pmc votes introduced misleading wording voting template binding vs non-binding votes yes non-binding votes binding regarding legal requirements useful feedback looking vote release work real users conclusion decided change wording remove binding vs non-binding count still need find new wording explain issue whole community get feedback already saw experiments around recent votes need feedback change need feedback release votes short need involvement hesitate tell us missed something share ideas hope improve future evolutions beloved apache maven project help whole community regards. maven team pleased announce release maven archetype even redirection package instead packageinpathformat descriptor contains token project old artifact create-from-project command required property archetype-metadata.xml properties work due faulty ordering fileset archetype like done old 0.x archetype archetype-0 allow fields like scm developers licenses etc set generating archetype code archetype add-archetype-metadata. apache maven team pleased announce release apache maven component provides abstract classes manage report generation run part site generation maven-reporting-api ismavenreport direct standalone goal invocation maven-plugin-api isspecify version project isdependencies configuration download appropriate sources etc download page make test. maven team pleased announce release maven project info reports specified project specify version project isplugin configuration mpir-0 create new report show include module different new feature. don't remember exactly think contention around binary distribution pretty much resolved fact _not_ releasing binaries convenience packaging don't even stored asf infra see issue really least bigtop it's others thought. correct wrong release component part stack make certain statement commitment users' community stand wrt quality/maturity component would 'alpha tag enough warrant including release. looks really well bhargav know would relatively easy reuse classes interacting isis obtaining properties collections updating properties executing actions existing project made ui libraries like extjs vaadin ones thanks http//replaced.url. well would field.titlestring null meaning referenced object rendered context parent adapter added idea context evaluating titles isis-0 week two ago works rather well ui ensuring shown parent don't mind titlestring undeprecated though say don't field.titlestring field it's. hi wondering solr following project working create search engine facets potentially hundreds similar say crawling amazon buy.com someone search sites website realise better ways example eventually would build search crawl index say someone would site search digital camera would get results indexes hopefully dynamic facets etc etc done fly ask currently developing webscrapers crawl websites dump data db thinking tacking solr server crawl db problem approach crawling worlds ecommerce sites forever seems solr might read multiple indexes etc many thanks. gut feel thing killing beam usual suspect oom see i've noted nothing wrt logs though 0.x works i'll see processes pids advice offer login run sudo 'couchdb -i it's interactive mode maybe something useful left. seen anything prevent. verify solves leaks web containers would say yes proper life cycle persistent state provide updated. thank answer today tried create big command file push shell 000k insert file said slow inserted i'm accumulo week i'm noob i'm learning actually app store large number data row timestamp family/qualif column catch data json file app scan new records parse record create mutation push accumulo batchwriter maybe wrong something increase speed inserts actually put mutation line it's information use batchwriter insert mutation accumulo right point slow it's necessary use fastest json parser i've found thank much sorry bad english. weird see attachment email sent means might stripped let know still see thanks. short-term think improving logging help debug cases like future good idea lot it's ways actually suggestions long-term prevalent problem throughout client implementation would likely better solved uniform manner. problem around discovered forgotten increase number max maps config killed job changed max maps restarted mapper tablets mappers running locally i'll let know verify went morning. first draft implement range syntax proposed later comments would easy swap ports seemed make sense support range log0j monitor port set system property merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. i'm sure understand alternative solution he discussed explain bit detail please interpretation _think_ proposing i'd rather risk putting words mouth specifically i'm interested think numbered changes propose good ideas agree he need seriously examine api chaining methods four- five-deep e.g new instance .tableoperations .createtable something great user experience determining scope changes another community conversation. almost definitely currently work think created ticket. chance look code yet sounds similar feature described http//replaced.url thoughts. thanks reminder josh testing vote deadline don't found issues far start though anyone else needed. thanks i've bit kerberos stuff real environments seems pretty solid wanted make sure people don't avoid thinking stable. went ahead pushed due lack activity thread consensus believe months open since original patches published reviewboard days since thread opened sufficient time imminent concerns raised issues addressed follow-up jiras initiating revert consensus vote expressing veto. follow-up fixes spaces eol merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. cnn rss feeds please refer http//replaced.url retained sure something needs called notice file regarding follow-on usage thanks. please see http//replaced.url benchmark results overall performance comparable expected don't see degradation significant improvement thank. application it's stuck accepted state long time shut shutting ui command line work help asap would amazing thank. order using regular expression like syntax initially coded idea operator activated/deactivated multiple times lifetime operator setup teardown called exactly far implementation necessitated use activate/deactivate lifetime operator activate/deactivate happens exactly time setup beginwindow unpredictable typically larger activate beginwindow recommend acquire resources e.g connecting high throughput stream activate right first beginwindow instead setup even case exceptions setup returned successfully teardown called activate called deactivate called case error container killed abruptly provision exists. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. i'd great next release ready apachecon april think pr publicity get without effort perhaps tactical standpoint shall call release believe major hurdle us release roman understand 're busy elsewhere could please let us know else needs done start regular builds community help it's highest priority imo couple tickets left unfixed/unassigned bigtop-0 don't resolved time move farther it's lesser half-dozen blockers none look big honestly whole lot active committers contributors wrap-up release couple weeks try upgrade hbase 0.x release might big distortion andrew think cycles else need get done release suggestions anyone wants step rm time around rm mean job rather efficient stick. committed master closing pull request"""
"""45845ce8346c2adfd741afb3553eaabad2c155fe4153e168f3ebb4d99f47f1a4""","""hi someone else might application would like send even though application may send fatal different logger first post makes sense configure logger invoicing later configure logger different properties access two loggers code logmanger.getlogger invoicing please first version two different loggers app using appender set level logger perhaps define another root logger invoicing configure global settings invoicing loggers every single fun. see example projects copied example remember perhaps find usefull. see internal log0net logging see. hi yes mamanged kick live following problem app-domain definied powershell workingdir powershell-snapin path located path way specify location configfile let know still need see lines code. message local queue service host processes time beeing bit specific service independed application holds references logging application isdlls words using generic service remote applications customized version every single kind objects log queue strings perhaps xml real bussiness message objects service hold logic getting msgs queue without looking inside forwarding ado far see thing win kind async logging price pay extra level design service app depends additional tech msmq think non-blocking app ok msgs logged possible wrong order long timestamp shows real date simply build depender following decorator pattern new appender holds property referencing real appender ado-appender case every time appender asked logg new message creates new thread starts leave inside new thread recieved msg passed real appender less lines code whole appender every logginrequest aync creating new thread even faster writing msmq plus add additional logic inside decorator wait writing db long x msgs recieved using threadpool store msgs long network unavailable whatever use apps becourse totaly independend rest app would put inside independed course appender reused appenders ado simply adds asynfunctionality every appender choose. know set properties depending appender use declaration follows generic description given docu reading docu know discover possibilies appender provides knowing follow docu setting properties config given example tag tag set top level config declare custom levels extensions provide said found docu maybe tags best thing would xml-schema config reads issue tracking config follow strict schema searching complete plain list xml tags config like etc. hallo able reproduce problem config runs without errors system time study log0net sources tell totaly way configure loggers reason repository stores logers hashtable logger used key table impossible store two loggers happens xmlconfig like loops root xml elements every logger element calls getlogger get logger wich create new logger return allready created exists level set/overriden appenderrefs erased recreated logger allways configured like last definition configfile profe simple hard say fileappender crashes using target file processes perhaps log0net able open filehandle runs write closed appender. perhaps simply use threadcontext store additional infos could store additional infos like. hi think line wrong use implementation ilog wrapper implementation checks implementation sets locationinfo onto position point _log instance code logging change ilogger log messages like copied code logimpl.cs loggerwrapperimpl.cs damm good peace code fun reading. well first code filters without toughing log0net code simple add fqdn config simple prevent log recieve specified level use buildin filter levelmatchfilter configfile possible know filter log0net defining custom loglevels easy without toughing code see trace example provided sources. hi shouldnt becourse created instance logger stored repository aplication ends use loggername get instance problem use logmanager repositorieisgetlogger member create instances wich result multiple instances logger need implement iloggerfactory interface create custom subclassed logger configure hierarchy use factory use logmanager repository create loggers instance created. research checked log0net sources error thrown appenderskelleton base appenders m_closed m_closed set reads code docu programming error append closed appender perhaps really bug log0net appenders post simple possible config reproduces error system try reproduce debug log0net see whats going wrong appender. thx pointing deliver messages appender fatals delivered. hallo perhaps provide hints get complete list log0net studied great docu examples googling found important tag listed docu ok least beside checking source parser anywhere full list available looking something special know possibilities. hi basic log0net things named logger using loggername allways work instance logger configured named logger need repository stores reference configured thought workflow like configure logger use custom logfile bad bad softwaredesign yes would affect threads use logger use two explicit loggers every location messages calculation lets say following locations location0 used logger could new logger inherits level appender message calculation like configured configfile every message goes child loggers logfile location see simple custom logmanager providing new members getmessagelogger getcalculationlogger location parameter return configured logger ilog precise log0net.ilog. problem use every message needs checked matchs filter perhaps set filterlevel inside definition would speed things. see point building sources coding log0net sources simply use log0net projects way add reference release like debug log0net.dll included inside zip thats. lgtm build several times succeeds resulting works tomcat awesome. small easy hopefully non-controversial grammar fixes definition uri refers rfc rfc obsolete replaced rfc merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. problem handle url path part might hacky another wiser solid solution let autoconf set right urls depending docs built say template changes docs_url /api/database/changes.html highlight=changes post db-_changes depending 'build_docs 'true replace docs_url 'http//replaced.url '/_utils/docs respectively. looks like readme touched merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. looks nice better think remilito tooltip scatter chart good changed keeping focusing marked axis. soralee conflict file could resolve. patch adds x-couch-rev headers responses custom default_handler act db changes e.g create tasks custom default_handler creates celery tasks via python process wraps couch_httpd_d handle_request/0 function looks response creates tasks needs document rev put/post/delete request available response level response body sent socket inside black box somewhere headers available decided add rev json responses update function x-couch-rev using update function rev available though x-couch-update-newrev added large amount small databases potentially thousands practical listen changes every db another recommendation achieve without patching please suggest alternative merge pull request git repository running alternatively review apply changes patch. pr editor hide page refresh editor gone pr tested hide editor mode paragraph configure show editor correct breaking changes older versions needs documentation merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. starting error i've found number text references brooklyn.entity.basic comments others blueprint unit tests. check packages.ubuntu.com requirements version/packages good"""
"""b13fd8f0636336d4fe72610e907dd976f348a4f2a632d66b0ce15063e2391ff9""","""hello everyone hope email finds well hope everyone excited apachecon would like remind couple important dates well ask assistance spreading word please use social media platform get word visibility better apachecon link main site found http//replaced.url planning attend apachecon north america may add-on option registration form join conference discounted fee us available apache big data north america attendees please tweet away look forward seeing vancouver groovy day. ago similar issue least back think possible use solr iscopy-field support create boosted version field copied field multiple times. hi frederik figure solution problem asking recently ran similar problem similar setup shards server occasionally query long time occasionally see timeout exceptions http requests e.g restarting jetty seems clear problem temporarily looking code solr handles distributed requests got interesting smells would surprised issue related using regards. hi frederik directly run issue solr experienced similar issues related context case custom made solrj requests generated aggregated/analyzed results load testing ran different issues load test software issue scaling assuming case seen happen e.g limit max parallel connections client used talk solr needed tune solrj settings httpconnectionmanager heavy load running free connections given got shards request going spawn http connections know top head manages connections whether possible tune stack trace sure looks like blocked getting free http connections needed optimize configuration jetty jvm gc etc lots knobs twiddle better worse. think need create separate background process least case web crawling challenge efficiently use samza process simultaneously fetch many urls increase complexity process iscode wind manage either multi-threaded async fetch state hadoop-based crawlers limited number parallel reduce tasks fetching see nutch bixo examples e.g fetchbuffer another project involved past. hi running problem queries distributed among multiple shards return binary field data properly hit single core xml response http request contains expected data hit request handler configured distribute request shards xml contains b b looks like wind getting .tostring data actual data anybody else run done fair amount searching hits yet next step create unit test solr nobody raises hand walk thanks. general yes subset terms occur frequently remove terms easy longer search either part query phrase combine common terms following terms works well bit complex significantly grow index either requires data analysis generate target set common terms. area might issues date range queries many docs run oom errors recent thread yonik others good suggestions ways avoid problem know impact would merging results use date ranges guessing low yonik would know best well 0-server configuration would work would 000m docs/server large number even data/doc small 0k real performance going heavily impacted nature data types queries shall need think distribute data avoid performance constrained worst case searchers nutch wound add termination logic avoid long-running queries clog things primarily dealing load best first step create single solr representative data see well performs issues going around limits box 000m docs versus distributed nature though keeping servers alive happy significant ops task finally would decide early whether search query words ok result set happens missing doc server timed looking query-type solution solr would less interesting. csvloader user found/fixed bug involved active development/maintenance piece code james make progress merging support csv dih great. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. thrift framework providing cross-platform rpc serialization thrift community continues see consistent growth increase new contributors since recent release resolved almost tickets working towards milestone release apache thrift pmc voted switch svn git primary management website svn repo updated links new repository seeing overwhelming positive response change. sorry realize thrift moved git forked mirror github submitted pull request http//replaced.url however original patch applied without conflicts compiled without warnings show last git commit 0a0c00a svn revision r0000000 give information equivalent without flush attached patch accomplishes minimizing code churn removing using namespace std introducing static variable endl. pr developer able debug issues dependency resolution easier pr tested breaking changes older versions merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. thanks sharing work checked code looks interesting please free create issue/pr don't mind thanks. updates make interpreter updated language zeppelin interpreter separate jvm process default updated make interpreter configure interpreter better explain process merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. goal community merge point two different r interpreters another still trying wrap head around difference. first would like introduce pedro toribio responsible product development sofia0 indra http//replaced.url evaluating use zeppelin entry point information sofia0 need develop zeppelin interpreter use sofia0 i've run problem able debug interpreter debug zeppelin-server dependencies remote interpreter runs remoteinterpreterserver lose control consequently debug interpreter 're done address development interpreter reason post indiqueis us whether possible debug interpreter running platform carried http//replaced.url replaced email.addr.es minsait.com. 0ambda thanks effort tested works well expected maybe it's nit colors different. pr pr tested breaking changes older versions merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. pr spark default value it'spark.executor.memory 0g 000m let it's follow change pr x replaced occurrences default value spark.executor.memory '0g' tested checked people it's eyes breaking changes older versions needs documentation merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. separate job manager notebook server good large amount duplicated code introduce maintenance overhead best extract common code make notebook server job manager server share. hi heiko it's really great hear interpreter prestodb think issue discussion presto db far please feel free create new issue best. hi cooperating frank seidinger j0me plugin need set bootclasspath compiler plugin done pom like however use unified emulator interface determine bootclasspath automatically configure compiler accordingly able access uei know access current container evaluator least test harness creates container uses stub evaluator get container running maven instance evaluator pass configurecomponent alternative frank proposes introduce wireless toolkits plexus compiler level find information would appropriate point view regards. able reproduce blueprint tests camel testing fix likely need respin build thanks finding. cxf contains several new features complete list changes required migrate cxf see addition apache cxf community released patch releases contain several fixes bugs issues users encountered downloads available information see feedback questions would like get involved cxf project please join mailing lists let us know thoughts. it's race condition evidenced lines /var/lib/cassandra/data/system/locationinfo-e-0-index.db file deleted file tried read create ticket. center point query circle indexed point query radius matched plugged numbers perhaps misled projection using view map far away points fyi default disterrpct fine general don't issue almost never use field means indexed non-point shapes rectangles said use indexed terms unless small rectangles relative grid resolution meter case using disterrpct=0 query safe hand. hi must supply dates format iso-0 indicate time-zone offset occurring reduce cardinality day level instead second currently performing date supply include datemathparser operations supply 0:0:00z/day think course loose ability search based granularity finer day date get back i.e stored value rounded date date prior rounding yes certainly need re-index since architected indexing strategy know go i'm sure aware update individual fields way current strategy involves periodic updates could strategy simply waiting data eventually gets re-indexed it's harm dates rounded it's rounded current problem sporadic oom. hi hoping get answer geospatial topic links basically confirm approach wanted work ok similar even bigger amount data plan instead good enough quicker implement"""
"""f8edcfb93545238bb180cd8e2deec494f9b41dc5bc16de388bc5e0a6a542cc71""","""hi first sorry form bad english new user apache solr read documentation check find solution problem need words order sensitivity query example two something two something three something four four something three something two something query two result something two something three something four two like query four something three something two something need result disturb returned query two result four something three something two something two like query something two something three something four need result disturb returned possible yes regards. hi indexed various types documents fields author already able use facet choose values narrow given use case runs something like example list authors matching steel count number documents associated user chooses entries gets document results author present sideways essentially present facets first results choose facet show results facets show match fashion based analysis chain based fuzzy search search term entered know get list author facet values documents steel matches author field problem author multi-valued field returns facet values match steel values author field really ugly approach work time hoping someone better idea read facet parameters searched various places across anything like use facet.prefix. looking facets begin search term looking facets contain search term could show anywhere fuzzy handling may exact matches anyways masochists know approach search something like searching documents author_boost field internal author field search term mark twain proximity distance somewhat arbitrary return hits really kludgy bits hoping enough hits hits would include mark twain however specify fl fields return field never exists getting back empty elements xml highlighting author_boost field tells us value search terms found sort document kind random sort try get many distinct highlighting results possible i.e score sequencing would cluster highlight values post processing build set strings highlighting results removing highlight elements intent becomes set mark twain strings chug facet_field list author_facet preserve entry set strings built highlighting results present result back users along counts facet really ugly usually work help visualize isexcerpts response join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. reading reply separate thread reminded ping handler might seem like good solution using ping handler would require code knows specific core server knows solrcloud collections collections admin request outlined previously work without code needing knowledge cloud contains solrcloud almost guarantee collection cloud accessible node cloud node functional extremely pedantic check ping handler defined config collection could use handler make sure collection functional well node check access collection named foo specific node start code sent previously replace last two lines code ping handler different name/path able use setpath method ping object change process method thanks. max= don't get error looking code lengthfilter don't see way behave differently max vs. max higher interfaces classes involved use int length means work perfectly numbers thanks. admin ui cloud-wide information like yet would need visit enough nodes find replica shard get info replicas add summary it's faster go query tab shard replica collection run default search look numfound results plans underway collection info admin ui well core info since 0.x source code branch eliminated bugfix releases seen 0.x collection info show admin ui least may later hopefully year would surprised next year rolls around first thanks. knowing number nodes shards replicas enough information even make guesses http//replaced.url even lot information recommendations made would guesses guesses might completely wrong represent lot expense really need exact kind setup need affected great many things it's request rate complexity queries contents index index solr cache settings schema settings number documents number shards amount memory server amount memory java heap even phrase improve performance vague kind performance issue thanks. it's http//replaced.url part plan able pre-configure everything xml right run archiva default port start app unpack edit file. hello wanted try archiva got source svn failure compiling archiva-reports-standard failure failure reported md0 checksum file exist using maven ideas thanks help. personal technical community goals year give visibility it's going let know others specifically devs ml media don't marketing miss days i'm partially responsible couchdb full otp rcouch branch land later today tomorrow sidetracked thing yesterday real couchapp engine around something new use full power otp embedded easily message passing improve view protocol make efficient possible fully embedded couchdb line really needed code experimentations last week gave convinction could use v0 without embed bloated nodejs propose alternative innovate rather embrace world like others think distinct views couchapp engine require distinct features hopefully able push code month use http transport today mixing core db code http api revisit auth handling remove users references core security doc propose callbacks events allows transport top manage authentication ie authz done http level http api i've started experiment http//replaced.url maybe use websockets improve lot way track improve user dev documentation collect experiments around give full panorama couchdb usage things view enough rest make couchdb interresting users technical level concept gain traction don't go necessarily like others ex nodejs using real technical reason marketing best evangelism don't don't pushed new features interesting concepts apache couchdb however improve user experiment could done addressing concerns users couchdb db couchapp users technical level documentation quick notes concerns couchapp trend among users say couchapps useless enough either remove use way replicate app handle logic either browser using external code top proxy behind workers partials responses mostly hack don't really address concern i'm quite annoyed hear people talking like thing written stone cadon't changed think better today couchapp side use power replication html0 propose new framework possibilities embedded external scripts used erlang don't enough need use specific library work like machine learning processing db level using couchdb database bigcouch rcouch give feature part lot improvements around made really wish core couchdb still used single db without cluster need don't use couchdb big dataset.s use small device mobile sorry don't use pouchdb touchdb use core code voila hopefully able help topics apache couchdb least rcouch new addons course others welcome help. thanks explanation raghav workload concern probably fine run tests pr update although may necessary. hi think trim api generally useful consume based retention well use cases probably optimization favor users could implemented top maybe implement first let applications trim point put burden application side bad downstream consumer group future find use cases multiple stream consumer groups need coordinate among broker side help would make things simpler add regarding relation kip kip-0 high level similar i.e trim timestamp vs. trim offsets would worth thinking together kip-0 search messages timestamp essentially translates timestamp offsets kip-0 built top trim offsets interface translating timestamp offsets jun suggested implementation kip-0 discussion thread introduces new trimrequest would look see could used kip-0 well thanks. sounds like don't latest snapshot version maven-scm. ok tracked infra-0. another point i've done another test node cluster created keyspace replication factor inserted data run full repair node make sstable appear disk run multiple times nodes due incremental repair altenate commands files change disk means anti-compaction used wall-clock timing first method takes average seconds second average seconds it's mutation commands"""
"""8d813ba09858cc9de6303df077cde873630baa77238f6ea9e7a073808a56522a""","""clearer rich text document xml make work example docs folder read solr book tried samples could make work. read solr book book way read need help mean time using example solr system send word document using curl system full path document tried various commands gives stream errors documents server solr second server would like experiment home indexing documents server giving experience outside b trying grasp issues design read books struggling grasp ideas yet using .net connector recommended webpage connects solr adds extra information solr query client see later returns data normal design using shopping trolley design orders created comments added order processed search order information many indexing requirements index many database tables proposing use solr instance indexes data use example text field copydata extra fields tables solr book gave short shift database indexing tool good tutorial using solr sqlserver many tables thinking might join lot might easier create xml output send pros cons way restricting searches client signed good idea secured behind application adding extras thinking would index order notes separately order alternative value note field order keep deleting best way keep replacing whole order join notes search like feature suppose solr index normal map url code using return data solr hope help. read various pages used curl lot figure correct command line add document example solr instance tried things however seem file server solr case pushing document windows machine solr indexing. iscomplete example. confirm download mirror synced merge discussion. fixes issue assembly plugin created source package without 'flume-shared project maven compile failed merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. 'it's good undesrtand details yes install vs verify notice it's good thing tm check releases vote verify instead install local repository wouldn't get polluted local build release later official build central change jankins configuration verify instead install regards. thanks let put way tutorial going cost 0k. talking friend attended javaone year apparently major keynote featured interviews executives behind nhl.com anyone see apparently sun claiming java made site success however nhl.com runs tapestry came keynote. intention test downloading actually remove trying fix integration tests back order plugin went ibiblio. atlassian don't support anything http//replaced.url practice think 're even aggressive it's probably problem moving justified said rest api don't need enabled people may rss anyway depends whether part broken causing extra work. thanks. cool would good get another list looks major error information work build outside ide looks correct use fewer build. hi cluster capacity used ram cores cpu number regionservers testing sorry indeed mentioned website missed thanks. believe solved based irc conversation something network cable unplugged -p. looks like needed add /accumulo instance.volumes property. makes sense. change gridpager latch immediate page range around upper lower bounds available pages ensures always least range pages select even first last page set. convert immutable data convert usages string content use project http//replaced.url commit http//replaced.url tree http//replaced.url diff http//replaced.url. preview releases vary stability always actively developed modified look new development activity beta release meaning concentrating adding stability documentation fixing bugs release candidate expected near future keep date latest events tapestry world addresses large number bugs including specific bug related using tapestry groovy version changed resolves number nonsensical errors using jdk approach localization. always loaded no-conflict mode part core stack project http//replaced.url commit http//replaced.url tree http//replaced.url diff http//replaced.url. preview releases vary stability always actively developed modified look new development activity http//replaced.url tapestry includes subset eventual functionality tapestry it's available solid useable increasingly stable feedback mailing lists encouraged keep date latest events tapestry world new improved tapestry release provide great amount value existing tapestry users changes ranging 0-00x performance gain general rendering pages overall improvement expansion tapestry team growing lately thought time introduce new members daniel working tapestry long time marcus programmer-turned physicist since gradually migrated c++ land world currently holds pet-belief scala java tapestry struts hibernate integration solves painful concurrency bug well many fixes alexander kolesnikov published new series tutorials tapestry find summary ejb0 integration it's designed use example starting point ejb0 integration it's designed use example starting point tour hello world log page real database transactions user forum place share questions answers ideas kent tong it's book updated t0.0 new chapter ajax included first developers often differ exactly percentage statistics made thing usually agree last year main push nothing special get done could get computer boring part could focus attention important component allows condense hundreds lines dry utopia efficiency builds edit forms objects automatically using bean introspection per-property basis allows use recursive properties eminently flexible keep date latest events tapestry world persistence fixes problems quickstart maven archetype allows expansions inside attributes rather inside body text well many fixes new improved tapestry release provide great amount value tapestry team growing lately thought time introduce daniel working tapestry long time c++ land world web-apps lucky enough discover tapestry currently holds pet-belief scala java tapestry hibernate integration solves painful concurrency bug well many fixes alexander kolesnikov published new series tutorials tapestry ejb0 integration it's designed use example starting point domain classes may added ejb0 integration it's designed use example starting point domain classes may added tour hello world log page real database transactions tour shows tapestry app hangs together i'll see ejb action user forum place share questions answers ideas kent tong it's book updated t0.0 new chapter ajax included first hibernate it's organized around real application implemented small go great way towards filling gaps people it's tapestry knowlege developers often differ exactly percentage statistics made thing usually agree last year main push production frameworks aim make simple things automatic rather nothing special get done could get computer boring part could focus attention important component allows condense hundreds lines dry utopia efficiency builds edit forms objects automatically using bean introspection per-property basis allows use recursive properties eminently flexible tapestry top-level apache project new home page http//replaced.url. comparing lists provide details output system.err values lists easier analysis differ. add committers list. stable releases relatively free critical bugs considered _safest_ option stability requirement tapestry released apr see release notes|release notes details tapestry released jul see release notes|release notes details note _primary_ distribution method binary source jars via maven repository binaries sources available zip archive tapestry downloaded either binary source format easiest way download tapestry dependencies using maven described getting started see release notes differences versions upgrade stable releases relatively free critical bugs considered safest option stability requirement tapestry released jul see release notes details note primary distribution method binary source jars via maven repository binaries sources available zip archive tapestry released jun maintenance release addressing minor issues tapestry see release notes details features upgrade information tapestry stable release tapestry released apr tapestry addresses limitations improves performance scalability tapestry final version tapestry released sep improved upon early tapestry releases bug fixes ajax support performance enhancements see tapestry release notes looking older version tapestry try archives releases development less stable alpha releases often incomplete beta releases occur lead creation stable release source checked anonymously svn command access behind firewall refer documentation scm used information access behind firewall subversion client go proxy configure first edit servers configuration file indicate proxy use file it's location depends operating system unix located directory ~/.subversion windows appdata \subversion try echo appdata note directory comments file explaining don't file get latest subversion client run command cause configuration directory template files created example edit it'servers file add something like http-proxy-host your.proxy.name. may use file except compliance license may use file except compliance license datefield component easy use generated result element it's really necessary newline file may use file except compliance license use delegate multiple times template render block passing parameters delegate block body reference render variables using binding prefix parameter note default binding prefix informal parameter values literal may use file except compliance license element around banner message around unnumbered list allows slightly shorter urls allows multiple components within container generate identical may use file except compliance license hides element removes dom entirely may use file except compliance license"""
"""999076a19d33ea80f5d6380fe64bf5094611a510bd64fd29ef8ff26fb467c42e""","""set apache reverse proxy couchdb problems replicating idea setup apache later handle authentication however even disabled get following failure trying replicate apache runs port forwarding couchdb error get error part database actual document occurs seems vary unable reproduce behaviour smaller database well mb already tried dumping reloading database contents nothing changed proxy configuration follows hope fairly standard proxyrequests allowencodedslashes normal requests couchdb futon interface seem working fine proxy ideas problem could thanks. always minutes. hi devs possible push fluido skin work happens completed fixes time ago would like another bootstrap 0.x based released modification work bootstrap 0.x begins lot stuff time. quick search central shall even apache project adhere convention receive. vote please period already due. dependency different poms dependency think like inject annotation guice thank. parse.vm lives next template.vm call parse parse.vm macro available rest though tried probably create custom skin really create. course piss lot people would course several reasons people not popularity old collisions etc even enforce happen maven added plugin dev center. done r0000000. probably query resolution unresolved. vote please. hi paul looks way better thing though alread fixed issue bound distinct maven version change e.g. mng-0. hi folks herve quite busy right preparing doxia sitetools along maven site plugin several tickets would like fix changes impact skins provide need adapted close look skins updated years active improvements happen fluido skin given fact probably work none changed html0 would like retire skins keep two active would ultimately mean two updated later proposal please share opinion agree start vote retire. hi previously suggested makes sense retire skins updated long time resources maintain properly last releases therefore propose retire skins vote successful make final release skin making clear skin site retired source code moved retired area subversion repository process retiring skin described though plugins process universal vote open hours yes time would ask kindly already cast vote revote make. following services unavailable upgrade. unreachable two different carriers. fact mentioned front page skin addtionally maven. read mind idea weeks option work strongly support remove build files ant maven maven along maven ant tasks need removed need free use aether ant tasks. terms contributions zero someone keen enough code prepare patch simply lazy need fork clone push create pull request even work see benefit. mshared-0. problem intimately connected solr batch job hadoop distributed real-time query scheme hate add yet another complex framework solr rp job simply problem transform solr query subset query shard let solrcloud mechanism well aware 'zoo alternatives evaluating don't get solr. particular case came solr.py http//replaced.url way going become official solr python client would nice someone knows unicode/python go fix someone wants point place lists invalid xml characters probably figure. thanks steve smoking gun tested see issue don't upgrade yet however found setting mm=0 sorts case giving following still testing however looks positive far thing still noticed single word synonyms output synonym within debug query multi-word even issue honest sure means something noticed difference parsed query thanks help thought going mad. hi he gave excellent explanation lucene might know ability plug codec full control postings stored format used files written field within segment codec ie field foo standard field bar uses pulsing instance case since pulsing wrapper around standard codec codecs try write set files segment reason introduced codec segment really ordinal build set codecs used segment ordinal used build filenames case codec suppose standard codec ord ordinal used files codec writes files without ordinal nrm fdt fdx fnrm written indexwriter directly functionality behind might exposed via codec sooner later afterall lucene functionality system fine right thing feature codecs introduced aka trunk thanks simon. hi working solr web interfact calling well sized index don't remember changing anything significantly past hours query like returning documents query specific text return documents relevant query although issue affect application got curious happening suggestions anyone. could create option would allow turning updatelog update request useful would something could specified dataimporthandler request way could full import log performance solrj maintains index logging would enabled thanks. i'm trying prevent search going across multiple values multivalued field running issue i've read standard way positionincrementgap larger ps value however don't make phrase query another field searched specific i'm indexing collection music albums multiple tracks album artist searches contain artist track don't make single phrase query indexed across two separate fields small ps large posincgap don't anything way get past view message context http//replaced.url sent solr user mailing list archive nabble.com. don't said version solr 're using asking replication built-in see http//replaced.url slave don't block update happening automatically switches updated index upon older versions solr used rsynch etc best. solr admin exposes time last commit use http//replaced.url. instance solr don't start since added replication startup using solr see index contains 000k documents total 000mb removing replicate startup instance starts without error found needed replicate startup version information master restarting instance something special needs done using replicate startup bug solr portion stacktrace thanks. wiener comparator applying current schema byte value read disk schema read describes value components it's trying apply current schema cast bytes comparison something must gone wrong deleted part statement store schema data problem changing schema incompatible way existing data nodetool scrub probably best bet i've checked handles specific problem general drop rows sstables read problem best thing snapshot copy data node qa box run tests hope helps. little topic remember reading facebook web user give focus inbox search box sent message back warm appropriate caches think may original paper interest way warm key cache say sending get_range_slices command asks columns don't particular need wondering. current issues delete know safe upgrade things check read column back cli timestamp expected proven otherwise assume it's client side thing add logging app turn debug logging cassandra notice fault stop running task try produce cli works cl small possibility could connect node get delete warn messages dropped messages still troubles provide info delete performed.a. general symptom mvn clean compile don't always work mvn clean package dependencies accumulo sub-projects require jars exist rather compiled classes root cause seems current configuration maven-dependency-plugin copy-dependencies execution get better move wikisearch outside trunk however suspect could generally solve problem reconfiguring copy-dependencies execution differently anybody know properly. another way say cross-data center replication accumulo left layer top accumulo application space cassandra supports mode bigger write replication write quorum allowing writes eventually propagate reads happen stale versions data increases availability cost consistency important dealing links less reliable higher latency nothing special lower consistency application space might slightly less. hi reading email twice skimming w0c stuff planning wiki page strong advocate getting done first don't even need w0c data structures agree range api complex imagined implementations seem differ across extending hacky code robustify selection looked project possible alternative whether w0c whatever data model standard correctly implemented relevance right web app written definitely adhere first big milestone happy contribute package uses reactjs highlight stuff demo available replace dns dare expose http//replaced.url try annotator selecting text please tell something interested best"""
"""35d689bead6b31c10cbf7e8bd33da99ac9e6dd77ca7675f3226cb4a4faa88f56""","""hello quite new solr trying get grip currently reading enjoying solr enterprise search server book trying failing need advise following given following schema subscriptions would like group accountid facet isclosed accountid addition would like see number created subscriptions since given date would possible figure grouping something like advise would highly appreciated regards. ok shall try answer would correct give data need indent=on f.createddate.facet.date.start=now/days-0months f.createddate.facet.date.end=now. yes see question bit confusing thanks answers try clarify bit query date field validtodate value field present documents would like get number documents given date range r0 value validtodate i.e documents number documents given date range r0 value validtodate question really possible query need two queries facet.range.other=all help way. hello following faceting parameters gives unwanted non-null dates result set way query index give non-null dates return i.e would like get result set contains non-nulls validtodate faceting non-null values validtodate would like get non-null values faceting result response example gives results non-null validtodates would like get results non-null validtodate facets write start wonder possible facets dependent result set might better handle application layer extracting help would appreciated. hi moving towards embedding multiple solr cores versus using multiple solr webapps way simplifying build/deploy getting control startup/update process would hate lose handy gui inspecting schema importantly trying queries explain turned anybody tried dual-mode method operation thoughts whether workable issues would taken quick look supporting java code ideas would needed hoping easy approach whacking admin support code thanks. well ultimately heading towards single multiple embedded solr cores case could .jsp-based gui/admin functionality peacefully co-exist use embedded cores description roadmap solr gui example assuming files still exist going forward become much gui layer top new/beefed plan eventually get html using json responses request handlers thanks. hi have got ancient lucene tokenizer code m trying avoid forward-porting not think isequivalent solr specifically isapplying shingles output something like worddelimiterfilter e.g mysupersink gets split super sink shingled re using shingle mysuper super supersink sink not follow wdf single filter shingles not created across terms coming wdf ispieces generated wdf actually way make work solr thanks. hi renee mike right question post users list yes separate setconnectiontimeout used though familiar possibility ping response handler responding connection established getting data back. turn facets query facet=true facet.field= shall get back distinct values though might play settings e.g facet.limit=0 get results need. hi yonik ah explains little seen resin parameter used inside init method typically specified using tag know resin-specific yes would easier update resin runs unpack .war kind pain way knew get working versus jindi approach got special reason ask ongoing support init-param technique. available cores dynamic cores requirement custom code wasted. hi feel like must missing something working customized version supports distributed searching multiple local cores assuming support searchcomponents handler needs create/maintain responsebuilder passed various methods responsebuilder finished list shardrequest objects requests received responses shards inside shardrequest responses list shardresponse objects contain things like solrresponse solrresponse field shardresponse private method set package private appear like easy way create shardresponse objects searchcomponents expect receive inside responsebuilder put custom package shardresponse call setsolrresponse builds run locally deploy jar code runtime get illegal access exception running jetty make work re-building solr.war custom pretty painful thanks. way mean configuring current extractingrequesthandler fundamental issue solr uses tika prevents extractingrequesthandler modified work way seems like useful configuration regards. hi would copied/pasted schema fields testbed schema based version solr using back dark ages version handy comment warning changing version fields multivalued default would checked docs realize behavior changed days would used http//replaced.url examine field would seen thanks. useful use morelikethis support see http//replaced.url attached patch. receive documents day various sizes documents could pertain contacts stored database could include file maintain list contacts related involved file know never exact i'd like index possible names text attempt identify files document might pertain looking files tied contacts contained document i've found regex code parse names text anyone ideas set index currently approximately documents library. really don't like giving unhelpful responses like don't think it's way go solr-user mailing list end-users regulars including little experience lucene even though solr lucene application source code number lucene-specific discussion places available thanks. hello writing xml hand xml writer cause problems exception says latitude converted think use java0 unfortunatly stream writer filter invalid xml characters point helpful website hope helps. search engine groups text give users ability search ability conjunction selection searching conjunction means user able search fields fields i'm realizing give ability search everywhere archieved copyfields parameter user search bunch terms different groups i'm using syntax give oportunity search fields ability specify field0 field0 term view message context http//replaced.url sent solr user mailing list archive nabble.com. unique it's index added worked mentioned requirement distributed search thanks jaikit set cores single collection schema different index set unique required field false run query single core works fine add shard param point different core request fails npe looked source code querycomponent line isresultids.put sharddoc.id.tostring sharddoc looks like sharddoc id.tostring throwing npe.http //grepcode.com/file/repo0.maven.org/maven0/org.apache.solr/solr-core/0.0/org/apache/solr/handler/component/querycomponent.java querycomponent.mergeids 00org.apache.solr.handler.component.responsebuilder 0corg.apache.solr.handler.component.shardrequest clue set incorrect appreciate pointers thanks jaikit. wanted eliminate administration core web site could eliminate either solr.xml remove solr.xml file mentioned page manipulation example adminpath= /admin/cores configures access via http//replaced.url attribute specified dynamic manipulation unavailable. thanks erik reply know it'set it's goal give better example add another list_c look like however add list_a reindexing documents depends goal i'm guessing 're using atomic updates case need use set rather add former replaces contents see http//replaced.url 're simply re-indexing documents send entire fresh document solr i'll replace earlier document best. two databases unfortunately separate get imported solr database primary key time concerned comes importing two solr item db therefore order keep two separate databases two different uniqueids wondering kind options append letter primary key db even possible. bet it's point covers believe it's simply thread-local hashmap nothing stored loggers code using need careful remove variable mdc 're done. mandatory. yes don't need either don't need scoring. try looking logs lucidimagination.com"""
"""e1e776cd2f7efc4f5b2ac35594d0c991db9b41bceceb2e089500d784af1c2c65""","""hi using solr store time series data log events etc right use solr cloud collection cleaning deleting documents via queries would like know approaches people using way create collection receiving post inexistent inded could use date part index cleanup process would delete old collections. hi hoping someone thoughts running solr patch solrqueryparser.java getluceneversion calls use lucenematchversion directly running solr cores schema use solrj run searches java app running jboss jboss tomcat solr index folders server case relevant using jmeter load test harness running solaris processor box 00gb physical memory run successful load test user load rate solr searches second solr search responses coming 000ms tried ramp far tell solr hanging logging statements around solrj calls log long query construction takes run solrj query log search times getting number query construction logs corresponding search time logs tomcat jboss processes show well cpu still top processes cpu states show around idle usage two java processes around 0gb lwp state shows sleep jboss still alive get piece software talks jboss app get data set things use log0j logging solr log showing errors exceptions indexing searching back january load testing prototype problems though solr time ramped beautifully bottle necks apps solr benchmarking descendent prototyping bit complex searches fields schema basic search logic far solrj usage ideas else look ringing bells send details anyone wants specifics. hoping sort work multivalued field normally trying makes sense example two authors document would expect document sort issmith jones probably specific rule choose least generic sense wanted example able sort first author could index first author separate non-multivalued field purely sort still authors multivalued field join conversation like us facebook follow us twitter. set new field concatenation field category group facet many combinations would talking field run query something bit similar wanted author search author field documents field set based author search well field based author faceting search author field return results facet values display facet values counts users select issue new query return documents author facet value. setting reading thread seems could search macman hit macman maecman since seems could map single replacement mapped multiple times generating multiple tokens thanks view message context http//replaced.url sent solr user mailing list archive nabble.com. make field multi-valued field indexing time split text sentences putting sentence solr document values mv field think normal highlighting code used pull entire value i.e sentence matching mv instance within document i.e put overhead index step rather trying search time. put default value group_id field solr schema would work e.g something like unknown shall get original group_id value still grouped together figure display time. i'm trying build following solr cluster seems solr don't use different config trees zookeeper certain time even manage get state 'configs node solr seems like picks looks config files single picked thus get messages like zookeeper node found assuming see node zookeper configs node looks like i've tried many different ways solr.xml editing none helped setting full paths collection error says invalid path setting relative paths collection error says cant find zookeper node searchs defaultcollectionpath+relativepath running core solr doesnt see collections idea even possible current solr version thanks. issue commit delete specify delete command. moved master/slave believe happened i'merge help least don't see performance problem master. someone confirm needs reported bug exception patch seems different solr-0 issue sporadic occurs every time graphite reporter regards replaced email.addr.es wrote. please southern california area. search 'referal_url http//replaced.url wildcard start cause dictionary scan every term search unless use reversedwildcardfilterfactory could cause slowdown i/o bound even cpu bound matter. hi guys i've noticed new features solr termscomponent enables autosuggest puzzles actually use application autosuggests case insensitive difference it'san francisco it'san francisco i've tried 'text field it'string field joy string providing best result still case sensitivity moment i'm using custom field converts field lower case allows submit query lower case better good results point email find get autosuggest return mixed case results require lower case query send. first question whether 're sorting and/or faceting many unique string values i'm guessing sometime questions help pin fields sorting fields faceting many unique terms see solr admin page best. believe sortmissinglast fieldtype attribute fieldtype sortmissinglast= true. currently logged normally container access log currently logged currently logged number documents matched it's higher level concept rather specific request handler info returned responses though it's response would largely meaningless general could determine number parameters number docs matched better number might response normally fields could small large faceting highlighting data could dwarf size/speed due main response. hello far know don't remove response numfound start docs response prepared solr apart removing header don't anything. hi trying upgrade solr instance use solrj api fetch data index see solrj version compatible index generated known issue workaround thanks. yes url something like. hello saw taxonomy faceting slides http//replaced.url question many taxonomies document apply dont know many taxonomies cant define field schema taxonomy field taxonomy use feature need know handle context document apply taxonomies cant define field taxonomy schema dinamyc solr handle. chosen use improper field names fl parameter need reference using field function basic concept solr don't ban improper field names don't work contexts. need two clauses something like could work best. hi may silly question sure contraindication run solrcloud java thanks patience. i've gotten far better answers already use spanqueryparser maintain published maven central carry nlp would allow literal headache headache shows within words http//replaced.url http//replaced.url. store solrinputdocument filesystem sent solr server via solrj client using quartz job periodically query table contains listing solrinputdocuments stored java.io.file need thanks time. question trying prevent big bad world stuff don't supposed solr going prevent big bad world posting delete query restrict hitting admin console looking schema.xml question big bad world internet large employees/colleagues organization it's internet large i'd totally decouple solr b/c sure thing internet access get /select restrictions could done many places it's clear coupling solr place big bad world within organization basic protections around don't see saying reasonable perhaps another option consider query component rather creating sublcass request handler query component promotes re-use flexibility could make necessary parameter changes prepare method make sure safe parameter component comes query component list components handler fine. currently calculate twice documents returned significant rows set. jetty running port running ports defined instances sequence apache0 reverse proxy front. don't indicated problem symptom actually think problem comma operator solr query parsers comma another character may may included discarded depending specific field analyzer example white space analyzer keep commas standard analyzer word delimiter filter discard string punctuation would preserved including commas spaces spaces would need escaped let us know symptom though first mean filter query looks perfectly reasonable abstract. nevermind got details thanks view message context http//replaced.url sent solr user mailing list archive nabble.com. may dynamic field stored ignore specific fields would otherwise matching dynamic field mask useful trying get metadata content something based specific field names matching dynamic ones regards linkedin http//replaced.url time quality nature keeps events happening lately don't seem working anonymous via gtd book. don't delete 0s ,0s etc field value keep text format i'll apply slop search needed search done. hello new solr trying understand sort functionality working thanks advance help following questions taken default download started solr posted mem.xml updated mem.xml copying items changing price fields xml file shown sort price seem work simply shows documents order inserted expecting results matched term sorted price despite it'score wrong it'score overwrite sort parameter get list match it'sdram field sorted price http//replaced.url sdram"""
"""94fb5e656357db4b2bbc7f9ce3f56241293b797ffc31e5c3c0773e5ac3e33c07""","""trying use archiva proxy maven central instance configured following configured local maven installation use repository see lot checksum validation failed issues warning could validate integrity download maybe use see archiva secure tls go directly maven central issues tried different combinations like maven central group using maven central group makes insane slow still validation errors manually download respective sha0 hash file see expected hash see maven suddenly think value start anyone ever seen anyone help kind regards. hi way group values like shopping.yahoo.com shopper.cnet.com instance documents like i'd like result grouping product value range product something like like current facet information grouped item entire result idea thanks. performance difference using fields defined schema vs dynamic fields. case getting results stop word issue. top shawn rightly said two things try benchmark best bet solution without shingles know better story numbers tell go shingles approach consider removing duplicates. chromosome gene object types really boils able give number e.g return documents regions containing number i'd document list like look build features using spatial believe david covered usecase talk san diego get error able find abstractsubtypefieldtype first bit trace hints wrong provide source code fuller stack trace config settings etc try unpack solr.war stick jar web-inf/lib repack however get noclassdeffounderror plugin fuller stack trace might help key question order try two approaches exactly fieldtype declaration look like tried repacking first maybe exploded still polluted old jar repacked multiple copies plugin initial noclassdeffounderror could try starting competley clean using stock sample configs make sure get errors try declaring custom fieldtype using hte fully qualified w/o even telling solr jar ensure get noclassdeffounderror custom get error abstractsubtypefieldtype still directive load jar load still wouldn't work provide us details container solr version full stack trace details configuring declared filesystem looks like solrhome etc. thanks sorry wouldn't really solr-related monitor wouldn't rely output free command think could still achieve significant improvements going performance tuning advice wiki. see http//replaced.url basically list special characters text file types attribute map alpha. hi trying implement auto suggest functionality currently looking terms component solr example form query like searches field strings starting results could possible get info results component like return data fields example along results field corresponding data telephone field maybe simply execute normal wildcard search thanks. thanks exactly every incoming query via entry custom requesthandler thus question reference original query text requesthandler xml believe i'm going use simpleparams syntax yes ideally would simple requesthandler would recognize q magic variable holds current search text came q url i'm guessing wouldn't access query requesthandler without inside brackets simpleparams like however pointed alternatives believe i've found easier way least case think ensures standard search catchall field text happen boosting additional search occur boost listed thanks caveat boosting function i'll set expectations users wouldn't limit search docs figo could guarantee show boost seems best compromise unless parse search results code i'd rather avoid whenever possible. displayname displayphone even schema print solrdocument object directly see results. able read indexes produced way back including 0.x sometimes experimental formats excepted case fine since you're upgrading always though i'd recommend copying indexes someplace paranoid upgrading best. different solution need i'm measuring response times different collections measuring online/batch queries apart using new relic i've added filter analyses request makes info available new relic request argument built new relic solr plug wouldn't provide much. thank. continuing fight keep solr setup functioning result made significant changes schema reduce amount data write setup new cluster data initially ran import replicas achieved quite impressive results peak new documents minute shard loses outages due garbage collection issue see production load index stood documents 00gb shard highest insertion rate would say querying suffered concern right added replica shard indexing time doubled surprising good start problem continue write leaders issue replicas continually going recovery leaders show ioexception occured talking server replica busy garbage collecting wouldn't coincide full gc collection times low replica appears accepting adds milliseconds appears log org.apache.solr.handler.admin.coreadminhandler requested recover reduced load documents minute appear stay couple minutes would like confident could handle peak times initially getting connection reset errors leaders changed jetty connector nio message received upped header request response ideas using replicas proposed colleague thanks much advance. hello shawn thanks reply look asap know dev environments persistent flag set true i'll check others production see someone get copy logs production environment see detail contained within thanks. ahhhh it's lightbulb last paragraph cleared confusion carrying responses incidentally likely reason confusion zk question could wouldn't make sense thinking zookeeper separate means handling things solrcloud two entirely different approaches scaling much helpful see balance assumption thanks shawn-. detailed perhaps alphabetical hierarchical table contents ether wikis sole site sent mail android. demonstration feature would good addition example/multicore directory. hello index fields dynamically wouldn't suit need wouldn't know fields advance fields must set dynamically need strong typage think solution handle programmatically best way custom handler api use. using example stack trace looks like error finding solr.home index directory configured example schema.xml works try adding little bit schema time. yup definitely need address make next release accumulo. hi thanks interest gave wiki permission. you're right patch updated catch exact exception thanks comments. hi thanks review updated following things latest patch check server starts successfully sqoop0 server started don't displayed remove file stopping server bug updated good suggestion get server status check file exist exist validate add checks environment variables hadoop lib related set start server wait seconds check result exceptions sqoop0 server started don't displayed. reworking break renderer source file might revisit elements embedded html elements renderering i'll let pass text. build stop unsatisfied dependencies currently functionality behave well maven jar override feature fixing first step toward stopping build continue failed dependencies attempt download still failed dependencies tried satisfy dependencies problem might problem use remote repository create message user stating dependencies unsatisfied. hi devs i'm getting following exception starting tomcat server airavata rc0 deployed idea resolve configuration add credential-store/client.xml able start tomcat without issues first time error comes try start tomcat server second time regards. think data blocks written dfs created index blocks think buffered written groups rfile operations scan index fast read lots index blocks sprinkled file groups contiguous index blocks read quickly sent phone please excuse typos brevity. eric looked locality running continuous ingest found tablets local data matches expectations default balancer try migrate child split sibling tserver concept may complex implement sigh cool. hi sorry missed respond earlier please see good comparison need make sure relevant jsdl semantics covered debated add field early compared saga jsdl rsl api it's like moab web services could conclusively narrow three parameters include problem including three could contradict making validation tricky think need select two three three confuse usage may kenneth opinion current app catalog design interfaces deployment decoupled parallel environment described application deployment question user specify particular parallel environment currently implicitly done picking right deployment could re-think allow users/gateway select deployment based parallelism reverses current approach timely actively revisiting data models keep mind get back http//replaced.url. hi network plugin implementation question i'd like extend bigswitch networking plugin firewall service plugin element implement firewallserviceprovider interface well included 'local notation rebuilding management server still see plugin showing firewall provider drop menu adding network offering network guru help point might missed thanks. tried without files trying import backup new dont know revision main site gui theres http error smthing information try command line admin.sh -i last thing says value org.simpleframework.xml.element name= data=true required=true org.apache.openmeetings.persistence.beans.domain.organisation.name seem like backup right first place tell tries find doesnt thanks advance"""
"""f2a3b32a84d8e62507db6d4ca3c640fc3d6c267dfd5a9eb829266482f859f9fd""","""hi still problem projectbuilder followed api change projectbuildingrequest see following setup gist problematic code works version beta-0 gets npe version beta-0 later hints welcome better regards kristian. plugin use maven0 api maven-beta-0 gives maven-rc0 gives maybe used projectbuilder maybe part public api maybe writing plugin regards kristian. trying understand various options worddelimiterfilterfactory tried setting options seems prevent number words output particular wouldn't 00dxl wouldn't get output wods containing hypens correct behavior solr analyzer output schema. hi you're trying push security related info index control users search certain fields you're wondering best way accomplish records indexed searched certain fields marked private field marked private querying users see/search whereas super users it's solutions you're considering index separate boolean value new _internal field indicate corresponding field value marked private include filter query searching user super user eg. consider record contain fields field000 field0 field0 marked private field0 record field0 marked private record b field0 index records it's i'd index searching user super user query let it's say security needs look like this- field0_internal:0 field0 security field0 security manipulating query way seems painful error prone we're wondering solr provides anything box would help determine fields query depending visibility using example it's indexed records would look like searching user super user query needs regular fields whereas searching user super user query needs regular internal fields issue solution since number docs include internal fields going much fewer you're wondering relevancy would messed you're querying regular internal fields thanks. particular reason would limit documents facet calculation mean whole point facet numbers let users know it's must rationale mind. personally i'd stick solr it's built-in dynamic field definitions keep things smooth future developers ease matching i'll see list via support channels use field aliasing know dynamic field fl=price price_f sort thing client deal friendlier names wouldn't think reasons best practices dynamic field naming conventions personally i'd use _/underscore separator though even legal characters support it's java identifier i'd steer clear http//replaced.url. kinda late party interesting thread i'm wondering anyone using solrcloud hdfs large scales really like capability since data inside hadoop run solr shards nodes need manage metal although cluster actually nodes run typical query takes seconds run faceting clustering minute range depends queried little seconds index contains fields looking switching different method indexing data involve much larger number fields little stored index index help improve performance i've used shardsplit success server split needs triple amount direct memory using hdfs node needs three x amount running three shards lead it'swap you're careful large indexes split long time run much longer rest timeout monitored checking zookeeper it's clusterstate.json. first starting play dataimporthandler dih cool rather simple case indexing rss feed contains articles articles entry database contains url article rating config appended db looks like rss feed comes blog heh it's convenient control question let it's say initial set ratings feed full import articles feed everything peachy far get new rating existing article i've already indexed thus child entity named delta however run delta-import wouldn't pick changes since believe parent wouldn't changed either something wrong seems like akin parentdeltaquery problem course parent query since parent table db sense least see relevant logs case handled suggestions alternatives help would appreciated thanks. hmm i've seen bug like wouldn't think would tickled replicating config files def looks related though i'll try around next time happens look slave files index slave plain 'index timestamp part timestamp.index. hi lot posts talk hardening /admin handler user credentials etc hardened considering fact could allow secure external access admin interface allow proper cluster work setting security admin/cores option thanks. errors persist complete index rebuild wouldn't done extensive checks far index seems work correctly need concerned thanks. hi noticed use removeduplicatestokenfilter query time consider term positions really anything e.g query 'term term term far see term positions make difference simple non-phrase search built-in way deal know write filter feel like would something quite basic query wouldn't think it's even anything weird normal users consider e.g searching music verified least according anecdotal evicende search really slows repeat term enough. thanks yes makes sense i'll experimenting along lines hitcollector solr standalone programs well yield meaningful resolution would benefit community it's interest i'll concept patch mentioned. could create list field might well create add fields schema.xml original point able refer field username string define explicitly solr reread schema file post add action starup solr reads schema.xml file indexschema object builds schema.xml used pervasively document adds searches understand use field general wonder adding suffix dynamic fields posing think user programmer it's intuitive think integer therefore enter searching think it's definitely tradeoff dynamic fields make easy add arbitrary fields information infered naming convention use names cleaner names create explict fields advance. defaulting behavior since forever changing behavior going happen making fit new version correct change behavior every application specified default behavior it's a-priori reason expect words equal fewer docs easily argue words return docs expect depends mental model providing default request handlers allows implement whatever model application chooses best replaced email.addr.es wrote. would use bq parameter boost question_source==0 documents first similar. look http//replaced.url it's pretty decent explanation memory mapped files wouldn't believe default configuration solr use mmapdirectory even understanding entire file wouldn't forcibly cached solr it's filesystem cache control it's actually ram eviction process depend thanks. pierce commented thrift-0. general comment speaking viewpoint hbase committer phoenix never deploy not-released rc code production setting upgrade path official-version-x official-version-y version-0/0/0 version-x-rc official-version-y james mentioned things may change really stable official versions although rc probably safe anyway i'll likely sorry bit rant unreleased software rcs meant production author lhofhansl general comment speaking viewpoint hbase committer phoenix never deploy not-released rc code production setting upgrade path official-version-x official-version-y version-0/0/0 version-x-rc official-version-y james mentioned things may change really stable official versions although rc probably safe anyway i'll likely sorry bit rant unreleased software rcs meant production. kafka-0 improve simpleconsumershell max messages config option kafka-0 controlled shutdown don't seem work broker cluster kafka-0 kafka broker give better error message running zookeeper kafka-0 error messages logged failing-to-send messages producer kafka-0 using chroot path create chroot startup don't exist kafka-0 seems show less performance kafka-0 subsequent calls consumerconnector.createmessagestreams cause consumer offset incorrect may edit subscription. using propertyshadowbuilder build service property null immediate exception needed rather nullpointerexception. new replication command needed force backup committed index data email describing problem possible solution agree think options could useful perhaps 'forcebackup well documentation would rest added info wiki yet http//replaced.url. don't reliably kill oozie jobs mostly fails occasionally succeeds kill fails oozie server restart usually fixes issue job killed shutdown/startup sequence hangs forever occasionally exits command error cmd line several errors oozie log find following errors logs tain object lock null caused org.apache.openjpa.persistence.optimisticlockexception unable obtain object lock null caused java.sql.sqltransactionrollbackexception lock could obtained within time requested caused java.sql.sqlexception lock could obtained within time requested caused error 00xl0 lock could obtained within time requested. long time still thank work"""
"""b25f329d7482d6ec4cbac4b7b82804b156f42ac3601340af82dce72c0b989f25""","""omri need indicate solr at_location field accept multiple values add field declaration see reference information options. makes next i.e still answered question next previous really means already know next page another query get next record. hi henri make sure container running solr set utf-0 example tomcat server.xml file connector definitions include join conversation like us facebook follow us twitter. oops sorry hijacking thread put real subject place. need include unique field fl parameter needed anyways match highlight fragments result docs highlighting working join conversation may even get ipad nook like us facebook follow us twitter. hi anyone ever set things figured good way access solr filter chain java code specifically would like tokenize data search strings possibly even facet values way solr filter chains additional pre-search processing e.g tokenizing way produce better fuzzy search query post-search processing find matches within facet values thanks advance. well sort depends mean previous next record sequencing built concept solr lucene indexes sequential isi.e use case data available support use case. hi mark used dih shall need leave comments set others done another question initial index create delta run commit run optimize without optimize deleted records still show query results. actual query look hl.snippets parameter join conversation may even get ipad nook like us facebook follow us twitter. could add standard field shard populate distinct value shard facet field look facet counts value corresponds shard hey-presto done. hi michael well stock answer it depends example would able search filename without searching file contents would always search together copy file parsed file content pdf single search field set default search field kind processing normalizing data case insensitive accent insensitive word contains camel case e.g theveryidea split case changes watch things like ipad word contains numbers left together separated stemming searching stemming would find stem stemmed sort thing always english languages involved text processing indexing vs searching able find hits based first characters term ngrams able highlight text segments search terms found probably read various tokenizers filters available prototyping see looks basically one fits part power solr lucene configurability achieve results business case calls part drawback solr lucene especially new folks configurability achieve results business case calls anyone got anything else suggest michael. hi nizan realize replying thread email client would get back isinfo thread since replication replicate top-level solr.xml file defines available cores dynamic cores requirement custom code wasted. hi sorry duplication seems like sent yesterday never made troubles solr spellcheck response running version overview search something really ugly like get back response suggestions list rck suggestions list two words book fine spelled correctly i.e got hits word suggestions ugly thing though hits problem handling result tell difference suggestions correctly spelled term suggestions something odd like happening searches obviously garbage i.e words real words show index suggestions illustrate point setup running multiple shards may part issue example book might found shards another think anything schema since really search suggestions returned us bits pieces would really like see response coming back indication word found suggestions hacked around code little bit wondering anyone across approaches taken created new classes extend indexbasedspellchecker spellcheckcomponent follows package imports excluded sort brevity methods taken overridden classes changes noted sd comments modification allows correctly spelled words returned suggestion modification working tandem sirsidynixspellcheckcomponent allows words suggestions returned spell check component even sharded search changes marked sd comments modification designed may work tandem sirsidynixindexbasedspellchecker return mispelled words suggestions flipped false suggestions index true thesuggestions null //sd removed thesuggestions.size shardrequest allow misspelled words suggestions thesuggestions.size always true hence removal continue //if shardrequest word mispelled add list mispelled words else extendedresults suggestions.size word misspelled added suggestions freqinfo isxml getting back search applying modified code thanks. looks like specified value pdfy reflected results query query searching vpn hence matches query yield. using unique solr index sounds like may value solr index unique bears resemblance unique derived data another way put makes two records solr index the unique istwo entries solr index isrelated original data unique immutable i.e update row database unique derived row would update otherwise nothing solr recognize duplicate entry delete insert instead insert. oops sorry missed well multivalued setting explicitly allow multiple values actual use case i.e multiple values field multivalued problem trying solve. without speaking directly indexing searching specific fields certainly possible retrieve xml file solr db allow binary field associated index document store gzipped xml file binary field retrieve certain conditions get original document information found solr handle much faster db regularly index large portion documents xml files prone frequent changes keep blob solr index make sure retrieve field really xml files relatively static i.e change rarely new ones still might make sense use real db store keep primary key db row solr index join conversation like us facebook follow us twitter. reading wrote originally wrote think misunderstanding based architecture code code server level solrj indexing calls meant server solr instance mean client thinking without thinking server sorry hopefully someone else chime specific view message context http//replaced.url sent solr user mailing list archive nabble.com. suppose something silly like fact indexing chain includes words= stopwords.txt query chain register february get time viewing three course circulation basics self-paced training suite. hi downloaded file unzipped see inside file tried unzipping errors look inside file see java code example ended unzipped .java extension example path apache-solr-0.0\lucene\backwards\src\test\org\apache\lucene\analysis\tokenattributes see two files ideas specific tool using expand windows xp thanks join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. use replication call inexperience really early working fully understanding solr best way approach various issues mention prototype non-production code covered though look replication feature thanks. possibility consider really need documents specifically empty non-defined values oxymoron control values send indexing could set special value means no value done similar vein using something like empty given field meaning original document actually value field i.e something unlikely real value easily select documents querying field empty instead negated form select however considered things like index relatively rare us value gut feel impacting indexes much size-wise performance-wise. thing think post-process snippets i.e pull highlighting tags strings look match result description field looking match find replace description original highlight text i.e highlight tags still place join conversation may even get ipad nook like us facebook follow us twitter"""
"""f9624337619765efae4f99a7c5ab852f2712306cb84c17cf189f4aa7afb8bad9""","""well could magnitude notation approach depends complex strings based examples would work identify series integers string assumes lengths series insert number integers string integer series sorting would sort string00 string00 string000 use original strings displays. hi relatively special case parent child relationship trying model currently using solr lucene example parent documents represent information e.g bibliographic information parent document contain children child representing physical copy book information e.g think library multiple branches child documents represent books format available given branch think special set information need solr lucene make use facet based need facet counts child facets example harry potter last crusade j.k. rowlings upcoming block novel following information etc etc bit simplified actually fields involved child document five fields used individually easy combination much harder refine result set relatively straightforward approach location format ordinary everyday facet fields certainly get results although ignore facet counts search book without facets applied get back facets like narrow things work though logically hole trying fill example suppose user chooses narrow location branch format dvd still get hit back child record values user looking dvd isbranch library dvd main completely controlling indexing searching side code i.e formulate document content indexed parse results presenting users approach thinking brute force method accomplishing using facets using facet.prefix parameter query could generate facets like narrowing single facet e.g location branch would usual facet search something like would request back facets like parse values returned location-format_facet retrieve follows branch- prefix would facet values format facet presented users book remains value fields pretty straightforward could somewhat simplified two facet fields instead four keeping paired facets using singleton facets retrieving paired fields limiting taking place parsing pairs location format facets limiting element use facet.prefix limiting choose facets look concatenated value however gets complex ramp fields generally requires individual facet fields number underlying fields i.e two fields two facet fields needed solr/lucene index support three fields could facets required fields would required facets getting bit much hmmm little scribbling ok fair bit scribbling actually reduce facet fields cover fields maybe bad interesting actually coded anything yet paper-napkin level exercise thing done perused various archived threads upcoming functionality regarding parent child hierarchical document strategies found anything would help much least directly saw jira lucene-0 nested document query support looks slides overview structurally would indicates query parser support place yet i.e solr query able relate child level queries either within base query clause question finally logical problem seem resolvable approach brute force outlined willing dive solr lucene code would like indication people think would good possible approach get level e.g way providing indexer tuple found combination values something searching facet queries thanks. dumb question time using bit java bit java. hi working upgrading troubles unit test code custom filters wrote tests extend abstractsolrtestcase reading thread test-harness elements present distributables checked branch code built ant generate-maven-artifacts found lucene-test-framework-0-xxx.jar however contain lucene level framework elements none solr solr test framework actually get built embedded solr jars somewhere way build jar contains solr portion test harnesses thanks. hi went apparently normal build install using instructions http//replaced.url get finally trying http//replaced.url get plenty searching looking information couch.ini anything useful mochiweb looked couchdb wiki troubleshooting forth near tell complaint made something somewhere host request header undefined sure know. nice thought trying suggestions inspired jan brad commenting ipv0 hosts /etc/hosts monkeying hostname along way near tell solved problem tests test suite passed sake reference actual build/install complaints debian make ebin directory lib/common_test may manually created something along lines anyhow problem actual location erl_rx_driver.so start playing earnest couch 0.0a000000 well whatever latest version etch let people know coming along. hello already code builds files yeah already document collection change values fields documents indexing possibility think instead modifying documents sending solr could write updateprocessor tha runs direclty solr gets access documents solr already parsed xml even documents someplace else like dih csv file make changes. contains information extracted document-file question part api document file path returns java object gives us way modify inbetween sending object indexing think gave answer api instead go external java xml apis completion task sorry description really making things complicated thanks. indy artefact use jdk0 sadly use invokedynamic default gpars wrt groovy/jdk0 situation sadly currently jdk0 version gpars mess jdk0 version works suspect people days work could fix almost hackathon happen groovy grails. 'it's fair wouldn't ready yet earliest release. hi steve indexed stored among field default properties listed specifiable -s. hi guys guys commit permission help commit thanks much. think increase acceptable yes think i'll try although case like groups documents smallish number planning variety different index sizes aiming sweet spot around docs. seems like able reproduce issue try investigate/fix. unfortunately wouldn't find commits. looked parameter. hmmm maybe need define mean server mean client view message context http//replaced.url sent solr user mailing list archive nabble.com. hi sorry spam testing posts actually seen sent queries past couple weeks single response anyways two would respond would appreciate let know ignored vs unseen thanks join conversation may even get ipad nook cid replaced email.addr.es like us facebook cid replaced email.addr.es follow us twitter. second query clause instead otherwise mixing apples themes_raw oranges themes. ok figured solr really surprised prototype benchmarks used different instance tomcat using production load tests prototype tomcat instance maxthreads value set using default value production tomcat environment maxthreads value running threads getting connection refused exceptions thrown ramped solr hits past certain level thanks considering yonik others waiting see reply made others said listserv great. guarantee best algorithm isuse final static helper methods set characters strings solr treats special meaning query corresponding escaped versions note actual operators show escape characters wherever occur escapes special characters search terms get confused. hmmm maybe understanding getting jonathan say there good way solr run query across multiple solr indexes shards parameter allows searching across multiple cores instance shards across multiple instances certainly implications like relevance consistent across cores shards works pretty well us thanks"""
"""3d7a0f2243e5c6521e64c397be8cb6dc90793503a65a8e13b1c9ba6abcfa57a9""","""trick learned marketing look say product reverse adjectives ask would competition say things product answer drop terms endorsement product definition vacuous empty stick facts whit cleaner clear fluff might include metrics project health releases committers pmc additions etc info board would really find useful fluff-free shall weigh sure. aapche oodt use soley test ensure imap handling works. oozie-0 oozie cleanup failed actions fills namespace quota oozie-0 applied launcher job oozie-0 fork/join workflow fails oozie.action.yarn.tag must null oozie-0 oozie restart required oozie metrics graphing tool broken oozie-0 spark-opts value workflow.xml parsed properly oozie-0 minioozie work outside oozie oozie-0 introducing new counter instrumentation log distinguish reasons launcher failure oozie-0 oozie mask passwords logs logging command arguments oozie-0 ssh action succeed exists command fail oozie-0 rerun failed/killed/timedout coordinator actions rather specifying action numbers oozie-0 specifying coordinator datasets logical ways oozie-0 queuedump command display queue information server oozie-0 loginfo uses action instead oozie-0 workflow get failed state kill control node resolve variable message oozie-0 instrumentation configuration rest api web ui include oozie servers oozie-0 cache list available timezones admin oozie-0 workflowgenerator package tar.gz file though seems creating intention oozie-0 docs explicit multiple sub-workflow definitions possible may edit subscription. make possible easy use content delivery network. didn't change i'm afraid forgot mention sorry ndoc finally reached point wouldn't work codebase anymore nullreferenceexception i've created docs sandcastle wouldn't got option creating static html files far tell please help us fix uses official approach still works. hi matthew i'm glad getting ids deleting separate query need it's dozens thousands ids delete it's strategy delete. something open-source like http//replaced.url instead. shown query ment please excuse tested lot little bit confused right query course select/ q=titleprocessed life start=0 rows=0 indent=on view message context http//replaced.url sent solr user mailing list archive nabble.com. cool. hi wouldn't intentional course let see based traces. thanks guys would looking around tb total index million documents period days purge indexes.i estimated slightly higher side things it's feel would thanks. hello hope running jboss run solr simpler containers e.g. jetty oom things look better replicate less often e.g every minutes instead every seconds all/some -x__ jvm params actually help. working write tests feature get done soon send review thanks. ah yes important lucene mlt see term vector stored still able perform querying much much much less efficient way lucene analyze document variable default_max_num_tokens_parsed used limit number tokens parsed don't go details since don't really dug code p field don't stored either rather difficult re-analyze general note really understand mlt works look wiki read thorough blog post http//replaced.url regards. thanks informed advices thanks trey detailed point view clear search multiple fields grow slower search actual search model problematic search catchall field need know fields match highlighting fields indexed stored improve performance get rid highlighting use solr explain output get explain output fields need search fields test removing highlighting adding fields search improve performances best regards. hi saying seems logical currently case collapseddocs functionality functionality build computing aggregated statistics mind really documents collected order appear search result collapsetype adjacent saved order appear really need collapse group search result order collapsed need tweak code change collapseddocumentcollapsecollector document ids stored inside openbitset collapse group change arraylist example way order documents collapsed preserved think downside change increase memory usage openbitset memory wise efficient arraylist integers think real problem collapse groups become large hope answer question. hi everyone need lucene jars path seems solr-solrj solr-core jars enough http//replaced.url asking lucene jars could know jars required run thanks advance. thanks reply response ok sure internally educated don't store entire documents index going index way small created using nutch store entire documents pretty sure part. hello everyone newbie question find documents indexed across shards thanks much. empty path message becayse nutch unable find url url location provide kindly ensure url. hi memory problems oom solr suppose something fieldcache entries count fieldcache grows grows rebuilt commit commit every seconds memory consumption solr increased within day tried solve problem reducing cache sizes filtercache documentcache queryresultcache delayed oom exception solve problem memory consumption increases continuously possible reset fieldcache explicitly. hello question related local solr certain locations latitude longitude spatial search work query try make gives results however make query radius=0 gives results part containing starttier endtier need fix problem. hello i've trying integrate ner solr search get really good facets i've already managed plug search handler code get feel works i'm trying plug update request processor pull facets i've gotten kind stuck implementing i've bookmarked specific problem area bolded messages see 're using field content determine language though could specify many fields wished next need add chain update request handler like right i've used processor chains trim remove blank fields i'm quite sure using totally different request handler go 're good go indexing documents via curl dataimporthandlers etc query see results say indexing happened use query get results question /update handler used it's used index it's used search used /select search handler heck /update processor implement probably generic query use get results documents person organization field correctly populated index data question use information better/easier information finding users answer course resounding yes faceting field. brackets range operators parser need escape enclose quotes expecting \r. hi guys need help search handler need override solr it's scoring chose implement rankquery api gettopdocscollector gets called instantiates topdocscollector instance every dicid gets //initialized constrctor scorer.start //the scorer it's api needs call start every query close query get specific field doc using docvalues calculate score using scorer add docid score scoredoc object priorityqueue problem cant find place call scorer.close need executed query ends calculated score docid saw deligatingcollector finish method called collector done extend topdocscollector. really depends schema change addition/deletion usually implies avoid re-indexing old documents remain outdated change change data structures involved enabling docvalues norms ect ect without full re-index no-go lot discussions past allow solr manage schema changes fly background jobs transparent user/administrator nothing concrete yet know. discussion cites paper via url unfortunately go url get tracked paper reference may require subscription sorry us http//replaced.url journal american society information science technology. don't specific date far i'd like say year i've working svn head build solr noticed bug get committed fixed quickly found think need development features 're probably safe use svn head remember dev always test new builds actually using =p thanks zappos.com. hi appears happen trunk appears add command request parameters get sent nodes comment like add commit things work expected otherwise params like stream.url gets sent replicant nodes causes failure file missing worse repeatedly importing file exists replicant might right thing sent. it's bit tight index cached best bet go 0gb+ figure way retrieve many stored fields. hi done anything yet radar sometime month speaking unencumbered experience substantial understanding shingles would great select shingles something like terms prefix combines single token could set max shingle find way use terms component shingled i'm interested find please post back find something outside mailing list thanks see something like solr enterprise search server book well worth money believe ebook version. company search uses stopwords quezary time stopwords list entries like ltd. search companies like hr club get results similarly search india hr giving results get results query following companies would still maintain list stopwords since please guide need change strategy thanks. using following defines query hightlight body elements html documents set hl.fl highlightcomponent body result bigram cjktokenizer highlighted regards. attached patch well fix could please confirm. it's interesting approach i'd consider question essentially get indexschema solrj client without needing parse xml file hopefully i'm afraid don't work index empty luke don't return fields fields written method returns information i'd like know know field"""
"""4b889711d92465ada1f34e529edb774180412496e385322b9b33f40ec907edbd""","""hi potential likely extension couchdb contract working need win00 work done would rather even time similar couchdbx rather specialized weeks effort based experience similar thing osx preliminary enquiry see anyone suitably capable available interested order get client approval would need completed march rfn would fine currently deploy couchdb platform-specific ruby gem binary libraries required erlang icu spidermonkey etc binaries nginx packaged individual gems requirements managed using gem dependencies package ruby knows extend command line environment include binary resources path/env vars/libpath etc given command line environment starting couch case couch ruby allows configure multiple instances run/manage ruby binaries package location-independent done munging rakefile builds gem means package application isolated machine environment relative given baseline done osx cool although flaws always terminating process concerned getting running system deployed windows need done windows nginx could either must configurable managed via ruby wrapper must native code compilation required gem install e.g cygwin required would conflict existing cygwin installation use private gem repository reason needs work windows xp vista corresponding server versions simple osx wrapper application provides server coupled merb-based app present purely html interface similar osx finder itunes part contract manages updating gems compaction using xmpp talk presence server etc none contract need gui portion installer done windows xp vista although must require additional downloads e.g .net must use shared ruby install gui manages/starts/stops/log admin app points embedded safari mozilla automatically selected port nothing else fails start app must raise explanatory dialog obviously functions handled admin app must fully this-is-just-a-windows-app e.g box help link pretty trivial clear tradeoffs embedding gecko windows app rather ie concerned tracking ie version different client installs maybe issue part really nothing couch would person parts single contract copyright would vest linkuistics company bsd license credit author linkuistics including admin although gui wrapper may required hence finished stage gauging interest/possibility open suggestions/ quotes/offers/enquiries done non-local subbing would need way verifying competence presuming list reaches couchdb-aware talent. netflix switched site search solr two weeks ago it's great walter could persuade add notes. ok whole dbq thing baffles heck may totally base would committing help least worth ths wouldn't dbq specifically said olddeletes map used dbi problem acording heap dumps looked suspect correct root cause ooms perhaps wouldn't using hard/soft commits effectively enough uncommitted data it's causing oom hard say w/o details confirmation exactly looking claim heap dump thanks replying using solr version even saw bounded 0k count looking heap dump amazed keep 0k entries yes see around 0m entries according heap dump around 00g memory occupied bytesref exactly looking say see 0m entries sure wouldn't confusing keys olddeletes instances bytesref jvm. in-reply-to starting new discussion mailing list please reply existing message instead start fresh email even change subject line email mail headers still track thread replied question thread gets less particularly difficult see http//replaced.url. hi francesco mostly covered difference additon virtual machine cloudservice would support availability sets therefore autoscaling article0 brief azure thinking cloud services http//replaced.url. thanks jack it's wouldn't multiple entities invocation two simultaneous invocations dih different entities thanks http//replaced.url. discussion irc regarding collecting data items performance regression start draw conclusions intention understand needs release i'll reply inline issues release run i'make check make distcheck assume see hang others yet everybody wouldn't comment bigcouch different interesting 0.x wouldn't hang i'make check 0.x r00b hangs sometimes different places i'm currently trying gather information question whether i'make check passing r00b release requirement vote considered happy go community decision emerges addition wouldn't question investigate happens address issue hence couchdb-0 insight would appreciated well assume mean tests wouldn't supposed work 0.x i'm happy backport changes master 0.x make work refrained wouldn't bring much change release branch i'm happy reconsider wouldn't think release vote good place discuss feature backports explaining away think warranted chrome broken attachment_ranges wouldn't know reported upstream robert wouldn't release blocker replicator_db test try running understand best situation hence move cli test suite master get test pass least wouldn't problem holds 0.x make absolutely clear report performance regression seriously i'm rather annoyed information ends dev understand irc it's shared understanding scenarios performance regressions shown asked three times posted mailing list i'm asking comprehensive report anything really found robert newson it's simple test irc ran test suspicion posted earlier mail tiny docs slower bigger docs faster nobody else bothered post see discussion observed expected would acceptable release far list concerned know people claimed things slower it's real hold release i'm happy hold release figured things asked help figuring need something work understand voluntary project people wouldn't infinite time spend least message you're collecting things report done would great start far hold horses might something going please let know request unreasonable whether overreacting sorry rant anyone looking performance regression please send list info comprehensive analysis awesome ran machine send us let it's collect data get situation solved need help it's three issues hand robert i'd release artefact understand needs happen make release includes assessing issues raises squaring release vote it's vague far dev concerned report performance regression need get behind it's non-dev discussion performance regression referenced influence dev decision need discussion it's information dev proceed make absolutely clear performance regression issue grateful people including robert newson robert dionne jason smith look it's need treat issue get info onto dev jria. record nightly builds test building automatically. hi sorry flood created filter jira shows blocking issues subscribe whatever means hope it's track matching issues using link. bob it's latest commits fixed replication issue i'd love hear things mentioned best. field solr.sortabledoublefield i'm actually migrating project solr process trying update schema solrconfig stages updating field thanks yonik. oh yes workarround sure needs added. see changes went wrong run stacktrace option get stack trace run info debug option get log output. see changes jfarrell thrift-0 thrift :framedtransport sometimes calls close undefined value fixes thrift :framedtransport module sometimes ends calling close method undefined value inside close. question sentence data model things stored retrieved came across o'reilly book data model chapter cassandra index subcolumns load super column memory columns loaded well mean exhaustive list column names values super column map keys contain two columns max wouldn't really performance concern correct becomes issue lots subcolumns i'm reading correctly i'm looking using super column good way cluster data say storing home addresses might use super column cared mostly accessing data logical area instance thanks help. could use vdu function fixes structure target db replicate old new db"""
"""5d25b0725f67b2ca1a4ec8d521523ff5cf9d08ae453584bfbd6acd705fb1a197""","""hi hi furkan ahmet thanks reply last email solr search proposal sent last sunday 0-mar-0 announce solr search simple html interface searching documents indexed apache solr tm actually developed last two months spare time small html interface solr far complete mature project features options might found useful users solr glad share code hosted http//replaced.url page quick overview link solr search found well link downloading thanks best regards ahmed replaced email.addr.es. hi ahmed egypt spent last two months developing custom web-interface solr using html called solr search actually basic idea inspired ajax solr solr search provides different approach terms usability available options users know might interesting information already better search interfaces send email hope sharing solr search community know exact steps share kindly please guide appropriate please find attached quick overview document solr search. still wip let know help please rebase/fix conflicts. nitin-maharana please rebase currently merge conflicts thanks. trillian-jenkins test job centos0 mgmt vmware-00u0 kicked run smoke tests. currently ui allow migration different versions changing ui change elegant purpose change user intentionally migration lower higher via api ui way around hypervisor allow point looks good ordering could defined. zhongneu syntax dynamic checkbox example text syntax value dynamic drop menu drop_down_label=default value|value 0|value |value. you're right usually users wouldn't make production using docker received many question running zeppelin production environments think pr gives hints users solve problems making production environments. thanks indeed it's great test confirm said i.e wouldn't need mark configkey annotation able use top level config. yeah i'll abort change it's implemented others. pr user need invoke print make output displayed notebook behavior natural consistent notebooks pr make pyspark interpreter zeppelin behave notebook main changes use single mode compile last statement evaluation result last statement printed stdout consistent notebooks like jupyter make sparkoutputstream extends logoutputstream see output inner process python/r helpful diagnosing pr tested tested manually following text pyspark paragraph get following output breaking changes older versions user wouldn't need call print explicitly needs documentation yes merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. tinkoff-dwh sorry problem. may concern breaking current ux change change many benefits comparing current embedded spark wrote pr description tae-jun mentioned comment http//replaced.url well thanks always kind big change brings downside well e.g breaking current ux wan write address major cases think would better share opinion get feedback merging quite easy cover already handled updating related docs pages definitely case harder handle user already expectation local mode works surely wouldn't read docs resolve i'll update 'bin/download-spark.sh print sth like wouldn't local-spark/ download embedded spark 'get-spark option user run './bin/zeppelin-daemon.sh start sentences removed future zeppelin users getting accustomed 'get-spark option hard handle user might assume spark works would suggest start applying change first step since zeppelin-provided official docker since bzz raised concern issue let answer make sure reason removed '-ppyspark '.travis 'pyspark profile existed it'spark-dependencies/pom.xml 'pyspark profile wouldn't anymore pr merged actually pyspark testcase astroshim added recently conflict change solved simply adding 'export spark_ver-bin-hadoop hadoop_ver '.travis.yml travis run running issues especially concerning removing it'spark-dependencies related build profiles. pull request adds support annotation disable emit code operator printto allowing custom streaming output format provided consuming application merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. thanks granthenke wouldn't heard anything madrob i'll look merging. sync-processes 'get-valid-new-worker-ids checks topology files launching workers it's issue corrupts topology code topology code sync-processes wouldn't launch worker next runs sync-supervisor try download topology code. could close pushing dummy commit yes agree sending pr weird communication form. isolated network network vm already created setupclass adding verification steps separate test case could run dvs setup port group verification code already tested dvs setup tested remaining steps advanced kvm setup ensure issues merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. looks great thank 0ambda valuable build improvements merging master discussion. tylertreat-wf pushed tag http//replaced.url mirror picks github. streamoperatortask need final merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. barthel think need rebase feature branch mng-0 upstream/master something like assuming git clone upstream configured point apache/maven origin configured point barthel/maven. looks good merging comments. shared 0/master testing coordination thread simon weller team run latest master hardware labs testing master preparation rc still getting 'addhost issue periodically showed lot bigger issues concerned production honest long running production client. thank contributing apache opennlp order streamline review contribution ask ensure following steps taken changes x jira ticket associated pr referenced x pr start opennlp-xxxx xxxx jira number trying resolve pay particular attention hyphen character x pr rebased latest commit within target branch typically master x ensured full suite tests executed via mvn clean install root opennlp folder written updated unit tests verify changes adding new dependencies code dependencies licensed way compatible inclusion asf http//replaced.url applicable updated license file including main license file opennlp folder applicable updated notice file including main notice file found opennlp folder x ensured format looks appropriate output rendered please ensure pr submitted check travis-ci build issues submit update pr soon possible merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. rdowner aledsage review merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. followup ticket kafka-0 improve windowsize calculation quotas i've made following changes calling rate.windowsize clientquotamanager return exact window use computing delay time changed window calculation subtly current calculation bug wherein used number elapsed seconds lastwindowseconds recent sample object however lastwindowseconds time sample created causes issue implies current window elapsed time always sample created incorrect demonstrated testcase added metricstest i've fixed calculation count elapsed time oldest sample set since gives us accurate value exact amount time elapsed merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message"""
"""574ca75cae0d6cee843ca843d83fba69638211430d175ff62ef30fd1c06dfc89""","""files uploaded users deleted save space server mobile sorry typos. debian. hello maxim obvious red0 maybe right tested upload jpg file avatar conference room upload .ppt office file extensions. hello alvaro works flawlessly server starts image/office doc upload nothing common. ok. time ago something similar testing another question upload files properly use refer upload office files know debian could please url server. have visited right url works right uploading files files uploaded users drop whiteboard. hello hermann using different start/stop http//replaced.url necessary tags. verifyerror subclass uses pageactivationcontext extends uses pageactivationcontext. added new getrootexpressionclass method ognlexpressioncompiler interface impl handle special corner case casting previously component classes. page moved typecoercer service. -curious see real tapestry application live wish fulfilled booking demo available 's based well-known seam demo updated make use tapestry including fancy ajax updates code available github curious see real tapestry application live wish fulfilled booking demo available.it 's based well-known seam demo updated make use tapestry including fancy ajax updates code available github -project website basics homepage project.apache.org project naming descriptions use proper apache forms describe product etc website navigation links links included link http//replaced.url included license security links missingtrademark attributions attribution asf marks included footers etc logos graphics include tm use consistent product site tm missingproject metadata doap file checked date 's still referencing tapestry need update -we i've series beta releases tapestry voted due problem voted release december 00th first release almost months -a live demo tapestry available http//replaced.url -we deployed new improved web site authored confluence exported static web site includes brand new improved addition i've much community work non-committers access confluence organizing rewriting documentation tapestry component-oriented web framework java.branding requirements implementation progress project website basics homepage project.apache.org project naming descriptions use proper apache forms describe product etc website navigation links links included link http//replaced.url included license security links missingtrademark attributions attribution asf marks included footers etc logos graphics include tm use consistent product site tm missingproject metadata doap file checked date 's still referencing tapestry need update that.we i've series beta releases tapestry voted due problem voted release december 00th first release almost months.a live demo tapestry available http//replaced.url deployed new improved web site authored confluence exported static web site includes brand new improved addition i've much community work non-committers access confluence organizing rewriting documentation binary files diff available modified websites/production/tapestry/content/getting-started.html getting started tapestry easy lots ways begin watch video browse source code working demo app create skeleton app using maven step tutorial.watch short videofor fast-paced introduction watch mark w. shead 's minute demo video shows set simple tapestry application complete form validation hibernate-based persistence ajax video provides preview development speed productivity experienced tapestry users enjoy.play working demo appyou play tapestry via live demonstration applications start look booking demo source code provided download play it.create first tapestry projectthe easiest way start new app use apache maven create initial project maven use archetype kind project template create bare-bones tapestry application you.once maven installed execute following command getting started tapestry easy lots ways begin watch video browse source code working demo app create skeleton app using maven step tutorial.watch short videofor fast-paced introduction watch mark w. shead 's minute demo video shows set simple tapestry application complete form validation hibernate-based persistence ajax video provides preview development speed productivity experienced tapestry users enjoy.play working demo appyou play tapestry via live demonstration applications start look booking demo source code provided download play it.create first tapestry projectthe easiest way start new app use apache maven create initial project maven use archetype kind project template create bare-bones tapestry application you.once maven installed execute following command maven prompt archetype create tapestry quickstart project exact version number e.g. asks group artifact version number use staging uri get archetype not-yet-released version tapestry.you see following transcript modified websites/production/tapestry/content/the-tapestry-jail.html -ccordenier uli deploy manage webapps ccordenier thiagohp uli log restart tomcat -only use i've got jail tapestry.zones.apache.org running tomcat deploy demo applications booking demo running there.the jail replaced vm tapestry-vm.apache.org booking app moved document needs updated reflect new server ccordenier uli deploy manage webapps ccordenier thiagohp uli log restart tomcat.restarting tomcatonly use -if need restart tomcat anything else fail -if jail lost java tomcat +if need restart tomcat anything else fail.reinstalling jailif jail lost java tomcat -if n't exist yet +if n't exist yet -some files need manual download placed /usr/ports/distfiles follow instructions screen -some files already live http//replaced.url downloaded time zone update utility n't downloaded oracle directly -afterwards +some files need manual download placed /usr/ports/distfiles follow instructions screen.some files already live http//replaced.url downloaded time zone update utility n't downloaded oracle directly.afterwards -follow instructions manually install unresolved dependencies might encounter -check env packagesite point tb.apache.org +follow instructions manually install unresolved dependencies might encounter.install tomcat pre-built package preferred check env packagesite point tb.apache.org -this really discouraged +from ports discouraged really discouraged -tomcat resides /usr/local/apache-tomcat-0/ may wish set users conf/tomcat-users.xml note tomcat onwards manager role split separate roles manager gui manager-gui default tomcat listens port listen though edit conf/server.xml change port -tomcat0_enable= yes +configure resides /usr/local/apache-tomcat-0/ may wish set users conf/tomcat-users.xml note tomcat onwards manager role split separate roles manager gui manager-gui default tomcat listens port listen though edit conf/server.xml change port.add +tomcat0_enable= yes -to /etc/rc.conf start tomcat system startup tell use diablo jvm -the sudoers file may lost entry allowing members tomcat-restart group restart tomcat add back using visudo +to /etc/rc.conf start tomcat system startup tell use diablo jvm.the sudoers file may lost entry allowing members tomcat-restart group restart tomcat add back using visudo. fixes tapestry-0 component n't rendering clientid tag rendering enabled cycle rewinding particular form nothing n't even render body get condition work field necessary tapestry.form.validation.validateform form. touch documentation simplify export project http//replaced.url commit http//replaced.url tree http//replaced.url diff http//replaced.url. removed auto-injection service ids string backward incompatible decorators"""
