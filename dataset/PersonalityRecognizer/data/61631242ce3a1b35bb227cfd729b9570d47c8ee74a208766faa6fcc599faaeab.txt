hi geert-jan yup http //replaced.url modifications patch could work thanks info. run similar issue mix webapps non-solr guys use log0j rotating daily appender since supported default jdk logger mismatch log files produced would looked mention wrapping log0j logmanager big deal bit annoyance search reusable implementation find anything looked good since use resin resin support describing approach taking short-term embedded jetty usage shall back situation ryan arguing use apache commons logging 've enough fun nutch commenting discussion. might look nutch handles issue nutch stopwords wants keep around generates combo terms like the- index query parser thing query phrase common terms wind searching across much smaller slice comes course expense larger index lot unique terms due combo terms big win example site http //replaced.url index source files without optimization searches could several seconds got 000ms lots breathing room. hi checking seems indexed fields specified sorting purposes query string least seeing date field correct miss info wiki someplace case would handy standardrequesthandler return error invalid request sorting non-indexed field requested thanks. hi regexreplaceprocessorfactory line means use match groups replacement string reasoning behind missing something groups used making hard write simple solution training exercise students need clean incorrectly formatted dates thanks. hi creating combo field searching straight-forward way boost contribution fields used create combined field would read past threads seem anything built solr simple hack copy field multiple times e.g schema field-to-boost effective boost 0x since highlighting display multi-value field seems work ok long course level boosting 0x 0x etc acceptable something continue work future issues approach run yet thanks. got version index appears open luke reports problem grins crafted matching schema tried use index solr solr-trunk either case get exception startup startup logging says trying something destined fail would expected schema/index miss-matches show later right index opened would seen various posts error due corrupt indexes buggy version java obscure lucene none seem apply situation thanks. thing bit previously using apis area solr call corecontainer.getcore increments open count balance call close call naming could better think common expectation calls get something change state maybe. hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor 's datadir gets used construction changing coredescriptor 's datadir effect since would go changing datadir core multi- thanks. hi way explicitly add record using solr add fail record already exists value unique field specified schema 's uniquekey field course pre-flight query see record unique exists multi-client environment solr multiple users reliable thanks. exact version resin still confirm uncommented lines. hi dave think pushing solr space probably going non-starter solr 's focus index management serving side things different multitude issues faced looking good free would try ibm 's uses lucene hood fact wonder could reverse index create solr schema. could use lengthfilterfactory restrict terms least character long believe patternreplacefilterfactory creates patternreplacefilter says. providing maven0 build would useful us currently jar in-house maven repo works clean would favor changing directory layout selfish reasons would favor changing build rely maven0 use maven internally krugle sometimes works well times royal pain butt option would nice handy personally would hate foist maven everybody else. looked briefly passing analyzer use morelikethis fields lot analyzer used given filters play performance really stunk use stored term vectors would nice yes thanks. hi list anybody know suggester component designed work shards asking documentation implies since suggester reuses much spellcheckcomponent infrastructure spellcheckcomponent documented supporting distributed setup make request get exception looking querycomponent.java:0 code see assuming docs variable null would happen response element solr response make direct request request handler core e.g http //hostname:0/solr/core0/select qt=suggest-core q=rad query works see element named response unlike regular query wondering configuration borked work fact suggester return response field means work shards thanks. hi got situation key result initial search request let 's say list values faceted top faceted field values need get top hit target request restricted value currently total requests requests following initial query made parallel still questions magic query handle solr as-is best solution create request handler case developing custom thanks. hi particularly uwe robert yes gist question specify use simpletextxxx e.g simpletextstoredfieldsformat solr currently possible thanks. grabbed latest greatest trunk make main.css .query-box height tall enough least mac 0/ff config character descenders get bumped fixed issue solr looks like contains anchor text missing constraints see .constraints css added main.css ianawd probably best way fix issue see css used intent set character gray seems silly open jira issues types things add noise list approach preferred thanks. hi marcus look new project called katta first code check-in happening weekend would wait monday look. clarify issue using actual user search traffic seo content expose example people commonly search java hint url static content page language part generating static pages based search traffic though might decide content favor see based popularity yes need automate content generation regular e.g weekly basis big challenges ran dealing badly behaved bots would hammer site wound putting content separate system would impact users main system generating regular report user agent ip address could block robots.txt ip necessary figuring structure static content look like spam many links page much depth constrains many pages reasonably expose project scores based code activity usage used rank content focus exposing early low depth good stuff could based popularity search logs anyway lot topic feel solr specific apologies reducing signal-to-noise ratio. thanks fast response beginning worry nobody read posts see http //replaced.url attached test code issue plus simple fix json case regards. hi part interesting work creating custom query parser writing unit tests exercised extendeddismaxqparser first created extendeddismaxqparserplugin used create qparser via query something like complexphrase query expecting complex phrase query parser get used 's happening local param treated regular text makes think conceptual model local params processing wrong 's higher level code pre-processing step first hoping 'd get disjunctionmaxqueries queries complexphrasequery would mean processing happen inside extendeddismaxqparser code pointers poke around thanks. hi upgrading solr noticed filtercache hit ratio dropped significantly previously enabled recording entries debugging looking seems edismax faceting creating entries sharded setup distributed search search string bogus text using edismax two fields get entry shard 's filter caches looks like expected similar situation happening faceted search even though fields single-value/untokenized strings using enum facet method shall get many many entries filtercache facet values look like item_ net result even big filtercache 0k hit ratio still thanks insights. must missing something 's schema exactly original example schema look index using latest luke would need rebuild luke current lucene code since format version tried using solr/admin/analysis see query returning get server error numerictokenstream support chartermattribute looks like happy analysis triefloatfield searches index using solr/admin results single value e.g ideas might going thanks. hi yonik thanks fast response ok curious made bit confusing api well-formed xml xml parser happy reserved xml characters need escaped using org.apache.commons.lang.stringescapeutils seems work pretty well great advice let give try thanks. hi quick note caveat using slightly older version solr without fragile sense relies current working directory resin home directory b path must ./webapps/ anyway worth solrsevlet.java 's init method change beginning see instance resin specifying explicit location configuration data note relative original source least version parameter solr.configdir lower-case build deploy solr resin called solr-notes example 's /webapps/solr-notes/web-inf/web.xml a. uncomment first three system-property lines noted comment work around bug resin b. add init-param section copy contents solr config directory /webapps/solr-notes/conf part edit specify location data directory repeat steps second different. answering question use corecontainer.reload core force assume got embeddedsolrserver running time everything happen correctly covers need find programmatically change settings using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks. time text. hi documentation wikipediatokenizer specifically wondering pieces source xml get mapped field names solr schema example seems going date field example schema got goes body way get example thanks. using solrj programmatic way force course assumes able programmatically change location datadir another issue thanks. would parse html extract text first use index data nutch tika projects examples using html. hi ryan dug client code natural inclination would go latter fits better would write test use test request object. mitch right looking recommendation engine understand question properly yes mahout work though taste recommendation engine supports pretty new owen robin anil mahout action book early release via manning lots assuming list recommendations given user based past behavior recommendation engine could use adjust search results waiting jump best handle. ran minor problem clicked facet tried search would get error think problem fqs velocity macro vm_global_library.vm missing else insert url macro fqs p foreach p velocitycount else without url becomes /solr/browsefq=xxxx instead /solr/ completely new world velocity templating got low confidence right way fix. hi martin lurker interesting thread would suggest talking solr committers experience merging lucene got many similarities discussing though solr mature happened seen externally ultimately win though without lot teething issues many seemed personality conflicts groups versus technical/admin/operational issues additional comment inline ps counter example fine separate project solr releases eventually came faster lucene release cycle improved slow-down everyone expecting certainly reduced continuous painful debates belonged solr vs. lucene. believe code support handling form data sent put request logic make sure either unspecified application/x-www-form-urlencoded read body byte array use request convert string request unspecified assume us-ascii typically specified e.g use curl tool post data sent header convert key/value pairs using urldecoder.deccode string utf-0 since key/value pairs url-encoded believe standard assume us-ascii utf-0 would work well us-ascii viewed sub-set utf-0. two cases think cases two characters variant forms unicode tried unify still exist gb tons wanted support phonetic pinyin zhuyin search might collapse syllables commonly confused course would storing phonetic forms words. hi especially yonik http //replaced.url page mentions duplicate field collapsing later allow duplicate see mention deduplication happens search time normally requires field stored indexed efficiency might need fieldcache wondering status support thoughts potential thanks. hi james general immediate updating index continuous stream new content fast search results work opposition searcher 's various caches getting continuously flushed avoid stale content easily kill performance issue interesting topics discussed lucene bof meeting apachecon alone wanting ways clear hard problem relax need immediate updates index accept level lag time receiving new content showing index would suggest splitting two processes backend system deals updates. code appears try maybe code date see /solr/conf used path code see created cwd/data data default specified work tried number different permutations maybe missed magic combination grepped source see pattern solr.solr.home used anywhere able control location config file using set location /data directory still gets created saw though seem use either see solr.datadir used code anywhere trying control command line seem work could uncomment edit tag file worked makes worried missing something wrong sources tell ways control location data directory used index a. change cwd sure inside resin b. edit tag file lets change directory ways control location config directory lets change directory thanks. got situation data directory needs live elsewhere besides inside solr home b moves different location updating indexes setting symlink /data great option best approach making work solrj low- level solution seems create solrcore instance specify data directory use update corecontainer wrong would like avoid mucking around low-level solrcore approaches thanks. hi robert -xx heapdumppath= something look versus gedankenexperiment. moving solr-dev solr-user quick impression given scope described page feels like boil ocean problem spent afternoon looking could use solr code getting much love somehow leveraging solr would seem like win three attributes solr interesting context complex query processing caching though critical things live noticed described issues automatic doc- server mapping calc stable hash quick exploration use hadoop rpc talk guts solr assumes query processing happens search server level versus master currently nutch way request summaries document via subsequent post-merge call master immediate problem ran notion solr running inside container currently penetrates deep bowels code even core level calls made extract query parameters url step going try clean manner would define side/solr core api layer would relatively easy least first cut hooking solr core nutch master via hadoop prc. hi michael build meant gui whereby user use special characters say quoting collection clauses programmatically build query without using query parser code wound write seemed like simple escaping quickly got complex convoluted e.g allow term get processed specially query parser ok case sounds like stuck escaping. hi ken given comments seemed describe using nrt opposite use case set solr use solr.mmapdirectoryfactory bother test whether nrt would better use case mostly sound like advantage focused things relating solr would love hear results someone testing batch indexing use case tested various xxxdirectoryfactory implementations please let know results testing. hi chris would thought queries would allow thing supported currently standard query would recommend re-posting lucene user list. general notice http //replaced.url uses solr search project data store index user-generated notes code finally went live last week openly post thanks solr community useful tool great. yes came exactly conclusion could try use ram jvm load index ramdirectory mmapdirectory wound slower configuration smaller jvm relied os-level cache make index accesses really fast trick heard /dev/null force index data. hi got fields contain embedded xml two questions relating appears though shall need xml-escape field data otherwise solr complains find start tag embedded tags finds tag field expected constraint xml-escaping data best way handle kind related question would easiest way ignore xml tag data indexing types xml-containing fields seems like could define new field e.g set associated tokenizer something new create though would un-escape data ick parsing skip tags thanks. hi uses solr generate terms mailing list text analysis extract good features things like classification similarity clustering last part cover using solr implement real-time similarity engine maybe recommendation engine well undoubtedly things unclear even incorrect please comment regards. ah yes would explain oddity saw order hits matching document boost since querying field omit norms really short. hi uses character separate elements e.g state|city solr 0.x worked fine query gets distributed across shards get solrexception severe org.apache.solr.common.solrexception use fieldcache field neither indexed doc values state problem appears fl= state|city parameter getting split functionqparser tries use state field actually exists ignored field since q=state|city ca| find entries california known issue way disable parsing field names field list thanks. actually tika htmlparser wraps tagsoup good option. hi ian public terabyte dataset project would good match need course means actually finish crawl finalize avro format use data free collections data around though none know target top-ranked pages. would great facing issue rolling code keep solr index sync mysql db access via hibernate thanks. hi otis sorry missed reply thanks trying find similar report wondering file jira issue might get attention. sounds like hitting max url length 0k common default http web server using run solr web servers know let bump limit via configuration settings. think ran would like run two copies solr inside resin since two sets searchable data distinct schemas/usage patterns specify location configuration directory least using either solr.configdir solr.solr.home subscribed dev list shall post question timing fix versus something quick needs fwiw quick fix pass directory location init thanks. response header and/or meta tags page see code used nutch could missing could wrong either/both issue determining right arbitrary web page easy way analysis advance know sure always x going simplify things soon possible means right processing page collation settings db change db interprets data change data. ok thanks confirming absolute easiest approach would support new init value codecfactory schemacodecfactory would use select different base codec use versus always using lucenecodec would switch everything different codec could extend schemacodecfactory support additional per-field settings stored fields format etc beyond currently available quick dirty hack specified different codecfactory factory hard-codes simpletextcodec works files simpletextxxx format segments.gen segments_xx files pluggable. hi using simpletextcodec past noticed something odd running solr enable posting format via something like resulting index expected text output noticed files standard binary format e.g .fdt field data based simpletextcodec.java assuming would get simpletextstoredfieldsformat stored data holds true files e.g http //replaced.url adding simple text format docvalues walk code figure hoping need change configuration setting thanks. right looking snapinstaller seemed though deletion index directory would deleting files indexreader thanks. url work please include full stack trace runtimeexception version tika using thanks. worked around way. could background gc depending got jvm configured use though stay long. another trick described andrzej another field days write token e.g number times matches age document starting reasonable base date add +days:0 query lucene winds boosting results recent. hi assuming use first characters specific field grouping results thing possible out-of-the-box would next best option e.g custom function query thanks. hi olivier setting mime explicitly via stream.type parameter. auto-generated unique value would like. included bit information configure top-level sharding request handler use suggest component get npe get response results completeness pieces puzzle suggest-one suggest-one suggest-two thanks. hi geert-jan thanks ref good stuff think close understand correctly could get using top two versus top simplicity results looked like actual faceted field value/hit counts would top hit facet field followed facet field used field collapsing would improve probability asked top hits would find entries top faceted thanks. seemed work thanks suggestion though using case anybody else reads shall need run tests check performance improvements hadoop workflows output always fresh unless interesting helicopter stunts yes default index always rebuilt scratch thus long primary key used reduce-phase key easy ensure uniqueness index thanks. finally got around looking short field values returned java.lang.short xmlwriter.writeval textresponsewriter.writeval missing check val instanceof short thus bit code used thing happens binary field since val case byte get b b anybody else run seems odd known issue wondering something odd schema especially true since binaryfield write methods xml json via textresponsewriter handle base00-encoding data wondering normally binaryfield.write methods would get used whether actual problem lies elsewhere. different schemas would combine results two schemas define third core different conf separate conf/solrschema.xml set request handler dispatches two real cores. noted lot different approaches could search email list archive discussion using solr subject solr support integration providing compass-like mechanism keeping solr index sync db via hibernateeventlistener. works excellent trying build distribution trunk use prototyping noticed things fresh check-out build trunk/solr sub-dir due dependencies lucene classes done top-level ant compile cd /solr ant builds noticed run-example target trunk/solr/build.xml description show ant -p. tried ant create-package trunk/solr got error near see contrib/velocity anywhere lucene trunk tree recommended way build solr distribution trunk meantime shall use example/start.jar solr.solr.home thanks. others noted currently updating field means deleting inserting entire document depending use field might able create another core/container field plus key field use join support note http //replaced.url improvement looks like 0.x code line though see fix version. hi robert confuses suggester says based spellchecker supposedly work shards issue got configuration shards already trying leverage auto-complete quick dirty work-around would add custom response handler wraps suggester returns results fields needs merge. problems knowing possible set case seems like minor tweak call solranalyzer though understand mark 's comment setanalyzer method says required using like docnum method call tell analyzer either default standardanalyzer whatever gets set explicitly still get used case term vector. actually tagsoup 's reason existence clean html wild tika 's html parser wraps uses generate stream sax events consumes turns normalized. note shall need ant compile top lucene directory first trying solr-specific builds inside /solr sub-dir least ran trying build solr dist recently. tested impact certainly would cause index files read thus cached depending much ram available came big commercial user lucene think running something like fc0. sounds like roysolr running embedded jetty launching solr using start.jar app container newrelic installed. would use copyfield copy contents new field uses string use faceting. sir christmas card list shall fire tomorrow morning let know goes. documentation http //replaced.url specify names/positions fields csv file ignore fieldnames seems like would solve requirement different layout could specify mapping import could handy provide map versus value map updatecsv supports could use header provide mapping header fieldnames schema fieldnames. hi would started using embedded solr back via patched version in-progress code base wondered paragraph said given current state solrj expected roadmap solr general would guidelines special circumstances warrant use solrj know back namely multiple indexes run multiple webapps handled multi-core generating lots http traffic handled dih maybe solr search system since integrated system hands customers restart container option anything got wedged might still issue commonly compelling reasons use solrj thanks. hi arno need add boilerpipecontenthandler tika 's content pretty sure means would need modify solr e.g trunk tikaentityprocessor.gethtmlhandler method would try something like return new boilerpipecontenthandler new contenthandlerdecorator though quick look code curious use. hi otis exactly something similar nutch searches using ehcache http //replaced.url store rewritten query string serialized xml response way dependencies stable searcher/doc ids nutch reference get remote searchers depending entries cache hit docs xml representation storing xml might. hi savannah comments scattered in-line store xpath expressions text file strings load/compile needed definitely yes using tagsoup clean bad html definitely yes needing per-site rules typically xpath optional regex needed extract specific details common sites powered back-end often re-use general rules markup consistent kind thing use bixo http //replaced.url requires knowledge cascading hadoop order yes would separate job field index though often job titles slight variants would probably work much better automatically found common phrases used otherwise get senior bottlewasher sr. actual format extension b characters xml posted. normally would say like getting swap based settings 0gb jvm space used 00gb box confirm nothing else using lots memory right top command showing swap usage right encounter slow search times top command say system load cpu vs. i/o percentages. hi sandhya would post question replaced email.addr.es mailing list include details document confidence level often low enough tika assume good match thus report language. hi erik let us say grins different field besides used autocompletion would places would need hit change field besides terms.fl value layout.vm example asking trying use latest support index uses product_name auto-complete field getting auto-completes happening see solr logs requests made /solr/terms auto-complete look like would expect work seem generating results odd try curling thing use would consider minimum set parameters get expected xml response ideas wrong thanks. curious mean utf-0 complaint mean thanks. interested need sorted output faceted browsing alternative output formats something along lines merge xml responses w/o schema proposal would fine much better would use hadoop prc versus http call sub-searchers assuming better performance might fewer connectivity issues leveraging work done embedded jetty example anybody data points relative performance master schema main search server could get distributed remote searchers would part thanks. hi trying morelikethis support getting odd results realized unless fields used similarity lucene re-analyze field using standardanalyzer case quite different using solr schema first note anybody using morelikethis make sure specify termvectors=true solr schema fields passed query mlt.fl parameters second note wiki page example schema might include reference termvectors field attribute example sample schema says made think initially attributes http //replaced.url make mention termvectors termpositions would edit page currently section talks attributes common ones thanks. shamed taking position vote earlier real experience slf0j indirectly via use jetty know would _something_ logging hood use log0j everywhere finishing earlier work creating jul logger bridged log0j another option would rather avoid. hi got field defined solr schema always contains two fixed values documents get added boost supplied varies max query field expecting ordering results would match boost values longer specifying omitnorms= true field still get seemingly random results use full search interface solr results two documents different boosts looks like expecting value different different document boost values missing thanks. currently links seem wind pointing api-0_0_0-alpha versions expected e.g search streamingupdatesolrserver first hit streamingupdatesolrserver solr api follow link get page http //replaced.url. difference free speech free beer see http //replaced.url. processing html definitely use something like nekohtml tagsoup note tika uses tagsoup makes easy special processing specific elements give content handler gets fed stream cleaned-up html elements. hi floyd typically would creating custom analyzer converts words pinyin zhuyin index would actual hanzi characters plus via copyfield phonetic version search would use dismax search fields higher weighting hanzi field segmentation error prone requires embedding specialized code typically license high quality results commercial vendor first cut approach would use current synonym support map hanzi possible pronunciations numerous open source datasets contain information note might performance issues huge set synonyms weighting phrase matches sufficiently high using dismax think could get reasonable results. noticed prices showing even though got think issue line hit.vm number.currency function needs get passed something looks like number return could list values square brackets confuse number.currency get price think line needs since returns single value without brackets. never tried sending anything utf-0 solr comment issues shall run based experience date would strongly suggest converting utf-0 post solr. already 'fronts solr web service us pretty easy something similar use case map user list groups using ldap make required clause solr request field contains. given requirement break document separately controlled pieces would create fronts solr handles conversion could think ways using solr feel like unnatural acts general comment acls relatively easy way handle via group ids use restrict query document groupid list group ids authorized access user query converted query groupid xx groupid yy xx/yy groups user belongs. hi navina thanks confirming putall list required method supporting changelog functionality hoping others confirm range _not_ used samza system i.e used internally needed tasks true adding notes methods required used samza system changelog support vs. optional used task-specific code needed would helpful thanks. think change token frequency would cause jump quantized levels order change md0 hash obviously happen point quantizing frequencies make much less likely using tech page crawl seems working pretty well though done in-depth analysis seen different proximity hash functions ones referring thanks. hi otis learned krugle approach worked us block bots search page expose target content via statically linked pages separately generated backing store optimized target search terms extracted search logs. docs need updated believe code wrote back note escape solr uses support using bug two tokens white- note idea still issue regular escaping work around bug parser last character escape build expression looks like xxx signal escape character wrong general escaping characters query gets tricky parser shall save pain suffering since code dismaxrequesthandler added solr iirc tries smart handling escaping. hi erik missing change needed thanks much. hi jack thanks included details original question issue got index 000m records 00m unique value prefix characters long adding another indexed field would significant hoping way via grouping/collapsing query time possible thanks. note nutch try solve concerns searchers would slow rmi faster hadoop rpc less issue well handle potential summarizer problems case example remote searcher goes away gets bogged times got hit server needs summary case ran load testing though would serious getting completely wrong summary remote index updated search request happened summary requested munge count might enough pretty simple part remember issues servlet-esque objects getting passed deep shall another look afternoon poking weeks ago thanks. hi emmanuel spot room think would issues would useful able leverage solr 's. believe solritas supports autocompletion box wondering anybody experience using lucidworks distro leveraging autocomplete plugin hooking solr facets curious tricks traps getting work thanks. describing web mining web crawling extract price data web pages put specific field solr using nutch would need write custom plug-ins know extract price page add custom field crawl results topic nutch mailing list since solr downstream consumer whatever nutch provides. pros/cons using cache-control no-cache response header avoid problem tracked entire thread proposal would use response codes along xml body would client deal container response would typically html versus actual thanks. hi looking using embedded solr lets extract ranked results state publish part task 's operation methods defined problematic though specifically range methods return iterators iterating lots results solr feasible newer paging support still abuse architecture wondering whether need support methods called internally tasks e.g task thus optional assuming state automatically restored changelog samza system calling putall list repeatedly dug details would example required method thanks. hi looking snapinstaller seems following copy new index directory master slave 's solr data directory giving index.tmp rename temp index directory index commit send post /solr/update service new index gets use feel like must missing something seems like request middle processed step successful swap could fail due index changing underneath insights thanks. use prng mix ids form auto-incrementing field yes. started using regular dom parser coding hand switched xstream help handling solr pojo mappings seems work fine get past encoding. experience resin definitely explicit character encoding. hi christian erik mentioned appears line standardtokenizerimpl.jflex needs updated include extension b character range. typically run memory solr index update new index searcher getting warmed looking heap often shows ways reduce memory requirements e.g shall see really big chunk used sorted field see http //replaced.url http //replaced.url. another approach add additional acl field contents would list customers ids access document query implicit acl actual query idea would work case use control access source code krugle enterprise product though using ldap-provided groups line mike 's suggestion. fwiw krugle started using moinmoin switched confluence general good change us especially scale scope wiki activity increased integration jira handy far different classes pages easiest way would create spaces assume possible w/the confluence config apache would set space different quickly hacked app used convert moinmoin pages markup syntax confluence switch confluence happens look around. hi shawn different use case ones covered response robert wanted call currently use embedded server building indexes part hadoop workflow results get copied production analytics server daily basis writing multiple embedded servers reduce task gives us maximum performance proven reliable method daily rebuild pre-aggregations need analytics use case regards. intermediate approach use find potentially similar files using apply accurate slower comparison determine true similarity data common get files due small text changes frequency term moves quantized bands changes uber hash get combining terms bands still get matches hashes individual number matching fingerprint values. took look search results seems like word red shows description tag every sure extracting color information smart color-specific keywords found associated text. issue ran daily rolling log files maybe missed find functionality jdk logging package however log0j advocating change noting worked around leveraging resin 's support wrapping logger set daily. try uncommenting lines see fixes problem. code see seems using fsdirectory another layer wrapping going tom ever get useful results testing curious impact various xxxdirectoryfactory implementations batch indexing thanks. hi lucene index generated solr optimizing able view using limo download mac try use luke fails luke complains /index/_jdi0.f0 file directory files downloaded follows known issue luke solr-generated lucene indexes references found file directory involve index corruption far tell index fine thanks. hi got pattern document call xy turn two tokens xy approach could use patterntokenizer extract xy custom filter returns xy next call caches next result could extend patterntokenizer return multiple tokens match though figuring specify schema seems another approach would require custom code thanks. hi moving towards embedding multiple solr cores versus using multiple solr webapps way simplifying build/deploy getting control startup/update process would hate lose handy gui inspecting schema importantly trying queries explain turned anybody tried dual-mode method operation thoughts whether workable issues would taken quick look supporting java code ideas would needed hoping easy approach whacking admin support code thanks. well ultimately heading towards single multiple embedded solr cores case could .jsp-based gui/admin functionality peacefully co-exist use embedded cores description roadmap solr gui example assuming files still exist going forward become much gui layer top new/beefed plan eventually get html using json responses request handlers thanks. hi 've got ancient lucene tokenizer code 'm trying avoid forward-porting n't think 's equivalent solr specifically 's applying shingles output something like worddelimiterfilter e.g mysupersink gets split super sink shingled 're using shingle mysuper super supersink sink n't follow wdf single filter shingles n't created across terms coming wdf 's pieces generated wdf actually way make work solr thanks. hi renee mike right question post users list yes separate setconnectiontimeout used though familiar possibility ping response handler responding connection established getting data back. turn facets query facet=true facet.field= shall get back distinct values though might play settings e.g facet.limit=0 get results need. hi yonik ah explains little seen resin parameter used inside init method typically specified using tag know resin-specific yes would easier update resin runs unpack .war kind pain way knew get working versus jindi approach got special reason ask ongoing support init-param technique. available cores dynamic cores requirement custom code wasted. hi feel like must missing something working customized version supports distributed searching multiple local cores assuming support searchcomponents handler needs create/maintain responsebuilder passed various methods responsebuilder finished list shardrequest objects requests received responses shards inside shardrequest responses list shardresponse objects contain things like solrresponse solrresponse field shardresponse private method set package private appear like easy way create shardresponse objects searchcomponents expect receive inside responsebuilder put custom package shardresponse call setsolrresponse builds run locally deploy jar code runtime get illegal access exception running jetty make work re-building solr.war custom pretty painful thanks. way mean configuring current extractingrequesthandler fundamental issue solr uses tika prevents extractingrequesthandler modified work way seems like useful configuration regards. hi would copied/pasted schema fields testbed schema based version solr using back dark ages version handy comment warning changing version fields multivalued default would checked docs realize behavior changed days would used http //replaced.url examine field would seen thanks. useful use morelikethis support see http //replaced.url attached patch. ago similar issue least back think possible use solr 's copy-field support create boosted version field copied field multiple times. hi frederik figure solution problem asking recently ran similar problem similar setup shards server occasionally query long time occasionally see timeout exceptions http requests e.g restarting jetty seems clear problem temporarily looking code solr handles distributed requests got interesting smells would surprised issue related using regards. hi frederik directly run issue solr experienced similar issues related context case custom made solrj requests generated aggregated/analyzed results load testing ran different issues load test software issue scaling assuming case seen happen e.g limit max parallel connections client used talk solr needed tune solrj settings httpconnectionmanager heavy load running free connections given got shards request going spawn http connections know top head manages connections whether possible tune stack trace sure looks like blocked getting free http connections needed optimize configuration jetty jvm gc etc lots knobs twiddle better worse. think need create separate background process least case web crawling challenge efficiently use samza process simultaneously fetch many urls increase complexity process 's code wind manage either multi-threaded async fetch state hadoop-based crawlers limited number parallel reduce tasks fetching see nutch bixo examples e.g fetchbuffer another project involved past. hi running problem queries distributed among multiple shards return binary field data properly hit single core xml response http request contains expected data hit request handler configured distribute request shards xml contains b b looks like wind getting .tostring data actual data anybody else run done fair amount searching hits yet next step create unit test solr nobody raises hand walk thanks. general yes subset terms occur frequently remove terms easy longer search either part query phrase combine common terms following terms works well bit complex significantly grow index either requires data analysis generate target set common terms. area might issues date range queries many docs run oom errors recent thread yonik others good suggestions ways avoid problem know impact would merging results use date ranges guessing low yonik would know best well 0-server configuration would work would 000m docs/server large number even data/doc small 0k real performance going heavily impacted nature data types queries shall need think distribute data avoid performance constrained worst case searchers nutch wound add termination logic avoid long-running queries clog things primarily dealing load best first step create single solr representative data see well performs issues going around limits box 000m docs versus distributed nature though keeping servers alive happy significant ops task finally would decide early whether search query words ok result set happens missing doc server timed looking query-type solution solr would less interesting. csvloader user found/fixed bug involved active development/maintenance piece code james make progress merging support csv dih great. karsten said providing detail actually trying usually makes better helpful/accurate answers guessing search key value right create multi-value field custom indexed stored indexing add entries custom set analyzer strip index key e.g. would first try setting post explicitly utf-0 matter appears resin tomcat issues properly handling form data without matter issue configuring tomcat see http //replaced.url patch applied may 0th year work around appears bug tomcat assume version solr patch. hi sorry noise finally realized running using java code enwikicontentsource lucene benchmark explicitly set fields push results solr. use characterencodingutf0 think shall get back stream utf-0 bytes db know mysqlxmlfield 's start array bytes returned jdbc call create string array using utf-0 encoding use bytes directly writing xml best thing make sure xml send solr starts line make sure converted text xml fields wrap fields. hi hector low query rates using shards approach improve performance multi-core cpus solr cores setup distributing requests effectively use cpu cores parallel request spread shards across spindles maximizing i/o throughput issues approach binary fields work results back b versus actual data short fields get java.lang.short text prefixed every value deep queries result lots extra load e.g 0000th hit shall get shards hits collected/returned dispatcher though unique score returned case followed second request get actual top hits shards something wonky way distributed http requests queued processed load see ioexceptions always n-0 shards succeed shard request fails good reproducible case yet debug. general issue months ago generate reports things like scm commit activity given day larger customers users multiple timezones timezone use wrote blog post http //replaced.url short answer ultimately decided use utc times server report api ui least heinous various options. hi christian encoding using pushing documents solr specified xml post request separate issue mime-type use post using latest scripts solr characters look like xml pushing example encoded two surrogate characters instead code point extension b set xml parsers handle correctly source similar issues seen potential issue xml parser used updated handle extended unicode code points older parsers still failed handle example. hi tommaso slaves configured use vip talk master easy dynamically change master use via updates. related item might check jmaki http //replaced.url jsp-based amount jsp-generated content pretty easy add support dynamic. hi otis working project speak stefan friends going live separately something independent solr/nutch view search plumbing usable multiple environments makes sense view replicating core solr nutch functionality sucks sure outcome. error either schema.xml file messed might still need uncomment lines beginning file ones say uncomment trying use resin version even though using later version resin lots issues xml parsing. confused answer assuming based page referenced url provided approach textprofilesignature would generate different md0 hash single letter change change resulted change quantized frequency word uncommon word would even show signature. low qps multi-core servers believe reason multiple shards server provide better parallelism request thus reduce response time. idea thought given document/field set would contain text single language write special token language e.g analyzer add my-special-token-prefix-esperanto token field query time assuming know language make required term. depending complexity solrj could solution see section talks solrj provides apis create. would use data files. hi list working improving performance solr scheme cascading supports generating solr index output hadoop job use solrj write index locally via embeddedsolrserver mentions using overwrite=false csv request handler way improving performance see http //replaced.url removed support solrj deemed dangerous mere mortals question whether anyone knows much performance boost really provides hadoop-based workflows straightforward ensure unique key field really unique thus performance gain significant might look figuring way trigger lock re-enabling support solrj thanks. hi especially yonik grins trying solr resin following copied solr-nightly.war /webapps/ cloned/edited resin.conf file include mapping solr adding started resin created /webapps/solr-nightly directory expected tried use solr complained finding see stack trace email realized solrconf folder included default .war added inside /webapps/solr-nightly directory result tried moving uncommented lines file yonik indicated necessary get resin work properly solr change anything figured would ask correct way configure solr resin rather continue thrash would help resin jock far usage limited futzing resin config file add simple thanks. sort answering question seems like need get current core use instantiate new solrcore exact config documentation solrcore 's constructor says core already exists stopped replaced unclear whether graceful swap like hard shutdown old core thanks hoping somebody clarify coredescriptor since much documentation far tell create new solrcore saves coredescriptor pass nothing constructor solrcore takes datadir param see coredescriptor 's datadir gets used construction changing coredescriptor 's datadir effect since would go changing datadir core multi- thanks. right baking fine-grained level security information bad idea example worked pretty well code search krugle project number groups granted access rights file project would inherit list groups user logs get authenticated via ldap set groups belong returned ldap server becomes fairly well-bounded list terms query acl-groups field file/project document set boost portion query. large index would recommend separate solr installation use update/commit changes use snappuller equivalent swap live search unlikely switch lucene tune new parameters control memory usage updating. believe mistake recent email thread list. based experience using jira manner would agree mike automatically assign fix go path wind steadily growing wave deferred issues constantly getting pushed next
