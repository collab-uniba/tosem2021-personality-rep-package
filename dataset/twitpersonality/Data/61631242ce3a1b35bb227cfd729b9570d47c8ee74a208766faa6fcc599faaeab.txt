processing html definitely use something like nekohtml tagsoup note tika uses tagsoup makes easy special processing specific elements give content handler gets fed stream cleaned-up html elements. hi floyd typically would creating custom analyzer converts words pinyin zhuyin index would actual hanzi characters plus via copyfield phonetic version search would use dismax search fields higher weighting hanzi field segmentation error prone requires embedding specialized code typically license high quality results commercial vendor first cut approach would use current synonym support map hanzi possible pronunciations numerous open source datasets contain information note might performance issues huge set synonyms weighting phrase matches sufficiently high using dismax think could get reasonable results. noticed prices showing even though got think issue line hit.vm number.currency function needs get passed something looks like number return could list values square brackets confuse number.currency get price think line needs since returns single value without brackets. never tried sending anything utf-0 solr comment issues shall run based experience date would strongly suggest converting utf-0 post solr. already fronts solr web service us pretty easy something similar use case map user list groups using ldap make required clause solr request field contains. given requirement break document separately controlled pieces would create fronts solr handles conversion could think ways using solr feel like unnatural acts general comment acls relatively easy way handle via group ids use restrict query document groupid list group ids authorized access user query converted query groupid xx groupid yy xx/yy groups user belongs. hi navina thanks confirming putall list required method supporting changelog functionality hoping others confirm range _not_ used samza system i.e used internally needed tasks true adding notes methods required used samza system changelog support vs. optional used task-specific code needed would helpful thanks. think change token frequency would cause jump quantized levels order change md0 hash obviously happen point quantizing frequencies make much less likely using tech page crawl seems working pretty well though done in-depth analysis seen different proximity hash functions ones referring thanks. hi learned krugle approach worked us block bots search page expose target content via statically linked pages separately generated backing store optimized target search terms extracted search logs. docs need updated believe code wrote back note escape solr uses support using bug two tokens white- note idea still issue regular escaping work around bug parser last character escape build expression looks like xxx signal escape character wrong general escaping characters query gets tricky parser shall save pain suffering since code dismaxrequesthandler added solr iirc tries smart handling escaping. hi erik missing change needed thanks much. hi jack thanks included details original question issue got index 000m records 00m unique value prefix characters long adding another indexed field would significant hoping way via grouping/collapsing query time possible thanks. note nutch try solve concerns searchers would slow rmi faster hadoop rpc less issue well handle potential summarizer problems case example remote searcher goes away gets bogged times got hit server needs summary case ran load testing though would serious getting completely wrong summary remote index updated search request happened summary requested munge count might enough pretty simple part remember issues servlet-esque objects getting passed deep shall another look afternoon poking weeks ago thanks. hi emmanuel spot room think would issues would useful able leverage solr s. believe solritas supports autocompletion box wondering anybody experience using lucidworks distro leveraging autocomplete plugin hooking solr facets curious tricks traps getting work thanks. describing web mining web crawling extract price data web pages put specific field solr using nutch would need write custom plug-ins know extract price page add custom field crawl results topic nutch mailing list since solr downstream consumer whatever nutch provides. pros/cons using cache-control no-cache response header avoid problem tracked entire thread proposal would use response codes along xml body would client deal container response would typically html versus actual thanks. hi looking using embedded solr lets extract ranked results state publish part task isoperation methods defined problematic though specifically range methods return iterators iterating lots results solr feasible newer paging support still abuse architecture wondering whether need support methods called internally tasks e.g task thus optional assuming state automatically restored changelog samza system calling putall list repeatedly dug details would example required method thanks. hi looking snapinstaller seems following copy new index directory master slave issolr data directory giving index.tmp rename temp index directory index commit send post /solr/update service new index gets use feel like must missing something seems like request middle processed step successful swap could fail due index changing underneath insights thanks. use prng mix ids form auto-incrementing field yes. started using regular dom parser coding hand switched xstream help handling solr pojo mappings seems work fine get past encoding. experience resin definitely explicit character encoding. hi erik mentioned appears line standardtokenizerimpl.jflex needs updated include extension b character range. typically run memory solr index update new index searcher getting warmed looking heap often shows ways reduce memory requirements e.g shall see really big chunk used sorted field see http//replaced.url http//replaced.url. another approach add additional acl field contents would list customers ids access document query implicit acl actual query idea would work case use control access source code krugle enterprise product though using ldap-provided groups line mike issuggestion. fwiw krugle started using moinmoin switched confluence general good change us especially scale scope wiki activity increased integration jira handy far different classes pages easiest way would create spaces assume possible w/the confluence config apache would set space different quickly hacked app used convert moinmoin pages markup syntax confluence switch confluence happens look around. hi shawn different use case ones covered response robert wanted call currently use embedded server building indexes part hadoop workflow results get copied production analytics server daily basis writing multiple embedded servers reduce task gives us maximum performance proven reliable method daily rebuild pre-aggregations need analytics use case regards. intermediate approach use find potentially similar files using apply accurate slower comparison determine true similarity data common get files due small text changes frequency term moves quantized bands changes uber hash get combining terms bands still get matches hashes individual number matching fingerprint values. took look search results seems like word red shows description tag every sure extracting color information smart color-specific keywords found associated text. issue ran daily rolling log files maybe missed find functionality jdk logging package however log0j advocating change noting worked around leveraging resin issupport wrapping logger set daily. try uncommenting lines see fixes problem. code see seems using fsdirectory another layer wrapping going tom ever get useful results testing curious impact various xxxdirectoryfactory implementations batch indexing thanks. hi lucene index generated solr optimizing able view using limo download mac try use luke fails luke complains /index/_jdi0.f0 file directory files downloaded follows known issue luke solr-generated lucene indexes references found file directory involve index corruption far tell index fine thanks. hi got pattern document call xy turn two tokens xy approach could use patterntokenizer extract xy custom filter returns xy next call caches next result could extend patterntokenizer return multiple tokens match though figuring specify schema seems another approach would require custom code thanks
