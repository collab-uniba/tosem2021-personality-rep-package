hi list requirement producers wait indefinitely kafka broker comes case maintenance network interruptions disk caching front producer producer waits drains producer eventually reconnects functionality working fine kafka producer settings used acks=all would like acknowledge gwen shapiras slideshare http//replaced.url setting derived. hi ian public terabyte dataset project would good match need course means actually finish crawl finalize avro format use data free collections data around though none know target top-ranked pages. would great facing issue rolling code keep solr index sync mysql db access via hibernate thanks. hi sorry missed reply thanks trying find similar report wondering file jira issue might get attention. sounds like hitting max url length 0k common default http web server using run solr web servers know let bump limit via configuration settings. think ran would like run two copies solr inside resin since two sets searchable data distinct schemas/usage patterns specify location configuration directory least using either solr.configdir solr.solr.home subscribed dev list shall post question timing fix versus something quick needs fwiw quick fix pass directory location init thanks. response header and/or meta tags page see code used nutch could missing could wrong either/both issue determining right arbitrary web page easy way analysis advance know sure always x going simplify things soon possible means right processing page collation settings db change db interprets data change data. ok thanks confirming absolute easiest approach would support new init value codecfactory schemacodecfactory would use select different base codec use versus always using lucenecodec would switch everything different codec could extend schemacodecfactory support additional per-field settings stored fields format etc beyond currently available quick dirty hack specified different codecfactory factory hard-codes simpletextcodec works files simpletextxxx format segments.gen segments_xx files pluggable. hi using simpletextcodec past noticed something odd running solr enable posting format via something like resulting index expected text output noticed files standard binary format e.g .fdt field data based simpletextcodec.java assuming would get simpletextstoredfieldsformat stored data holds true files e.g http//replaced.url adding simple text format docvalues walk code figure hoping need change configuration setting thanks. right looking snapinstaller seemed though deletion index directory would deleting files indexreader thanks. url work please include full stack trace runtimeexception version tika using thanks. worked around way. could background gc depending got jvm configured use though stay long. another trick described andrzej another field days write token e.g number times matches age document starting reasonable base date add +days:0 query lucene winds boosting results recent. hi assuming use first characters specific field grouping results thing possible out-of-the-box would next best option e.g custom function query thanks. hi olivier setting mime explicitly via stream.type parameter. auto-generated unique value would like. included bit information configure top-level sharding request handler use suggest component get npe get response results completeness pieces puzzle suggest-one suggest-one suggest-two thanks. hi geert-jan thanks ref good stuff think close understand correctly could get using top two versus top simplicity results looked like actual faceted field value/hit counts would top hit facet field followed facet field used field collapsing would improve probability asked top hits would find entries top faceted thanks. seemed work thanks suggestion though using case anybody else reads shall need run tests check performance improvements hadoop workflows output always fresh unless interesting helicopter stunts yes default index always rebuilt scratch thus long primary key used reduce-phase key easy ensure uniqueness index thanks. finally got around looking short field values returned java.lang.short xmlwriter.writeval textresponsewriter.writeval missing check val instanceof short thus bit code used thing happens binary field since val case byte get b b anybody else run seems odd known issue wondering something odd schema especially true since binaryfield write methods xml json via textresponsewriter handle base00-encoding data wondering normally binaryfield.write methods would get used whether actual problem lies elsewhere. different schemas would combine results two schemas define third core different conf separate conf/solrschema.xml set request handler dispatches two real cores. noted lot different approaches could search email list archive discussion using solr subject solr support integration providing compass-like mechanism keeping solr index sync db via hibernateeventlistener. works excellent trying build distribution trunk use prototyping noticed things fresh check-out build trunk/solr sub-dir due dependencies lucene classes done top-level ant compile cd /solr ant builds noticed run-example target trunk/solr/build.xml description show ant -p. tried ant create-package trunk/solr got error near see contrib/velocity anywhere lucene trunk tree recommended way build solr distribution trunk meantime shall use example/start.jar solr.solr.home thanks. others noted currently updating field means deleting inserting entire document depending use field might able create another core/container field plus key field use join support note http//replaced.url improvement looks like 0.x code line though see fix version. hi robert confuses suggester says based spellchecker supposedly work shards issue got configuration shards already trying leverage auto-complete quick dirty work-around would add custom response handler wraps suggester returns results fields needs merge. problems knowing possible set case seems like minor tweak call solranalyzer though understand mark iscomment setanalyzer method says required using like docnum method call tell analyzer either default standardanalyzer whatever gets set explicitly still get used case term vector. actually tagsoup isreason existence clean html wild tika ishtml parser wraps uses generate stream sax events consumes turns normalized. note shall need ant compile top lucene directory first trying solr-specific builds inside /solr sub-dir least ran trying build solr dist recently. tested impact certainly would cause index files read thus cached depending much ram available came big commercial user lucene think running something like fc0. sounds like roysolr running embedded jetty launching solr using start.jar app container newrelic installed. would use copyfield copy contents new field uses string use faceting. sir christmas card list shall fire tomorrow morning let know goes
