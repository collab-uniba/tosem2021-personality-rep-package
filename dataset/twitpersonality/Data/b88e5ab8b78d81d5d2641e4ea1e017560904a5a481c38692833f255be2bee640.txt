deb package rpm. cassandra testing using old server core celeron processor 0gib ram another 0gib cores two consumer sata hard disks works i.e memory error etc writes reads second maybe database extremely small even days megabytes configuration absolute stock configuration changed anything except noticable node small server remember somewhere hand noticable interesting disk higher log hard disk contained system data disk grain salt intention test setting cluster two distant datacenters performance test. thank mail restarted affected server noticed mail likely related leap second introduced today. yes column stored value every key may matter switch compression afaik advantages default worried storage space mysql table intend move cassandra columns long column names average characters column values mostly byte integers hand many colums empty specifically null afaik mysql able simply store column value default table data without indexes mysql gb millions rows cassandra gb without compression gb compression includes single index row compression switched specific case storage requirements roughly cassandra mysql sorted key sorted column. opposite true requests fast slow case percentile fast slow except order samples opposite direction usual. appearance old rows caused old timestamps set columns turn caused threadlocals cleaned since fixed timestamp rows returned corresponds latest saved state every case. requirement nodes unique token still global cluster/ring node needs unique logically seperate rings networktopologystrategy puts rest code. thinking frequent example theory using tokens dc0 dc0 significantly affect key distribution specifically two keys move next much however seems unexplained requirement least could find explanation nodes must unique token even put different circle networktopologystrategy otherwise data evenly distributed. use propertyfilesnitch networktopologystrategy create multi-datacenter setup two circles start reading page http//replaced.url moreover tokens must unique even across datacenters although pure curiosity wonder rationale behind way someone enlighten first line output nodetool obviously contains token nothing else seems like formatting glitch maybe role. played test cluster stopping cassandra node updating row another noticed delay delivering hinted handoffs know rationale node originally received update noticed server waited started pushing hints log endpoint. noticed strange phenomenon cassandra would like know something completely impossible see log extract new versions row written reads returns obsolete data read version even already written single cassandra node cluster client local network rows written read seconds would think test environment see obsolete data actually thousands log entries hours test say row read match latest data written checked detail history another node seems eventually receive up-to-date row took minutes specific case fyi started evaluate cassandra without significant. copy init.d datastax package. hi think you're good put vote bits needing discussion updated migration section kip please look receive non-null message step correct many thanks mayuresh don't objections receive non-null message step correct specially like part mentioned get rid older null value support log compaction later don't change semantics message format without long people reading documentation acting warning something fundamental need bump magic byte don't support versions forever said support direct upgrades years means message format version could theory removed years it's introduced heads would like mention even without bumping magic byte loose zero copy client x+0 convert internally null value tombstone bit set tombstone bit set null value automatically internally time move version x+0 clients would upgraded obviously support request consumer x loose zero case magic byte reasons explained ok since going meet side note update doc magic byte say bumped whenever message format changed interpretation message format usage reserved bits well currently magic-byte corresponds message.format.version broker set initially producer client sends message magic-byte since broker magic-byte convert means tombstone bit set value set null consumer understanding magic-byte still work consumer magic-byte able understand since still question supporting non-tombstone null value producer client magic-byte sure almost clients upgraded message.format.version broker changed conversion step happen point get consumer request older consumer might convert loose zero copy cases rare becket review plan add details missed/wronged something put kip thanks guys discussing getting consensus clear others proposed think understand make sure could ask either directly update kip detail migration able support non-tombstone null value kip noted discussing kip logic based null value don't clean separates concerns discussed already though split kip-00a kip-00b look deliver kip-00a compacted topic address discuss kip-00a completed options later renu mayuresh discussion following brief agreed bumping magic value may result losing zero given bumping magic value almost free benefit avoiding potential performance issue probably worth issue still need think whether support currently supported kafka allow non-tombstone null value message exist kip-0 problem message supported consumers prior kip-0 null value always interpreted tombstone option keep current way i.e support message would good know concrete use case message probably support something think migration plan migration plan look something like currently lets say broker version x move version broker x+0 supports tombstone client x+0 value set null automatically set well good producer client x broker tombstone bit broker x+0 supporting consumer client x+0 aware able handle consumer client x convert message broker point specify warning clearly docs behavior changed log compaction next release broker x+0 say tombstone used log compaction broker side clients x+0 still null tombstone set automatically client worried go magic byte route unless something sounds like kafka stuck supporting value tombstone bit log compaction life long it's good point might considered earlier plan think stage broker converts message stage next release say log compaction based organization moving release sure goal kafka clearly specifies log compaction means null value tombstone don't bump magic byte broker side always look tombstone bit value imagine broker sees message set broker know message produced message converted value see null order determine tombstone logic put consumer well consumer know message converted understand correctly sufficient messages appended upgraded include kip-0 information contained email strictly confidential use addressee unless otherwise indicated intended recipient please read copy use disclose message attachment please notify sender email telephone delete copies opinions conclusion etc relate official business company shall understood neither given information contained email strictly confidential use addressee unless otherwise indicated intended recipient please read copy use disclose others message attachment please notify sender replying email telephone delete email copies opinions conclusion etc relate official business company shall understood neither given information contained email strictly confidential use addressee unless otherwise indicated intended recipient please read copy use disclose others message attachment please notify sender replying email telephone delete email copies opinions conclusion etc relate official business company shall understood neither given endorsed ig trading ig markets limited company registered england wales company number ig index limited company registered england wales company number registered address cannon bridge house dowgate hill london ec0r 0ya ig markets limited register number ig index limited register number authorised regulated financial conduct authority. merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. hi contributors apache spark looking rat catching licensing errors codebase git log clear kafka apache rat build later removed clear alternative taken information would helpful sense learn experience. see changes. hi according github page kafka requires gradle higher. however try build kafka using gradle don't work com.github.johnrengelman.shadow plugin version used kafka requires gradle seems already jira http//replaced.url wondering supporting gradle higher intentional bug readme file github bug gradle build intention continue support gradle could someone help clarify thanks regards. ah thanks much insights position profile new library real data next week i'll let know goes. agree makes sense cover zk-based topic creation topic creation policy limit zk access brokers going forward point need way disable zk-based topic creation topic creation goes topic creation policy specified kip-0 make sense example solution add broker-side config defaults true user overridden config false controller delete znode /brokers/topics/ topic created controller probably need trick differentiate znode created controller znode created outdated tools example new controller code add new field iscontroller znode /brokers/topics/ topic creates new znode znode don't field child znode controller sure created outdated tools remove znode zookeeper users using outdated tools create topic find topic created
