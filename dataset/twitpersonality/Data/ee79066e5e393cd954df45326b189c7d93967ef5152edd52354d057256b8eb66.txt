hi hjars required typically configured though tez.lib.uris config property thanks regards kuhu missing tez jars likely missing custom setup please follow instructions setup client hadoop environment http//replaced.url. running windows default timestamp using time.time 0e0 get timestamp twice code wouldn't use pycassa thrift api wrapper created python code implemented following function getting timestamps increases call. sake updating thread orr wouldn't yet task trackers cassandra nodes time likely due copying ~000g data hadoop cluster prior processing you're going try installing task trackers nodes. general today large amounts hints still pretty much makes node angry longer nearly nasty unless really low throughput you're probably going gain much practice raising hints window today later get file system based hints think approach work better today i'm concerned practice larger hint windows buy lot see following details http//replaced.url. running http//replaced.url fixed believe need explicitly grant select permission onto system.schema_triggers user workaround. second exception states file sstable missing possible didnt delete commit logs nfs mount stale. i'll need temporarily lower gc_grace_seconds column family run compaction restore gc_grace_seconds original value see http//replaced.url info. re-introduced corrupted node followed process thanks folks mailing list helping listed operations wiki still cleanup node point noticed seeing exception appear times minute existing node new think started around removetoken solve restart node cleanups/resets need thanks. never mind found http//replaced.url. hi tl dr superficial understanding cassandra currently evaluating project cassandra embedded another jvm application embedded instances form cluster application use failure detection cluster membership. thank reply you're sure yet use application distributed cassandra embeds point see ring real pain ass goal prevent users connecting cassandra change anything internal flexibility might point cassandra shoot-and-forget kind really sure yet best regards disclaimer information contained message attachments intended solely attention use named addressee may confidential intended recipient reminded information remains property sender must use disclose distribute copy print rely e-mail received message error please contact sender immediately irrevocably delete message copies. imo deleting always better better store column value associated. nodetool repairs bring much data lot sstables created disk space almost doubled level compactions run slow turned throtting completely wouldn't see much utilization ssd cpu example 0.0mb/s ssd insane anything speed thanks. three things first design doc talking strongly consistent reads wiki gives simple exemple read it's even followed warning actual contradiction second point design docs slightly outdated point least support quorum writes since http//replaced.url resp read provided wrote quorum resp third good recall counters considered stable yet includes documentations. right it's said proxy layer would need read result appropriate consistency level returning memcached client application client application would need declare consistency preference using configuration file. thanks anybody know distributed in-memory system supports structured data e.g tables. dynamic endpoint snitch works keyspace true well seeing running bug left dynamic snitch disabled unless added extra option. yes almost done make possible. range wouldn't contain nodetool ring shows token-ranges node primary range thinking primary someone confirm primary replica always changes change racks secondary replica move next replica different rack either last case next node primary replica different rack r0 contacted prove disprove stopped ran query consistency came back fine meaning indeed hold data ss tables show mean data actually gets moved around racks change probably queries primary replica replicated data read repair automatic data move rack changed least sure deprecated ignore_rack flag useful move data manually rsync sstableloader. use nodetool cfstats show keyspaces cassandra-cli see flush settings default think minutes million ops 0/00th hte heap cf created automagical global memory manager see http//replaced.url http//replaced.url. first update schema cf run nodetool upgradesstables node sometimes works node restart upgrade leaves previous format compressed uncompressed best regards pagarbiai email replaced email.addr.es follow us twitter adforminsider adform watch short video disclaimer information contained message attachments intended solely attention use named addressee may confidential intended recipient reminded information remains property sender must use disclose distribute copy print rely e-mail received message error please contact sender immediately irrevocably delete message copies. client library use. irunnablecallback used many times assue interfaces clojure invoked java example make use translate time.clj java merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. nits docs otherwise looks really good. users implement iclustermetricsconsumer configure cluster conf file effect refactor nimbus.clj merge pull request git repository running alternatively review apply changes patch close pull request make commit master/trunk branch least following commit message. darionyaphet different wait strategy would least worth comparison time. don't really like plan.0.0.x 0.x sound great call 0.x version put master implies released even preview potentially regression yet really confusing going branch continue supported stability lets merge master think going longer stabilize time frame lets create feature branch tied release number personally don't think long stabilize order allow work continue master branch move closer release i've created following branches inactive unless important fixes discovered changes cherry-picked master unless specific 0.x changes cherry-picked master included release e.g storm-0 storm-0 merged master master changes i'm excited keep 0.x sync master think nimbus work needs many eyes much testing possible dedicated branch make easier create preview releases allow users kick tires ease burden parth it's spent lot time keeping work upmerged anyone questions concerns suggestions regarding branch/release management let know. may better revert mk-transfer-fn 0000c00000fc00c0000b00ab00ebc00c00fe00af grouping task handled don't need handled mk-transfer-tuples-handler. lujinhong could squash commits mind merging step let know way prefer. harshach merged think close pull resolve storm-0 dupe storm-0 please let know don't permissions jira grant. retry seems ok two concerns first retrying runtimeexceptions would prefer us restrict retry case conflict curator already lot retries experience curator returns error retry going help second concern quickly retrying suppose common case got unlucky downloaded assignments middle nimbus updating retrying tight loop feel correct interested others opinions don't consider something would block going. hi tried implement pluggable scheduler mentioned http//replaced.url get java.lang.classnotfoundexception error try run understand problem unable find source storm http//replaced.url tried adding demoscheduler mvn compile mvn assembly assembly fails backtype.storm.serialization someone please suggest done server service-handler. check console output http//replaced.url view results. mean available anytypes used keys last two maps regards. hi try use setformat method example know create new template access setformat methaod without process server throw error saying format doesnot exit given key please give answer regards. don't expect problems changes best regards
